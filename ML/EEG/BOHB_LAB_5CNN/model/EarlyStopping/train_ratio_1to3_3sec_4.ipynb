{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 3\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.5398 - accuracy: 0.7506 - val_loss: 0.5164 - val_accuracy: 0.7367\n",
      "Epoch 2/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4652 - accuracy: 0.7561\n",
      "Epoch 00002: val_loss improved from inf to 0.41839, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4643 - accuracy: 0.7576 - val_loss: 0.4184 - val_accuracy: 0.7928\n",
      "Epoch 3/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4022 - accuracy: 0.8138\n",
      "Epoch 00003: val_loss improved from 0.41839 to 0.38329, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4051 - accuracy: 0.8118 - val_loss: 0.3833 - val_accuracy: 0.8460\n",
      "Epoch 4/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8299\n",
      "Epoch 00004: val_loss improved from 0.38329 to 0.35016, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3692 - accuracy: 0.8311 - val_loss: 0.3502 - val_accuracy: 0.8536\n",
      "Epoch 5/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8315\n",
      "Epoch 00005: val_loss improved from 0.35016 to 0.34531, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3634 - accuracy: 0.8314 - val_loss: 0.3453 - val_accuracy: 0.8517\n",
      "Epoch 6/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.8460\n",
      "Epoch 00006: val_loss did not improve from 0.34531\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.3474 - accuracy: 0.8460 - val_loss: 0.3480 - val_accuracy: 0.8346\n",
      "Epoch 7/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3462 - accuracy: 0.8447\n",
      "Epoch 00007: val_loss improved from 0.34531 to 0.28889, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3445 - accuracy: 0.8457 - val_loss: 0.2889 - val_accuracy: 0.8916\n",
      "Epoch 8/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3206 - accuracy: 0.8573\n",
      "Epoch 00008: val_loss improved from 0.28889 to 0.27872, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3210 - accuracy: 0.8577 - val_loss: 0.2787 - val_accuracy: 0.8945\n",
      "Epoch 9/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.8711\n",
      "Epoch 00009: val_loss improved from 0.27872 to 0.24658, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3120 - accuracy: 0.8688 - val_loss: 0.2466 - val_accuracy: 0.9116\n",
      "Epoch 10/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8676\n",
      "Epoch 00010: val_loss did not improve from 0.24658\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2987 - accuracy: 0.8672 - val_loss: 0.2742 - val_accuracy: 0.8878\n",
      "Epoch 11/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8814\n",
      "Epoch 00011: val_loss improved from 0.24658 to 0.23235, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2791 - accuracy: 0.8824 - val_loss: 0.2324 - val_accuracy: 0.9154\n",
      "Epoch 12/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8689\n",
      "Epoch 00012: val_loss did not improve from 0.23235\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2955 - accuracy: 0.8688 - val_loss: 0.3362 - val_accuracy: 0.8356\n",
      "Epoch 13/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.8847\n",
      "Epoch 00013: val_loss improved from 0.23235 to 0.21936, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2711 - accuracy: 0.8853 - val_loss: 0.2194 - val_accuracy: 0.9125\n",
      "Epoch 14/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.8882\n",
      "Epoch 00014: val_loss improved from 0.21936 to 0.20529, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2527 - accuracy: 0.8881 - val_loss: 0.2053 - val_accuracy: 0.9116\n",
      "Epoch 15/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.8892\n",
      "Epoch 00015: val_loss improved from 0.20529 to 0.18802, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2500 - accuracy: 0.8897 - val_loss: 0.1880 - val_accuracy: 0.9325\n",
      "Epoch 16/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2448 - accuracy: 0.8940\n",
      "Epoch 00016: val_loss did not improve from 0.18802\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2451 - accuracy: 0.8945 - val_loss: 0.2296 - val_accuracy: 0.8945\n",
      "Epoch 17/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9005\n",
      "Epoch 00017: val_loss did not improve from 0.18802\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2237 - accuracy: 0.8999 - val_loss: 0.5007 - val_accuracy: 0.7462\n",
      "Epoch 18/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9014\n",
      "Epoch 00018: val_loss did not improve from 0.18802\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2401 - accuracy: 0.9002 - val_loss: 0.2450 - val_accuracy: 0.8907\n",
      "Epoch 19/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2199 - accuracy: 0.9088\n",
      "Epoch 00019: val_loss improved from 0.18802 to 0.17207, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2195 - accuracy: 0.9091 - val_loss: 0.1721 - val_accuracy: 0.9373\n",
      "Epoch 20/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2186 - accuracy: 0.9050\n",
      "Epoch 00020: val_loss did not improve from 0.17207\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2192 - accuracy: 0.9046 - val_loss: 0.5191 - val_accuracy: 0.8042\n",
      "Epoch 21/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9021\n",
      "Epoch 00021: val_loss improved from 0.17207 to 0.16630, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2264 - accuracy: 0.9030 - val_loss: 0.1663 - val_accuracy: 0.9316\n",
      "Epoch 22/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9211\n",
      "Epoch 00022: val_loss improved from 0.16630 to 0.15743, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1960 - accuracy: 0.9214 - val_loss: 0.1574 - val_accuracy: 0.9401\n",
      "Epoch 23/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2049 - accuracy: 0.9178\n",
      "Epoch 00023: val_loss did not improve from 0.15743\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2049 - accuracy: 0.9173 - val_loss: 0.1804 - val_accuracy: 0.9202\n",
      "Epoch 24/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9098\n",
      "Epoch 00024: val_loss did not improve from 0.15743\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2043 - accuracy: 0.9100 - val_loss: 0.2044 - val_accuracy: 0.9068\n",
      "Epoch 25/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2020 - accuracy: 0.9130\n",
      "Epoch 00025: val_loss improved from 0.15743 to 0.15288, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2006 - accuracy: 0.9138 - val_loss: 0.1529 - val_accuracy: 0.9382\n",
      "Epoch 26/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9188\n",
      "Epoch 00026: val_loss did not improve from 0.15288\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1929 - accuracy: 0.9183 - val_loss: 0.2647 - val_accuracy: 0.8764\n",
      "Epoch 27/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9317\n",
      "Epoch 00027: val_loss did not improve from 0.15288\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1785 - accuracy: 0.9309 - val_loss: 0.1579 - val_accuracy: 0.9420\n",
      "Epoch 28/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1913 - accuracy: 0.9198\n",
      "Epoch 00028: val_loss did not improve from 0.15288\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1906 - accuracy: 0.9202 - val_loss: 0.1773 - val_accuracy: 0.9259\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/99 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9253\n",
      "Epoch 00029: val_loss did not improve from 0.15288\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1889 - accuracy: 0.9255 - val_loss: 0.2177 - val_accuracy: 0.9068\n",
      "Epoch 30/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9394\n",
      "Epoch 00030: val_loss improved from 0.15288 to 0.15128, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1569 - accuracy: 0.9392 - val_loss: 0.1513 - val_accuracy: 0.9430\n",
      "Epoch 31/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9291\n",
      "Epoch 00031: val_loss did not improve from 0.15128\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1720 - accuracy: 0.9287 - val_loss: 0.2173 - val_accuracy: 0.9011\n",
      "Epoch 32/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9275\n",
      "Epoch 00032: val_loss did not improve from 0.15128\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1705 - accuracy: 0.9281 - val_loss: 0.1588 - val_accuracy: 0.9306\n",
      "Epoch 33/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9352\n",
      "Epoch 00033: val_loss did not improve from 0.15128\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1567 - accuracy: 0.9354 - val_loss: 0.4021 - val_accuracy: 0.8279\n",
      "Epoch 34/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1590 - accuracy: 0.9378\n",
      "Epoch 00034: val_loss did not improve from 0.15128\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1608 - accuracy: 0.9373 - val_loss: 0.1575 - val_accuracy: 0.9373\n",
      "Epoch 35/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9362\n",
      "Epoch 00035: val_loss improved from 0.15128 to 0.13970, saving model to best_model_4.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1560 - accuracy: 0.9360 - val_loss: 0.1397 - val_accuracy: 0.9411\n",
      "Epoch 36/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.9420\n",
      "Epoch 00036: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1494 - accuracy: 0.9420 - val_loss: 0.2695 - val_accuracy: 0.8812\n",
      "Epoch 37/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9378\n",
      "Epoch 00037: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1559 - accuracy: 0.9388 - val_loss: 0.2398 - val_accuracy: 0.8878\n",
      "Epoch 38/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9401\n",
      "Epoch 00038: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1507 - accuracy: 0.9395 - val_loss: 0.1601 - val_accuracy: 0.9287\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9468\n",
      "Epoch 00039: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1375 - accuracy: 0.9471 - val_loss: 0.1605 - val_accuracy: 0.9259\n",
      "Epoch 40/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9475\n",
      "Epoch 00040: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1260 - accuracy: 0.9477 - val_loss: 0.2248 - val_accuracy: 0.8926\n",
      "Epoch 41/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1375 - accuracy: 0.9494\n",
      "Epoch 00041: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1370 - accuracy: 0.9496 - val_loss: 0.2472 - val_accuracy: 0.9049\n",
      "Epoch 42/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9398\n",
      "Epoch 00042: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1418 - accuracy: 0.9395 - val_loss: 0.1787 - val_accuracy: 0.9316\n",
      "Epoch 43/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9417\n",
      "Epoch 00043: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1376 - accuracy: 0.9420 - val_loss: 0.1483 - val_accuracy: 0.9392\n",
      "Epoch 44/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9433\n",
      "Epoch 00044: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1379 - accuracy: 0.9439 - val_loss: 0.2266 - val_accuracy: 0.9030\n",
      "Epoch 45/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9472\n",
      "Epoch 00045: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1267 - accuracy: 0.9474 - val_loss: 0.1837 - val_accuracy: 0.9221\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9472\n",
      "Epoch 00046: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1218 - accuracy: 0.9477 - val_loss: 0.1516 - val_accuracy: 0.9306\n",
      "Epoch 47/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9520\n",
      "Epoch 00047: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1238 - accuracy: 0.9522 - val_loss: 0.1494 - val_accuracy: 0.9373\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9478\n",
      "Epoch 00048: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1385 - accuracy: 0.9471 - val_loss: 0.2142 - val_accuracy: 0.9049\n",
      "Epoch 49/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9546\n",
      "Epoch 00049: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1127 - accuracy: 0.9550 - val_loss: 0.1506 - val_accuracy: 0.9401\n",
      "Epoch 50/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9588\n",
      "Epoch 00050: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1090 - accuracy: 0.9582 - val_loss: 0.2917 - val_accuracy: 0.8764\n",
      "Epoch 51/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9555\n",
      "Epoch 00051: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1089 - accuracy: 0.9556 - val_loss: 0.1527 - val_accuracy: 0.9401\n",
      "Epoch 52/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9601\n",
      "Epoch 00052: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.1053 - accuracy: 0.9598 - val_loss: 0.2384 - val_accuracy: 0.8878\n",
      "Epoch 53/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9610\n",
      "Epoch 00053: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.0989 - accuracy: 0.9613 - val_loss: 0.1945 - val_accuracy: 0.9116\n",
      "Epoch 54/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9523\n",
      "Epoch 00054: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1170 - accuracy: 0.9525 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9633\n",
      "Epoch 00055: val_loss did not improve from 0.13970\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.0934 - accuracy: 0.9636 - val_loss: 0.1487 - val_accuracy: 0.9401\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 480, 154)          1078      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 160, 154)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 160, 157)          72691     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 53, 157)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 53, 11)            5192      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 17, 11)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 11)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 130)               73840     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 81)                10611     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 82        \n",
      "=================================================================\n",
      "Total params: 163,494\n",
      "Trainable params: 163,494\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 - 0s - loss: 0.1397 - accuracy: 0.9411\n",
      "27/27 - 0s - loss: 0.1500 - accuracy: 0.9325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.13969653844833374\n",
      "test_loss: 0.15000109374523163\n",
      "test_acc: 0.9324644804000854\n",
      "precision: 0.8155737704918032\n",
      "recall: 0.943127962085308\n",
      "specificity 0.9289099526066351\n",
      "sensitivity :  0.943127962085308\n",
      "far 0.07109004739336493\n",
      "frr 0.05687203791469194\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 154, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 157, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 11, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.22147858870100906)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(130)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(81, activation = 'relu')(lstm_1)\n",
    "# dense_2 = tf.keras.layers.Dense(50, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_1)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_3)\n",
    "\n",
    "# Adam\n",
    "# model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.001646253038527969), metrics = ['accuracy'])\n",
    "# SGD\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.01665328009108491, momentum=0.7014797306275332), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "\n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-durham",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
