{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 10\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.5482 - accuracy: 0.7452 - val_loss: 0.5611 - val_accuracy: 0.7367\n",
      "Epoch 2/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4758 - accuracy: 0.7545\n",
      "Epoch 00002: val_loss improved from inf to 0.45723, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4749 - accuracy: 0.7544 - val_loss: 0.4572 - val_accuracy: 0.7376\n",
      "Epoch 3/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4556 - accuracy: 0.7613\n",
      "Epoch 00003: val_loss improved from 0.45723 to 0.44605, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4557 - accuracy: 0.7614 - val_loss: 0.4460 - val_accuracy: 0.8108\n",
      "Epoch 4/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4425 - accuracy: 0.8006\n",
      "Epoch 00004: val_loss improved from 0.44605 to 0.43190, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4416 - accuracy: 0.8016 - val_loss: 0.4319 - val_accuracy: 0.8308\n",
      "Epoch 5/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8125\n",
      "Epoch 00005: val_loss did not improve from 0.43190\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.4235 - accuracy: 0.8127 - val_loss: 0.4597 - val_accuracy: 0.8489\n",
      "Epoch 6/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3824 - accuracy: 0.8270\n",
      "Epoch 00006: val_loss improved from 0.43190 to 0.38106, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3874 - accuracy: 0.8251 - val_loss: 0.3811 - val_accuracy: 0.8156\n",
      "Epoch 7/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8302\n",
      "Epoch 00007: val_loss improved from 0.38106 to 0.38048, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3916 - accuracy: 0.8302 - val_loss: 0.3805 - val_accuracy: 0.8146\n",
      "Epoch 8/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3637 - accuracy: 0.8363\n",
      "Epoch 00008: val_loss improved from 0.38048 to 0.32331, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3634 - accuracy: 0.8365 - val_loss: 0.3233 - val_accuracy: 0.8546\n",
      "Epoch 9/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8360\n",
      "Epoch 00009: val_loss did not improve from 0.32331\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3589 - accuracy: 0.8356 - val_loss: 0.3453 - val_accuracy: 0.8584\n",
      "Epoch 10/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8402\n",
      "Epoch 00010: val_loss did not improve from 0.32331\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3708 - accuracy: 0.8403 - val_loss: 0.3501 - val_accuracy: 0.8631\n",
      "Epoch 11/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3424 - accuracy: 0.8573\n",
      "Epoch 00011: val_loss did not improve from 0.32331\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3413 - accuracy: 0.8580 - val_loss: 0.3828 - val_accuracy: 0.8327\n",
      "Epoch 12/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8573\n",
      "Epoch 00012: val_loss did not improve from 0.32331\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3351 - accuracy: 0.8568 - val_loss: 0.3456 - val_accuracy: 0.8688\n",
      "Epoch 13/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3303 - accuracy: 0.8512\n",
      "Epoch 00013: val_loss did not improve from 0.32331\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3312 - accuracy: 0.8501 - val_loss: 0.3564 - val_accuracy: 0.8270\n",
      "Epoch 14/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8637\n",
      "Epoch 00014: val_loss improved from 0.32331 to 0.32078, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3210 - accuracy: 0.8644 - val_loss: 0.3208 - val_accuracy: 0.8840\n",
      "Epoch 15/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8592\n",
      "Epoch 00015: val_loss did not improve from 0.32078\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3368 - accuracy: 0.8593 - val_loss: 0.3277 - val_accuracy: 0.8707\n",
      "Epoch 16/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3071 - accuracy: 0.8715\n",
      "Epoch 00016: val_loss did not improve from 0.32078\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3061 - accuracy: 0.8723 - val_loss: 0.3694 - val_accuracy: 0.8460\n",
      "Epoch 17/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8595\n",
      "Epoch 00017: val_loss did not improve from 0.32078\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3239 - accuracy: 0.8609 - val_loss: 0.3460 - val_accuracy: 0.8679\n",
      "Epoch 18/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3144 - accuracy: 0.8624\n",
      "Epoch 00018: val_loss improved from 0.32078 to 0.31524, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3120 - accuracy: 0.8638 - val_loss: 0.3152 - val_accuracy: 0.8840\n",
      "Epoch 19/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8798\n",
      "Epoch 00019: val_loss improved from 0.31524 to 0.31414, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2914 - accuracy: 0.8805 - val_loss: 0.3141 - val_accuracy: 0.8831\n",
      "Epoch 20/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8666\n",
      "Epoch 00020: val_loss did not improve from 0.31414\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3129 - accuracy: 0.8663 - val_loss: 0.3201 - val_accuracy: 0.8688\n",
      "Epoch 21/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.8782\n",
      "Epoch 00021: val_loss did not improve from 0.31414\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2943 - accuracy: 0.8793 - val_loss: 0.3625 - val_accuracy: 0.8489\n",
      "Epoch 22/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8698\n",
      "Epoch 00022: val_loss improved from 0.31414 to 0.31110, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3030 - accuracy: 0.8701 - val_loss: 0.3111 - val_accuracy: 0.8679\n",
      "Epoch 23/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8744\n",
      "Epoch 00023: val_loss did not improve from 0.31110\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3026 - accuracy: 0.8742 - val_loss: 0.3798 - val_accuracy: 0.7861\n",
      "Epoch 24/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8805\n",
      "Epoch 00024: val_loss did not improve from 0.31110\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2922 - accuracy: 0.8802 - val_loss: 0.3544 - val_accuracy: 0.8346\n",
      "Epoch 25/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8721\n",
      "Epoch 00025: val_loss improved from 0.31110 to 0.29004, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3001 - accuracy: 0.8729 - val_loss: 0.2900 - val_accuracy: 0.8774\n",
      "Epoch 26/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3119 - accuracy: 0.8628\n",
      "Epoch 00026: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.3107 - accuracy: 0.8634 - val_loss: 0.3291 - val_accuracy: 0.8584\n",
      "Epoch 27/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.8818\n",
      "Epoch 00027: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2806 - accuracy: 0.8815 - val_loss: 0.3046 - val_accuracy: 0.8935\n",
      "Epoch 28/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.8814\n",
      "Epoch 00028: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2849 - accuracy: 0.8812 - val_loss: 0.3128 - val_accuracy: 0.8631\n",
      "Epoch 29/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2908 - accuracy: 0.8824\n",
      "Epoch 00029: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2916 - accuracy: 0.8821 - val_loss: 0.2997 - val_accuracy: 0.8945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2788 - accuracy: 0.8892\n",
      "Epoch 00030: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2780 - accuracy: 0.8901 - val_loss: 0.3077 - val_accuracy: 0.8850\n",
      "Epoch 31/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.8882\n",
      "Epoch 00031: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2893 - accuracy: 0.8881 - val_loss: 0.3391 - val_accuracy: 0.8774\n",
      "Epoch 32/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.8831\n",
      "Epoch 00032: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2783 - accuracy: 0.8824 - val_loss: 0.3805 - val_accuracy: 0.8384\n",
      "Epoch 33/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.8779\n",
      "Epoch 00033: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2890 - accuracy: 0.8780 - val_loss: 0.3010 - val_accuracy: 0.8783\n",
      "Epoch 34/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2878 - accuracy: 0.8789\n",
      "Epoch 00034: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2879 - accuracy: 0.8783 - val_loss: 0.3583 - val_accuracy: 0.8346\n",
      "Epoch 35/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2845 - accuracy: 0.8785\n",
      "Epoch 00035: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2865 - accuracy: 0.8774 - val_loss: 0.3686 - val_accuracy: 0.8346\n",
      "Epoch 36/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2792 - accuracy: 0.8850\n",
      "Epoch 00036: val_loss did not improve from 0.29004\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2794 - accuracy: 0.8850 - val_loss: 0.2992 - val_accuracy: 0.8945\n",
      "Epoch 37/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.8831\n",
      "Epoch 00037: val_loss improved from 0.29004 to 0.27618, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2789 - accuracy: 0.8837 - val_loss: 0.2762 - val_accuracy: 0.8983\n",
      "Epoch 38/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8911\n",
      "Epoch 00038: val_loss did not improve from 0.27618\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2816 - accuracy: 0.8913 - val_loss: 0.5282 - val_accuracy: 0.7652\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.8866\n",
      "Epoch 00039: val_loss improved from 0.27618 to 0.27369, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2779 - accuracy: 0.8872 - val_loss: 0.2737 - val_accuracy: 0.8983\n",
      "Epoch 40/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.8831\n",
      "Epoch 00040: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2827 - accuracy: 0.8824 - val_loss: 0.3149 - val_accuracy: 0.8755\n",
      "Epoch 41/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2747 - accuracy: 0.8895\n",
      "Epoch 00041: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2731 - accuracy: 0.8897 - val_loss: 0.3408 - val_accuracy: 0.8451\n",
      "Epoch 42/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2783 - accuracy: 0.8818\n",
      "Epoch 00042: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2793 - accuracy: 0.8812 - val_loss: 0.3154 - val_accuracy: 0.8736\n",
      "Epoch 43/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.8898\n",
      "Epoch 00043: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2559 - accuracy: 0.8904 - val_loss: 0.4110 - val_accuracy: 0.8317\n",
      "Epoch 44/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8811\n",
      "Epoch 00044: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2799 - accuracy: 0.8818 - val_loss: 0.2968 - val_accuracy: 0.8859\n",
      "Epoch 45/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8805\n",
      "Epoch 00045: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2832 - accuracy: 0.8805 - val_loss: 0.3273 - val_accuracy: 0.8878\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2822 - accuracy: 0.8808\n",
      "Epoch 00046: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2823 - accuracy: 0.8805 - val_loss: 0.4494 - val_accuracy: 0.7947\n",
      "Epoch 47/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.8924\n",
      "Epoch 00047: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2727 - accuracy: 0.8923 - val_loss: 0.2781 - val_accuracy: 0.8916\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2741 - accuracy: 0.8895\n",
      "Epoch 00048: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2728 - accuracy: 0.8901 - val_loss: 0.3001 - val_accuracy: 0.8812\n",
      "Epoch 49/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.8963\n",
      "Epoch 00049: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2647 - accuracy: 0.8958 - val_loss: 0.3214 - val_accuracy: 0.8593\n",
      "Epoch 50/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8889\n",
      "Epoch 00050: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2696 - accuracy: 0.8875 - val_loss: 0.2951 - val_accuracy: 0.8774\n",
      "Epoch 51/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8847\n",
      "Epoch 00051: val_loss did not improve from 0.27369\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2803 - accuracy: 0.8853 - val_loss: 0.3126 - val_accuracy: 0.8612\n",
      "Epoch 52/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9001\n",
      "Epoch 00052: val_loss improved from 0.27369 to 0.26875, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2527 - accuracy: 0.8999 - val_loss: 0.2688 - val_accuracy: 0.8964\n",
      "Epoch 53/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.8934\n",
      "Epoch 00053: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2553 - accuracy: 0.8942 - val_loss: 0.3169 - val_accuracy: 0.8565\n",
      "Epoch 54/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.8892\n",
      "Epoch 00054: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2671 - accuracy: 0.8881 - val_loss: 0.4104 - val_accuracy: 0.7956\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9011\n",
      "Epoch 00055: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2484 - accuracy: 0.9015 - val_loss: 0.5130 - val_accuracy: 0.7671\n",
      "Epoch 56/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.8947\n",
      "Epoch 00056: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2623 - accuracy: 0.8948 - val_loss: 0.4770 - val_accuracy: 0.7738\n",
      "Epoch 57/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.8927\n",
      "Epoch 00057: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2658 - accuracy: 0.8929 - val_loss: 0.3159 - val_accuracy: 0.8726\n",
      "Epoch 58/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.8940\n",
      "Epoch 00058: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2687 - accuracy: 0.8939 - val_loss: 0.3246 - val_accuracy: 0.8527\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/99 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9021\n",
      "Epoch 00059: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2527 - accuracy: 0.9030 - val_loss: 0.2824 - val_accuracy: 0.8821\n",
      "Epoch 60/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2675 - accuracy: 0.8921\n",
      "Epoch 00060: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2684 - accuracy: 0.8920 - val_loss: 0.3468 - val_accuracy: 0.8755\n",
      "Epoch 61/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.8843\n",
      "Epoch 00061: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2758 - accuracy: 0.8853 - val_loss: 0.2891 - val_accuracy: 0.8840\n",
      "Epoch 62/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.8927\n",
      "Epoch 00062: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2602 - accuracy: 0.8929 - val_loss: 0.3129 - val_accuracy: 0.8793\n",
      "Epoch 63/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2563 - accuracy: 0.8953\n",
      "Epoch 00063: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2557 - accuracy: 0.8954 - val_loss: 0.2894 - val_accuracy: 0.8774\n",
      "Epoch 64/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.8985\n",
      "Epoch 00064: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2516 - accuracy: 0.8992 - val_loss: 0.2701 - val_accuracy: 0.8926\n",
      "Epoch 65/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.8998\n",
      "Epoch 00065: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2627 - accuracy: 0.8989 - val_loss: 0.2946 - val_accuracy: 0.8821\n",
      "Epoch 66/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2690 - accuracy: 0.8921\n",
      "Epoch 00066: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.2686 - accuracy: 0.8926 - val_loss: 0.4286 - val_accuracy: 0.7890\n",
      "Epoch 67/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.8901\n",
      "Epoch 00067: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2565 - accuracy: 0.8907 - val_loss: 0.3436 - val_accuracy: 0.8232\n",
      "Epoch 68/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.8953\n",
      "Epoch 00068: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2560 - accuracy: 0.8958 - val_loss: 0.2994 - val_accuracy: 0.8897\n",
      "Epoch 69/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.8937\n",
      "Epoch 00069: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2590 - accuracy: 0.8926 - val_loss: 0.3055 - val_accuracy: 0.8764\n",
      "Epoch 70/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.8898\n",
      "Epoch 00070: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2637 - accuracy: 0.8897 - val_loss: 0.3794 - val_accuracy: 0.8156\n",
      "Epoch 71/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.8901\n",
      "Epoch 00071: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2605 - accuracy: 0.8897 - val_loss: 0.2727 - val_accuracy: 0.8869\n",
      "Epoch 72/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2693 - accuracy: 0.8953\n",
      "Epoch 00072: val_loss did not improve from 0.26875\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2685 - accuracy: 0.8958 - val_loss: 0.2975 - val_accuracy: 0.8821\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 480, 76)           532       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 160, 76)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 160, 20)           4580      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 53, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 53, 9)             549       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 17, 9)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 17, 9)             0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 232)               224576    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                7456      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 21)                693       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 22        \n",
      "=================================================================\n",
      "Total params: 238,408\n",
      "Trainable params: 238,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "33/33 - 0s - loss: 0.2688 - accuracy: 0.8964\n",
      "27/27 - 0s - loss: 0.2165 - accuracy: 0.9182\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.2687532603740692\n",
      "test_loss: 0.21645528078079224\n",
      "test_acc: 0.9182464480400085\n",
      "precision: 0.8585858585858586\n",
      "recall: 0.8056872037914692\n",
      "specificity 0.9557661927330173\n",
      "sensitivity :  0.8056872037914692\n",
      "far 0.044233807266982623\n",
      "frr 0.1943127962085308\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 76, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 20, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 9, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.569149552203474)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(232)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(32, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(21, activation = 'relu')(dense_1)\n",
    "# dense_3 = tf.keras.layers.Dense(134, activation = 'relu')(dense_2)\n",
    "dense_4 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_2)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_4)\n",
    "\n",
    "# Adam\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.009755746313544013), metrics = ['accuracy'])\n",
    "# SGD\n",
    "# model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.08119450015184537, momentum=0.4770521140454941), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "        \n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
