{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 10\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.5299 - accuracy: 0.7481 - val_loss: 0.5130 - val_accuracy: 0.7367\n",
      "Epoch 2/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.4586 - accuracy: 0.7617\n",
      "Epoch 00002: val_loss improved from inf to 0.45733, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.4586 - accuracy: 0.7617 - val_loss: 0.4573 - val_accuracy: 0.7861\n",
      "Epoch 3/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.4377 - accuracy: 0.7809\n",
      "Epoch 00003: val_loss improved from 0.45733 to 0.44109, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4379 - accuracy: 0.7811 - val_loss: 0.4411 - val_accuracy: 0.7880\n",
      "Epoch 4/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.7858\n",
      "Epoch 00004: val_loss improved from 0.44109 to 0.42005, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4352 - accuracy: 0.7858 - val_loss: 0.4200 - val_accuracy: 0.7975\n",
      "Epoch 5/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4270 - accuracy: 0.7922\n",
      "Epoch 00005: val_loss improved from 0.42005 to 0.41242, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4257 - accuracy: 0.7918 - val_loss: 0.4124 - val_accuracy: 0.8051\n",
      "Epoch 6/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.7944\n",
      "Epoch 00006: val_loss did not improve from 0.41242\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4185 - accuracy: 0.7944 - val_loss: 0.4184 - val_accuracy: 0.8080\n",
      "Epoch 7/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8009\n",
      "Epoch 00007: val_loss did not improve from 0.41242\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4012 - accuracy: 0.8035 - val_loss: 0.4937 - val_accuracy: 0.7947\n",
      "Epoch 8/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4010 - accuracy: 0.8041\n",
      "Epoch 00008: val_loss improved from 0.41242 to 0.39638, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.4003 - accuracy: 0.8042 - val_loss: 0.3964 - val_accuracy: 0.8127\n",
      "Epoch 9/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.3816 - accuracy: 0.8157\n",
      "Epoch 00009: val_loss did not improve from 0.39638\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.3808 - accuracy: 0.8162 - val_loss: 0.4041 - val_accuracy: 0.8279\n",
      "Epoch 10/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3938 - accuracy: 0.8041\n",
      "Epoch 00010: val_loss improved from 0.39638 to 0.38681, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3940 - accuracy: 0.8051 - val_loss: 0.3868 - val_accuracy: 0.8270\n",
      "Epoch 11/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3683 - accuracy: 0.8318\n",
      "Epoch 00011: val_loss did not improve from 0.38681\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3698 - accuracy: 0.8308 - val_loss: 0.4630 - val_accuracy: 0.7624\n",
      "Epoch 12/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.3633 - accuracy: 0.8298\n",
      "Epoch 00012: val_loss did not improve from 0.38681\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.3633 - accuracy: 0.8298 - val_loss: 0.4381 - val_accuracy: 0.7966\n",
      "Epoch 13/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3657 - accuracy: 0.8309\n",
      "Epoch 00013: val_loss did not improve from 0.38681\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3674 - accuracy: 0.8298 - val_loss: 0.4169 - val_accuracy: 0.8184\n",
      "Epoch 14/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8325\n",
      "Epoch 00014: val_loss improved from 0.38681 to 0.37817, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3571 - accuracy: 0.8333 - val_loss: 0.3782 - val_accuracy: 0.8118\n",
      "Epoch 15/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3495 - accuracy: 0.8325\n",
      "Epoch 00015: val_loss improved from 0.37817 to 0.33122, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3458 - accuracy: 0.8349 - val_loss: 0.3312 - val_accuracy: 0.8707\n",
      "Epoch 16/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.3458 - accuracy: 0.8463\n",
      "Epoch 00016: val_loss improved from 0.33122 to 0.29484, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3458 - accuracy: 0.8463 - val_loss: 0.2948 - val_accuracy: 0.8783\n",
      "Epoch 17/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8579\n",
      "Epoch 00017: val_loss improved from 0.29484 to 0.28962, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.3245 - accuracy: 0.8577 - val_loss: 0.2896 - val_accuracy: 0.8850\n",
      "Epoch 18/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8545\n",
      "Epoch 00018: val_loss did not improve from 0.28962\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.3365 - accuracy: 0.8546 - val_loss: 0.2982 - val_accuracy: 0.8840\n",
      "Epoch 19/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.3131 - accuracy: 0.8616\n",
      "Epoch 00019: val_loss did not improve from 0.28962\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.3130 - accuracy: 0.8619 - val_loss: 0.4408 - val_accuracy: 0.8137\n",
      "Epoch 20/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8545\n",
      "Epoch 00020: val_loss did not improve from 0.28962\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.3175 - accuracy: 0.8552 - val_loss: 0.4410 - val_accuracy: 0.8631\n",
      "Epoch 21/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.8729\n",
      "Epoch 00021: val_loss improved from 0.28962 to 0.28480, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2982 - accuracy: 0.8729 - val_loss: 0.2848 - val_accuracy: 0.8897\n",
      "Epoch 22/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.8843\n",
      "Epoch 00022: val_loss did not improve from 0.28480\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2814 - accuracy: 0.8821 - val_loss: 0.3101 - val_accuracy: 0.8964\n",
      "Epoch 23/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8789\n",
      "Epoch 00023: val_loss improved from 0.28480 to 0.22245, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2942 - accuracy: 0.8805 - val_loss: 0.2224 - val_accuracy: 0.9097\n",
      "Epoch 24/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.8909\n",
      "Epoch 00024: val_loss did not improve from 0.22245\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2700 - accuracy: 0.8904 - val_loss: 0.3708 - val_accuracy: 0.8527\n",
      "Epoch 25/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.2618 - accuracy: 0.8870\n",
      "Epoch 00025: val_loss did not improve from 0.22245\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2610 - accuracy: 0.8875 - val_loss: 0.2238 - val_accuracy: 0.9059\n",
      "Epoch 26/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.8939\n",
      "Epoch 00026: val_loss did not improve from 0.22245\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2641 - accuracy: 0.8939 - val_loss: 0.2434 - val_accuracy: 0.9011\n",
      "Epoch 27/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2583 - accuracy: 0.8945\n",
      "Epoch 00027: val_loss improved from 0.22245 to 0.21577, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2583 - accuracy: 0.8945 - val_loss: 0.2158 - val_accuracy: 0.9078\n",
      "Epoch 28/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9021\n",
      "Epoch 00028: val_loss improved from 0.21577 to 0.20553, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2394 - accuracy: 0.9037 - val_loss: 0.2055 - val_accuracy: 0.9106\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/99 [============================>.] - ETA: 0s - loss: 0.2401 - accuracy: 0.8996\n",
      "Epoch 00029: val_loss did not improve from 0.20553\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2399 - accuracy: 0.8996 - val_loss: 0.2491 - val_accuracy: 0.9030\n",
      "Epoch 30/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9072\n",
      "Epoch 00030: val_loss did not improve from 0.20553\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2319 - accuracy: 0.9062 - val_loss: 0.2061 - val_accuracy: 0.9097\n",
      "Epoch 31/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.9126\n",
      "Epoch 00031: val_loss did not improve from 0.20553\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2180 - accuracy: 0.9132 - val_loss: 0.2997 - val_accuracy: 0.8764\n",
      "Epoch 32/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2157 - accuracy: 0.9146\n",
      "Epoch 00032: val_loss did not improve from 0.20553\n",
      "99/99 [==============================] - 2s 20ms/step - loss: 0.2148 - accuracy: 0.9148 - val_loss: 0.3272 - val_accuracy: 0.8888\n",
      "Epoch 33/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9139\n",
      "Epoch 00033: val_loss improved from 0.20553 to 0.19348, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2193 - accuracy: 0.9135 - val_loss: 0.1935 - val_accuracy: 0.9135\n",
      "Epoch 34/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9161\n",
      "Epoch 00034: val_loss did not improve from 0.19348\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.2175 - accuracy: 0.9163 - val_loss: 0.2668 - val_accuracy: 0.8964\n",
      "Epoch 35/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9190\n",
      "Epoch 00035: val_loss improved from 0.19348 to 0.19006, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2051 - accuracy: 0.9189 - val_loss: 0.1901 - val_accuracy: 0.9249\n",
      "Epoch 36/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2132 - accuracy: 0.9133\n",
      "Epoch 00036: val_loss did not improve from 0.19006\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2140 - accuracy: 0.9132 - val_loss: 0.2323 - val_accuracy: 0.8983\n",
      "Epoch 37/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2091 - accuracy: 0.9162\n",
      "Epoch 00037: val_loss improved from 0.19006 to 0.17750, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.2080 - accuracy: 0.9167 - val_loss: 0.1775 - val_accuracy: 0.9287\n",
      "Epoch 38/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9243\n",
      "Epoch 00038: val_loss did not improve from 0.17750\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1966 - accuracy: 0.9240 - val_loss: 0.1862 - val_accuracy: 0.9259\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9233\n",
      "Epoch 00039: val_loss did not improve from 0.17750\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1841 - accuracy: 0.9230 - val_loss: 0.1987 - val_accuracy: 0.9297\n",
      "Epoch 40/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9217\n",
      "Epoch 00040: val_loss did not improve from 0.17750\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1903 - accuracy: 0.9217 - val_loss: 0.1961 - val_accuracy: 0.9297\n",
      "Epoch 41/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9253\n",
      "Epoch 00041: val_loss improved from 0.17750 to 0.17135, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1883 - accuracy: 0.9252 - val_loss: 0.1714 - val_accuracy: 0.9259\n",
      "Epoch 42/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9173\n",
      "Epoch 00042: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1949 - accuracy: 0.9173 - val_loss: 0.6655 - val_accuracy: 0.8527\n",
      "Epoch 43/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9288\n",
      "Epoch 00043: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1846 - accuracy: 0.9274 - val_loss: 0.3786 - val_accuracy: 0.8660\n",
      "Epoch 44/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9224\n",
      "Epoch 00044: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1886 - accuracy: 0.9224 - val_loss: 0.3141 - val_accuracy: 0.8945\n",
      "Epoch 45/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9343\n",
      "Epoch 00045: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1672 - accuracy: 0.9347 - val_loss: 0.1908 - val_accuracy: 0.9287\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9336\n",
      "Epoch 00046: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1695 - accuracy: 0.9341 - val_loss: 0.2296 - val_accuracy: 0.9259\n",
      "Epoch 47/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9385\n",
      "Epoch 00047: val_loss did not improve from 0.17135\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1651 - accuracy: 0.9388 - val_loss: 0.1823 - val_accuracy: 0.9259\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1695 - accuracy: 0.9352\n",
      "Epoch 00048: val_loss improved from 0.17135 to 0.15569, saving model to best_model_11.h5\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1710 - accuracy: 0.9344 - val_loss: 0.1557 - val_accuracy: 0.9354\n",
      "Epoch 49/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9359\n",
      "Epoch 00049: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1646 - accuracy: 0.9357 - val_loss: 0.1602 - val_accuracy: 0.9411\n",
      "Epoch 50/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9314\n",
      "Epoch 00050: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1738 - accuracy: 0.9322 - val_loss: 0.1798 - val_accuracy: 0.9363\n",
      "Epoch 51/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9356\n",
      "Epoch 00051: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1630 - accuracy: 0.9350 - val_loss: 0.1560 - val_accuracy: 0.9496\n",
      "Epoch 52/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9372\n",
      "Epoch 00052: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1622 - accuracy: 0.9373 - val_loss: 0.2037 - val_accuracy: 0.9240\n",
      "Epoch 53/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9378\n",
      "Epoch 00053: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1635 - accuracy: 0.9376 - val_loss: 0.3220 - val_accuracy: 0.8726\n",
      "Epoch 54/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.9452\n",
      "Epoch 00054: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1417 - accuracy: 0.9449 - val_loss: 0.2395 - val_accuracy: 0.9154\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9404\n",
      "Epoch 00055: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1655 - accuracy: 0.9392 - val_loss: 0.1899 - val_accuracy: 0.9297\n",
      "Epoch 56/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9401\n",
      "Epoch 00056: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1534 - accuracy: 0.9404 - val_loss: 0.2102 - val_accuracy: 0.9221\n",
      "Epoch 57/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9385\n",
      "Epoch 00057: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1575 - accuracy: 0.9395 - val_loss: 0.1674 - val_accuracy: 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9452\n",
      "Epoch 00058: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1489 - accuracy: 0.9452 - val_loss: 0.2503 - val_accuracy: 0.9040\n",
      "Epoch 59/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9459\n",
      "Epoch 00059: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1421 - accuracy: 0.9458 - val_loss: 0.2007 - val_accuracy: 0.9268\n",
      "Epoch 60/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9461\n",
      "Epoch 00060: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1478 - accuracy: 0.9461 - val_loss: 0.1650 - val_accuracy: 0.9354\n",
      "Epoch 61/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1374 - accuracy: 0.9481\n",
      "Epoch 00061: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1372 - accuracy: 0.9480 - val_loss: 0.1846 - val_accuracy: 0.9335\n",
      "Epoch 62/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1426 - accuracy: 0.9462\n",
      "Epoch 00062: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 21ms/step - loss: 0.1409 - accuracy: 0.9471 - val_loss: 0.1877 - val_accuracy: 0.9392\n",
      "Epoch 63/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1460 - accuracy: 0.9414\n",
      "Epoch 00063: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1450 - accuracy: 0.9417 - val_loss: 0.1721 - val_accuracy: 0.9354\n",
      "Epoch 64/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9436\n",
      "Epoch 00064: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1408 - accuracy: 0.9436 - val_loss: 0.2341 - val_accuracy: 0.9040\n",
      "Epoch 65/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9484\n",
      "Epoch 00065: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1430 - accuracy: 0.9484 - val_loss: 0.1868 - val_accuracy: 0.9240\n",
      "Epoch 66/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1415 - accuracy: 0.9410\n",
      "Epoch 00066: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1410 - accuracy: 0.9414 - val_loss: 0.2063 - val_accuracy: 0.9154\n",
      "Epoch 67/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9510\n",
      "Epoch 00067: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1251 - accuracy: 0.9512 - val_loss: 0.1627 - val_accuracy: 0.9373\n",
      "Epoch 68/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9552\n",
      "Epoch 00068: val_loss did not improve from 0.15569\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1239 - accuracy: 0.9553 - val_loss: 0.2065 - val_accuracy: 0.9183\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 480, 94)           658       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 160, 94)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 160, 14)           3962      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 53, 14)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 53, 93)            3999      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 17, 93)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 93)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 141)               132540    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 13)                1846      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 60)                840       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 134)               8174      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 135       \n",
      "=================================================================\n",
      "Total params: 152,154\n",
      "Trainable params: 152,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "33/33 - 0s - loss: 0.1557 - accuracy: 0.9354\n",
      "27/27 - 0s - loss: 0.0961 - accuracy: 0.9597\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.1556856483221054\n",
      "test_loss: 0.09605173021554947\n",
      "test_acc: 0.9597156643867493\n",
      "precision: 0.9154929577464789\n",
      "recall: 0.9241706161137441\n",
      "specificity 0.9715639810426541\n",
      "sensitivity :  0.9241706161137441\n",
      "far 0.02843601895734597\n",
      "frr 0.07582938388625593\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 94, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 14, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 93, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.6429100310306516)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(141)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(13, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(60, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(134, activation = 'relu')(dense_2)\n",
    "dense_4 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_3)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_4)\n",
    "\n",
    "# Adam\n",
    "# model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.0007892455796971044), metrics = ['accuracy'])\n",
    "# SGD\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.08119450015184537, momentum=0.4770521140454941), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "        \n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
