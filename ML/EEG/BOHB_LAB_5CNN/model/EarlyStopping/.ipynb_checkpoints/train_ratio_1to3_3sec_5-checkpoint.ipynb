{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 4\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 2s 19ms/step - loss: 0.5646 - accuracy: 0.7541 - val_loss: 0.5701 - val_accuracy: 0.7367\n",
      "Epoch 2/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.5038 - accuracy: 0.7532\n",
      "Epoch 00002: val_loss improved from inf to 0.51517, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.5015 - accuracy: 0.7544 - val_loss: 0.5152 - val_accuracy: 0.7367\n",
      "Epoch 3/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.8561\n",
      "Epoch 00003: val_loss improved from 0.51517 to 0.31978, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 17ms/step - loss: 0.3341 - accuracy: 0.8561 - val_loss: 0.3198 - val_accuracy: 0.8641\n",
      "Epoch 4/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.8821\n",
      "Epoch 00004: val_loss did not improve from 0.31978\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.2911 - accuracy: 0.8821 - val_loss: 0.3256 - val_accuracy: 0.8574\n",
      "Epoch 5/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9024\n",
      "Epoch 00005: val_loss improved from 0.31978 to 0.22879, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.2455 - accuracy: 0.9021 - val_loss: 0.2288 - val_accuracy: 0.9173\n",
      "Epoch 6/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9066\n",
      "Epoch 00006: val_loss improved from 0.22879 to 0.21756, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.2288 - accuracy: 0.9075 - val_loss: 0.2176 - val_accuracy: 0.9211\n",
      "Epoch 7/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9243\n",
      "Epoch 00007: val_loss improved from 0.21756 to 0.18996, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.2020 - accuracy: 0.9243 - val_loss: 0.1900 - val_accuracy: 0.9287\n",
      "Epoch 8/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9189\n",
      "Epoch 00008: val_loss did not improve from 0.18996\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.2093 - accuracy: 0.9189 - val_loss: 0.2328 - val_accuracy: 0.9116\n",
      "Epoch 9/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9230\n",
      "Epoch 00009: val_loss did not improve from 0.18996\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1912 - accuracy: 0.9230 - val_loss: 0.2448 - val_accuracy: 0.8954\n",
      "Epoch 10/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9316\n",
      "Epoch 00010: val_loss did not improve from 0.18996\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1796 - accuracy: 0.9319 - val_loss: 0.1905 - val_accuracy: 0.9240\n",
      "Epoch 11/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9349\n",
      "Epoch 00011: val_loss did not improve from 0.18996\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1690 - accuracy: 0.9331 - val_loss: 0.2031 - val_accuracy: 0.9173\n",
      "Epoch 12/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9388\n",
      "Epoch 00012: val_loss did not improve from 0.18996\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1599 - accuracy: 0.9392 - val_loss: 0.1997 - val_accuracy: 0.9316\n",
      "Epoch 13/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9382\n",
      "Epoch 00013: val_loss improved from 0.18996 to 0.16443, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1648 - accuracy: 0.9382 - val_loss: 0.1644 - val_accuracy: 0.9401\n",
      "Epoch 14/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9467\n",
      "Epoch 00014: val_loss did not improve from 0.16443\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.1475 - accuracy: 0.9468 - val_loss: 0.1860 - val_accuracy: 0.9382\n",
      "Epoch 15/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9433\n",
      "Epoch 00015: val_loss did not improve from 0.16443\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1519 - accuracy: 0.9433 - val_loss: 0.1664 - val_accuracy: 0.9316\n",
      "Epoch 16/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9375\n",
      "Epoch 00016: val_loss did not improve from 0.16443\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1566 - accuracy: 0.9373 - val_loss: 0.1836 - val_accuracy: 0.9354\n",
      "Epoch 17/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9394\n",
      "Epoch 00017: val_loss improved from 0.16443 to 0.15933, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1482 - accuracy: 0.9398 - val_loss: 0.1593 - val_accuracy: 0.9496\n",
      "Epoch 18/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9426\n",
      "Epoch 00018: val_loss did not improve from 0.15933\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1522 - accuracy: 0.9426 - val_loss: 0.1597 - val_accuracy: 0.9420\n",
      "Epoch 19/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9449\n",
      "Epoch 00019: val_loss did not improve from 0.15933\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1441 - accuracy: 0.9449 - val_loss: 0.1986 - val_accuracy: 0.9135\n",
      "Epoch 20/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9461\n",
      "Epoch 00020: val_loss did not improve from 0.15933\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1324 - accuracy: 0.9455 - val_loss: 0.1795 - val_accuracy: 0.9411\n",
      "Epoch 21/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9439\n",
      "Epoch 00021: val_loss did not improve from 0.15933\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1454 - accuracy: 0.9439 - val_loss: 0.1905 - val_accuracy: 0.9354\n",
      "Epoch 22/200\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 0.1444 - accuracy: 0.9431\n",
      "Epoch 00022: val_loss did not improve from 0.15933\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1457 - accuracy: 0.9433 - val_loss: 0.1643 - val_accuracy: 0.9439\n",
      "Epoch 23/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9604\n",
      "Epoch 00023: val_loss improved from 0.15933 to 0.14589, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1172 - accuracy: 0.9601 - val_loss: 0.1459 - val_accuracy: 0.9458\n",
      "Epoch 24/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9508\n",
      "Epoch 00024: val_loss did not improve from 0.14589\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1322 - accuracy: 0.9503 - val_loss: 0.1630 - val_accuracy: 0.9354\n",
      "Epoch 25/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.1323 - accuracy: 0.9469\n",
      "Epoch 00025: val_loss did not improve from 0.14589\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1297 - accuracy: 0.9484 - val_loss: 0.1547 - val_accuracy: 0.9468\n",
      "Epoch 26/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9536\n",
      "Epoch 00026: val_loss did not improve from 0.14589\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.1191 - accuracy: 0.9537 - val_loss: 0.1486 - val_accuracy: 0.9458\n",
      "Epoch 27/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9544\n",
      "Epoch 00027: val_loss improved from 0.14589 to 0.14427, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1206 - accuracy: 0.9544 - val_loss: 0.1443 - val_accuracy: 0.9487\n",
      "Epoch 28/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9613\n",
      "Epoch 00028: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1027 - accuracy: 0.9607 - val_loss: 0.1664 - val_accuracy: 0.9430\n",
      "Epoch 29/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9499\n",
      "Epoch 00029: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1209 - accuracy: 0.9499 - val_loss: 0.1771 - val_accuracy: 0.9335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1048 - accuracy: 0.9579\n",
      "Epoch 00030: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1046 - accuracy: 0.9582 - val_loss: 0.1722 - val_accuracy: 0.9458\n",
      "Epoch 31/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9570\n",
      "Epoch 00031: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1126 - accuracy: 0.9566 - val_loss: 0.1762 - val_accuracy: 0.9411\n",
      "Epoch 32/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1039 - accuracy: 0.9585\n",
      "Epoch 00032: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 2s 16ms/step - loss: 0.1046 - accuracy: 0.9579 - val_loss: 0.1675 - val_accuracy: 0.9382\n",
      "Epoch 33/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9630\n",
      "Epoch 00033: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0992 - accuracy: 0.9629 - val_loss: 0.1736 - val_accuracy: 0.9363\n",
      "Epoch 34/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1099 - accuracy: 0.9584\n",
      "Epoch 00034: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.1097 - accuracy: 0.9585 - val_loss: 0.1589 - val_accuracy: 0.9477\n",
      "Epoch 35/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1065 - accuracy: 0.9547\n",
      "Epoch 00035: val_loss did not improve from 0.14427\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1067 - accuracy: 0.9547 - val_loss: 0.3531 - val_accuracy: 0.8812\n",
      "Epoch 36/200\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 0.1028 - accuracy: 0.9576\n",
      "Epoch 00036: val_loss improved from 0.14427 to 0.14267, saving model to best_model_5.h5\n",
      "99/99 [==============================] - 2s 15ms/step - loss: 0.1017 - accuracy: 0.9579 - val_loss: 0.1427 - val_accuracy: 0.9496\n",
      "Epoch 37/200\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 0.0862 - accuracy: 0.9645\n",
      "Epoch 00037: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.0850 - accuracy: 0.9651 - val_loss: 0.1588 - val_accuracy: 0.9449\n",
      "Epoch 38/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9648\n",
      "Epoch 00038: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0931 - accuracy: 0.9648 - val_loss: 0.1506 - val_accuracy: 0.9487\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9617\n",
      "Epoch 00039: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0880 - accuracy: 0.9623 - val_loss: 0.1949 - val_accuracy: 0.9468\n",
      "Epoch 40/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0808 - accuracy: 0.9694\n",
      "Epoch 00040: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0804 - accuracy: 0.9696 - val_loss: 0.1654 - val_accuracy: 0.9496\n",
      "Epoch 41/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9668\n",
      "Epoch 00041: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.0937 - accuracy: 0.9667 - val_loss: 0.1591 - val_accuracy: 0.9487\n",
      "Epoch 42/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9633\n",
      "Epoch 00042: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0981 - accuracy: 0.9629 - val_loss: 0.2108 - val_accuracy: 0.9278\n",
      "Epoch 43/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.9656\n",
      "Epoch 00043: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0866 - accuracy: 0.9658 - val_loss: 0.1611 - val_accuracy: 0.9496\n",
      "Epoch 44/200\n",
      "95/99 [===========================>..] - ETA: 0s - loss: 0.0788 - accuracy: 0.9684\n",
      "Epoch 00044: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.0781 - accuracy: 0.9693 - val_loss: 0.1656 - val_accuracy: 0.9525\n",
      "Epoch 45/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9668\n",
      "Epoch 00045: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 15ms/step - loss: 0.0872 - accuracy: 0.9674 - val_loss: 0.2184 - val_accuracy: 0.9335\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9652\n",
      "Epoch 00046: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0855 - accuracy: 0.9655 - val_loss: 0.1861 - val_accuracy: 0.9439\n",
      "Epoch 47/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9749\n",
      "Epoch 00047: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0683 - accuracy: 0.9743 - val_loss: 0.2334 - val_accuracy: 0.9287\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9762\n",
      "Epoch 00048: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0669 - accuracy: 0.9759 - val_loss: 0.1737 - val_accuracy: 0.9458\n",
      "Epoch 49/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9767\n",
      "Epoch 00049: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0631 - accuracy: 0.9769 - val_loss: 0.1785 - val_accuracy: 0.9496\n",
      "Epoch 50/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9639\n",
      "Epoch 00050: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0883 - accuracy: 0.9639 - val_loss: 0.1546 - val_accuracy: 0.9477\n",
      "Epoch 51/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9805\n",
      "Epoch 00051: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0603 - accuracy: 0.9804 - val_loss: 0.1809 - val_accuracy: 0.9515\n",
      "Epoch 52/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9781\n",
      "Epoch 00052: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0586 - accuracy: 0.9781 - val_loss: 0.1920 - val_accuracy: 0.9325\n",
      "Epoch 53/200\n",
      "96/99 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9759\n",
      "Epoch 00053: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0584 - accuracy: 0.9756 - val_loss: 0.1907 - val_accuracy: 0.9468\n",
      "Epoch 54/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9729\n",
      "Epoch 00054: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 13ms/step - loss: 0.0696 - accuracy: 0.9734 - val_loss: 0.1776 - val_accuracy: 0.9449\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0572 - accuracy: 0.9797\n",
      "Epoch 00055: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0575 - accuracy: 0.9794 - val_loss: 0.2003 - val_accuracy: 0.9515\n",
      "Epoch 56/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9752\n",
      "Epoch 00056: val_loss did not improve from 0.14267\n",
      "99/99 [==============================] - 1s 14ms/step - loss: 0.0643 - accuracy: 0.9747 - val_loss: 0.1836 - val_accuracy: 0.9468\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 480, 26)           182       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 160, 26)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 160, 48)           3792      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 53, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 53, 32)            4640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 17, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 17, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 101)               54136     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 816       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 88)                792       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 64,447\n",
      "Trainable params: 64,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 - 0s - loss: 0.1427 - accuracy: 0.9496\n",
      "27/27 - 0s - loss: 0.2322 - accuracy: 0.9005\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAG5CAYAAACnXrwnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAivUlEQVR4nO3debweZXnw8d9FFsKaEEAakmBQAi2lghQQRFGEtwLakvJaBFsNGE1RRKu+KlTrin21WhEV0ShLsIKgQqGAgAZ5WcoiiIbNQorQBMKSjT0hOed6/3jmhAc8OUvyLGdmft/PZz6ZueeemfvE4HWua+6ZicxEkiSVx0bdHoAkSRoeg7ckSSVj8JYkqWQM3pIklYzBW5KkkjF4S5JUMgZvSYOKiAci4uAh9JsWERkRozsxLqmuDN4qlYh4R0TcGhFPR8TiiPhZRLyu2PfZInAc2dR/dNE2rdg+u9jep6nPThEx7BceRMQ1EfGedeybFRG/i4inIuLRiLg8IrYoxvt0sayOiOebtr8TEW8sxnfRS863e9F+zXDHKal6DN4qjYj4CPB14J+B7YAdgG8Dhzd1WwZ8LiJGDXCqZcDJQ7zmMRFx9jDH+YZijEdn5hbAnwDnA2TmoZm5eWZuDvwQ+Je+7cw8rjjF48B+EbF102lnAvcOZxySqsvgrVKIiPHA54HjM/PCzHwmM1dn5n9k5seaul4BPA/83QCnmwu8qgiy7bA3cGNm3g6Qmcsyc25mPjXE458H/h04CqD4ReTtNIJ9v5rK1cdGxMKIWB4Rx0XE3hExPyJWRMS3mvpvFBGfiogHI+KxiDin+Dvu2//OYt/SiPjkS661UUScGBH/Xey/ICImDvFnk9QCBm+VxX7AOOCiQfol8E/AZyJizDr6PEsjM/5i64b3IjcDb46Iz0XE/hGx8Xqc4xzgXcX6m4E7gYeHcNxrgOk0gv3XgU8CBwN/ChzZ9AvLMcVyIPAKYHPgWwARsStwOvBOYHtga2BK0zVOAGYAbyj2LwdOG+bPJ2kDGLxVFlsDSzJzzWAdM/MSGqXnfu9HF74L7BARh7ZofM3Xvw44AtgTuAxYGhFfG6SU/9Jz/CcwMSJ2oRHEzxnioV/IzJWZeRXwDHBeZj6WmQ8B1wGvLvr9LfC1zLw/M58GTgKOKiaavQ24NDOvzcxVNH4Z6m26xnHAJzNzUbH/s8DbnKQmdY7BW2WxFNhmGAHiUzSyznH97SyCzheK5UUi4ttFmXkFjXvq7+jbjoj5Q7l4Zv4sM/8SmEjjnvwxDPzLRH9+AHyARnY8WMWhz6NN68/1s715sb498GDTvgeB0TTmEmwPLOzbkZnP0Pj77/Ny4KKmv6N7gJ7iWEkdYPBWWdwIrKJRrh1UZv4cWAC8f4BuZwETaGTJzce+PzMnZOaE4vhz+7Yz81XDGXRm9mbmPOBqYLfhHEsjeL8fuDwznx3msYN5mEYQ7rMDsIZGsF8MTO3bERGb0qh89FkIHNr0dzIhM8cV2b2kDjB4qxQy8wng08BpETEjIjaNiDERcWhE/Ms6Dvsk8PEBzrkG+AzwiQ0Y2uiIGNe0jImIwyPiqIjYKhr2oXF/+KbhnDgzf18c98nB+q6H84APR8SOEbE5jTkA5xd/Jz8B3hoRr4uIsTQmCjb/f8V3gC9GxMsBImLbiDgcSR1j8FZpZOa/Ah+hURJ/nEYG+AEaM7P7638DcMsgpz2PRqa5vk6nUY7uW86iMYHrvcB9wJPAvwFfycx1zhZfl8y8PjOHMlFtuM6kkdlfC/weWEljIhqZeRdwPHAujb+b5cCipmNPBS4BroqIp2j8UvKaNoxR0jpE5rDfTSFJkrrIzFuSpJIxeEuSVDIGb0mSSsbgLUlSyRi8JUkqGYN3zUXEIRHxXxGxICJO7PZ4pE6IiDOLD7Lc2e2xSOvD4F1jxbu2TwMOBXYFji4+SiFV3dnAId0ehLS+DN71tg+woPg4xfPAj3jxt7GlSsrMa2l8110qJYN3vU2m6QMUNN6iNblLY5EkDZHBW5KkkjF419tDNH09CphStEmSRjCDd739CphefFlqLHAUjQ9OSJJGMIN3jRWff/wAcCVwD3BB8UUpqdIi4jwa34jfJSIWRcSsbo9JGg6/KiZJUsmYeUuSVDIGb0mSSsbgLUlSyRi8JUkqGYO3AIiI2d0eg9Rp/rtXWRm81cf/E1Md+e9epWTwliSpZEbUc97bTByV06aO6fYwaunxpT1su/Wobg+jlu6dv2m3h1Bbq1nFGDbu9jBqaSXP8Hyuik5d780HbpZLl/W05Fy3zV91ZWZ29ZOyo7t58ZeaNnUMt1w5dfCOUoW8efs9uj0EqeNuznkdvd7SZT3ccuUOLTnXqEn3bdOSE22AERW8JUlqhwR66e32MFrGe96SJJWMmbckqQaSnqxO5m3wliRVXqNsPnImaG8oy+aSJJWMmbckqRaqNGHN4C1Jqrwk6RlB7zXZUJbNJUkqGTNvSVItVGnCmsFbklR5CfRUKHhbNpckqWTMvCVJtVClsrmZtySp8hLoyWzJMhQRMSEifhIRv4uIeyJiv4iYGBE/j4j7ij+3KvpGRHwjIhZExPyI2HOw8xu8JUlqvVOBKzLzj4HdgXuAE4F5mTkdmFdsAxwKTC+W2cDpg53c4C1JqoXeFi2DiYjxwAHAGQCZ+XxmrgAOB+YW3eYCM4r1w4FzsuEmYEJETBroGt7zliRVXpKtnG2+TUTc2rQ9JzPnNG3vCDwOnBURuwO3AR8CtsvMxUWfR4DtivXJwMKm4xcVbYtZB4O3JEnDsyQz9xpg/2hgT+CEzLw5Ik7lhRI5AJmZEbHev01YNpckVV9CT4uWIVgELMrMm4vtn9AI5o/2lcOLPx8r9j8ETG06fkrRtk4Gb0lS5TU+CdqZe96Z+QiwMCJ2KZoOAu4GLgFmFm0zgYuL9UuAdxWzzvcFnmgqr/fLsrkkSa13AvDDiBgL3A8cSyNhviAiZgEPAkcWfS8HDgMWAM8WfQdk8JYk1UDQQ3Tsapn5G6C/++IH9dM3geOHc36DtySp8hLorc4L1rznLUlS2Zh5S5JqoZNl83YzeEuSKq/xSdDqBG/L5pIklYyZtySpFnqzOpm3wVuSVHmWzSVJUleZeUuSKi8JeiqUrxq8JUm14D1vSZJKxHvekiSpq8y8JUk1EPRkdfJVg7ckqfIa3/OuTvCuzk8iSVJNmHlLkmqhShPWDN6SpMrLrNY97+r8JJIk1YSZtySpFnotm0uSVB6Nl7RUp9hcnZ9EkqSaMPOWJNVAtSasGbwlSZXnS1okSVJXmXlLkmqhx0+CSpJUHkk421ySJHWPmbckqRZ6nW0uSVJ5+JIWSZLUVWbekqTKS8LZ5pIklY0vaZEkSV1j5i1JqrxMfLe5JEnlEpX6nnd1fg2RJKkmzLwlSZWXWDaXJKl0fEmLJEnqGjNvSVLlJUGvL2mRJKlcLJtLkqSuMfOWJFVe4idBJUkqmaDHl7RIkqRuMfOWJFWeZXNJkkrIsrkkSeoaM29JUuVlhmVzSZLKpkofJqnOTyJJUk2YeUuSKi+B3gpNWDN4S5JqIDpaNo+IB4CngB5gTWbuFRETgfOBacADwJGZuTwiAjgVOAx4FjgmM3890Pktm0uS1B4HZuYemblXsX0iMC8zpwPzim2AQ4HpxTIbOH2wExu8JUmV13hJS7Rk2QCHA3OL9bnAjKb2c7LhJmBCREwa6ESWzSVJtdDCT4JuExG3Nm3Pycw5L+mTwFURkcB3i/3bZebiYv8jwHbF+mRgYdOxi4q2xayDwVuSpOFZ0lQKX5fXZeZDEfEy4OcR8bvmnZmZRWBfLwZvSVLlJRtc8h7e9TIfKv58LCIuAvYBHo2ISZm5uCiLP1Z0fwiY2nT4lKJtnbznLUmqhV42askymIjYLCK26FsH/gK4E7gEmFl0mwlcXKxfArwrGvYFnmgqr/fLzFuSpNbaDrio8QQYo4FzM/OKiPgVcEFEzAIeBI4s+l9O4zGxBTQeFTt2sAsYvCVJlZcJPR0qm2fm/cDu/bQvBQ7qpz2B44dzDYO3JKkWOnnPu9285y1JUsmYeUuSKq8x27w6+arBW5JUCz1+mESVFFsQ4/8ZRk8HIJ84kRj3Ztj4QMjV0PM/5BMnQj4F4/6K2Ow9Lxw7ehdy6QxYc093xi5tgM3Gb8pHvvc+pu02FTL56qzTueeme7s9LLVQ3+tRq8LgrbViy0+Rq66FFScAYyDGkatugKe+CvQQm3+M2Ow48umvwMpLyJWXNA4cvTMx4XQDt0rr/V8/lluvvJ0vHPmvjB4zmo03HdvtIUkDqs4NAG2Y2BzG7A3P/bhoWN3IsJ+/nsYX7SBX/wZG/dEfHjrurbDy0o4NVWqlTbfclD87YFd+dsbVAKxZvYZnnni2y6NS6zXuebdiGQnMvNUwair0LiPGfxlG/zGsvpN86mTI59Z2iU3eRq687A+PHfcWcsVxHRys1DqTdnwZTzz+JB8783hesfvLue/X9/PtD53FymdXdXtoarHeCt3zbuuvEBFxSET8V0QsiIgTBz9C3TMKxvwp+ey55NLDIZ8jNvv7F3Zv9j5gDfSVyvuM2b0R4Nfc19HRSq0yavRGTN9zR/7jO1fyvj//OCufWcXbT5zR7WFJA2pb8I6IUcBpND4yvitwdETs2q7raQP1PtJYVv8WgFx5BYz+08a+TY4gNj6QXPHRPzgsxr2FtGSuEnt80TIeX7SU392yAIBrf3Ij01/9ii6PSq3W94a1ViwjQTsz732ABZl5f2Y+D/yIxgfHNRL1LoGexTBqRwBi4/2gZwGMfT2x2XvJ5ccBK19yUMC4Q6G/UrpUEssfXcHjC5cyZeftAXj1QX/Gg/cs6vKo1A7e8x6a/j4u/pqXdoqI2cBsgB0mewu+m/LJLxAT/hUYAz0LG4+KbX0hxFhi4tmNTqt/Qz756cb62L2h5xHoWbiuU0qlcNoHz+Skf/sgo8eOZvH9j/LVd3+720OSBtT1aJmZc4A5AHvtPm69P0yuFlhzD7n0iBc15ZKD193/+VvIZX/T5kFJ7fffv32A4/dxWk6Vdfp73u3WzuA97I+LS5LULs42H5pfAdMjYseIGAscReOD45IkaQO0LfPOzDUR8QHgSmAUcGZm3tWu60mStC6+HnUYMvNy4PJ2XkOSpKEYKTPFW6E6P4kkSTXR9dnmkiS1XTrbXJKkUkmcbS5JkrrIzFuSVAuWzSVJKpGqPSpm2VySpJIx85Yk1UKVMm+DtySp8qr2YRLL5pIklYyZtySpFqr0nLfBW5JUfVmte96WzSVJKhkzb0lS5VXtOW+DtySpFqoUvC2bS5JUMmbekqTKq9pz3gZvSVItZIWCt2VzSZJKxsxbklQLvqRFkqQSSV/SIkmSusnMW5JUC1WasGbwliTVQLUeFbNsLklSyZh5S5JqwbK5JEklUrUPk1g2lySpZMy8JUnVl41nvavC4C1JqoUqvWHNsrkkSSVj5i1JqrzE2eaSJJWML2mRJEldZOYtSaqFKs02N/OWJNVCZrRkGYqIGBURt0fEpcX2jhFxc0QsiIjzI2Js0b5xsb2g2D9tKOc3eEuS1HofAu5p2v4ycEpm7gQsB2YV7bOA5UX7KUW/QRm8JUmVl9m5zDsipgBvAb5fbAfwJuAnRZe5wIxi/fBim2L/QUX/AXnPW5JUCy2cbb5NRNzatD0nM+c0bX8d+DiwRbG9NbAiM9cU24uAycX6ZGAhQGauiYgniv5LBhqAwVuSpOFZkpl79bcjIt4KPJaZt0XEG9s1AIO3JKkWOjTbfH/gryLiMGAcsCVwKjAhIkYX2fcU4KGi/0PAVGBRRIwGxgNLB7uI97wlSbXQiXvemXlSZk7JzGnAUcDVmfm3wC+BtxXdZgIXF+uXFNsU+6/OHPzXDDNvSVLlJUN/zKtNPgH8KCJOBm4HzijazwB+EBELgGU0Av6gDN6SJLVBZl4DXFOs3w/s00+flcDfDPfcBm9JUi1U6AVrBm9JUg1ktb4q5oQ1SZJKxsxbklQPFaqbG7wlSbVg2VySJHWNmbckqRaq9D1vg7ckqfISy+aSJKmLzLwlSdWXQIUyb4O3JKkWqnTP27K5JEklY+YtSaqHCmXeBm9JUg10/ZOgLWXZXJKkkjHzliTVg2VzSZJKxE+CSpKkbjLzliTVg2VzSZLKxrK5JEnqEjNvSVI9WDaXJKlkKhS8LZtLklQyZt6SpOrzk6CSJJWPnwSVJEldY+YtSaqHCmXeBm9JUj1U6J63ZXNJkkrGzFuSVAth2VySpBJJKnXP27K5JEkls87MOyK+yQC/p2TmB9syIkmSWi4qNWFtoLL5rR0bhSRJ7Vahsvk6g3dmzu3kQCRJ0tAMOmEtIrYFPgHsCozra8/MN7VxXJIktVaFMu+hTFj7IXAPsCPwOeAB4FdtHJMkSa2XLVpGgKEE760z8wxgdWb+v8x8N2DWLUlSlwzlOe/VxZ+LI+ItwMPAxPYNSZKkFqvhJ0FPjojxwEeBbwJbAh9u66gkSWqxWr1hLTMvLVafAA5s73AkSdJghjLb/Cz6uUVf3PuWJKkc6pR5A5c2rY8D/prGfW9JktQFQymb/7R5OyLOA65v24gkSdKA1uerYtOBl7V6IAD33bsVhx34tnacWhqx1hw0vttDkDoub76x49es1YS1iHiKF98peITGG9ckSSqPOj0qlplbdGIgkiRpaAZ9w1pEzBtKmyRJI1arXo06QkrvA33PexywKbBNRGwF9NUbtgQmd2BskiS1zggJvK0wUNn874F/ALYHbuOF4P0k8K32DkuSpNaqxYS1zDwVODUiTsjMb3ZwTJIkaQBD+apYb0RM6NuIiK0i4v3tG5IkSW1QoXveQwne783MFX0bmbkceG/bRiRJUjt0KHhHxLiIuCUifhsRd0XE54r2HSPi5ohYEBHnR8TYon3jYntBsX/aYNcYSvAeFRFrH46LiFHA2CEcJ0lSHa0C3pSZuwN7AIdExL7Al4FTMnMnYDkwq+g/C1hetJ9S9BvQUIL3FcD5EXFQRBwEnAf8bLg/iSRJ3RLZumUw2fB0sTmmWBJ4E/CTon0uMKNYP7zYpth/UHPS3J+hvB71E8Bs4Lhiez7wR0M4TpKkkaN1b1jbJiJubdqek5lzmjsUVerbgJ2A04D/BlZk5pqiyyJeeOx6MrAQIDPXRMQTwNbAknUNYChvWOuNiJuBVwJHAtsAPx34KEmSKmtJZu41UIfM7AH2KCZ8XwT8cSsHMNBLWnYGji6WJcD5xYAObOUAJEnqiC7MFM/MFRHxS2A/YEJEjC6y7ynAQ0W3h4CpwKKIGA2MB5YOdN6B7nn/jkZ9/q2Z+briWe+eDfw5JEnqik7d846IbfsesY6ITYD/BdwD/BLo+3TmTODiYv2SYpti/9WZOeCVBiqbHwEcBfwyIq4AfsQLb1mTJEn9mwTMLe57bwRckJmXRsTdwI8i4mTgduCMov8ZwA8iYgGwjEbsHdBAb1j7d+DfI2IzGjPh/gF4WUScDlyUmVet948lSVKndahsnpnzgVf3034/sE8/7SuBvxnONQZ9VCwzn8nMczPzL2nU6G/H73lLksqkg4+KdcJQnvNeKzOXZ+aczDyoXQOSJEkDG8pz3pIkld8IyZpbweAtSaqHCgXvYZXNJUlS95l5S5JqYaRMNmsFM29JkkrG4C1JUslYNpck1UOFyuYGb0lS9Y2gF6y0gmVzSZJKxsxbklQPFcq8Dd6SpHqoUPC2bC5JUsmYeUuSKi+o1oQ1g7ckqR4qFLwtm0uSVDJm3pKk6qvYc94Gb0lSPVQoeFs2lySpZMy8JUn1UKHM2+AtSaqFKt3ztmwuSVLJmHlLkuqhQpm3wVuSVH1JpYK3ZXNJkkrGzFuSVAtVmrBm8JYk1UOFgrdlc0mSSsbMW5JUC5bNJUkqmwoFb8vmkiSVjJm3JKn6Kvact8FbklR5USxVYdlckqSSMfOWJNWDZXNJksqlSo+KWTaXJKlkzLwlSfVQoczb4C1JqocKBW/L5pIklYyZtySp+rJaE9YM3pKkejB4S5JULlXKvL3nLUlSyZh5S5LqoUKZt8FbklQLls0lSVLXmHlLkqrP73lLklRCFQrels0lSSoZM29JUuUFTliTJKl8skXLICJiakT8MiLujoi7IuJDRfvEiPh5RNxX/LlV0R4R8Y2IWBAR8yNiz8GuYfCWJKm11gAfzcxdgX2B4yNiV+BEYF5mTgfmFdsAhwLTi2U2cPpgFzB4S5JqITJbsgwmMxdn5q+L9aeAe4DJwOHA3KLbXGBGsX44cE423ARMiIhJA13D4C1Jqr5WlcwbsXubiLi1aZm9rstGxDTg1cDNwHaZubjY9QiwXbE+GVjYdNiiom2dnLAmSdLwLMnMvQbrFBGbAz8F/iEzn4yItfsyMyPWfwqdwVuSVAudnG0eEWNoBO4fZuaFRfOjETEpMxcXZfHHivaHgKlNh08p2tbJsrkkqR46N9s8gDOAezLza027LgFmFuszgYub2t9VzDrfF3iiqbzeLzNvSZJaa3/gncAdEfGbou0fgS8BF0TELOBB4Mhi3+XAYcAC4Fng2MEuYPCWJNVCp8rmmXk9jffC9OegfvoncPxwrmHwliTVg29YkyRJ3WLmLUmqvqzWu80N3pKkeqhQ8LZsLklSyZh5S5Iqr2qfBDV4S5LqYQgfFSkLy+aSJJWMmbckqRYsm0uSVCZDfC95WVg2lySpZMy8tdaUadtw0teOXrv9R1Mn8oNv/oL5t9zPCZ+dwdixo+np6eVbn7+Ye+9Y1MWRSq3z8Y8exr6veSUrVjzLu2efAcCxM1/P/q+dTmayfMWzfPkrl7F06dPsv990jj3m9WRm47+Fb8/jzrv8b6EsorfbI2gdg7fWWvTAEo4/4psAbLRR8G/XnMR//uIuPvT5I/jhafO49bp72fuAXXjP/zmUj8/8XpdHK7XGFVfdwUUX38ZJH3/r2rbzf3wzZ829DoAjZvw57/q7/Tnl1Cu57fYHuOHG+wB4xY7b8plPzWDmLP9bKA3L5qq6PfbdicULl/LYwysgk0033xiAzTYfx9LHnuzu4KQWmn/HQp58auWL2p599vm16+PGjSGLR4xWrlz94vYqRQOVipm3+vWGw17FNZfNB+A7//dSvvi9d/Pejx1GbBR85B3f6fLopPabdewB/MXBu/HMM6v48MfOXdv+uv135r3vfgMTJmzKSZ/6cRdHqOGq0mzztmXeEXFmRDwWEXe26xpqj9FjRrHvm/6E6668A4C3HrUv3/3SpbzzTV/mu1+6jA+f/L+7PEKp/c4461re/rff5hdX38VfH/7na9uvv+FeZs76Hv/02Qt59zEHdHGEGpak8ZKWViwjQDvL5mcDh7Tx/GqTvV6/MwvufpgVS58G4OAZe3LDz+8C4Lor7mDnP5vSzeFJHfWLeXdzwOt2+YP2+XcsZNKkCWy55SZdGJXqrm3BOzOvBZa16/xqnze+ZXeuuey3a7eXPvYkr9p7RwD22PeVPPzg0m4NTeqIyZO3Wru+/2un8z8LG//mt99+wtr26Tttx5gxo3jyyec6PTytp8jWLCNB1+95R8RsYDbAuNFbdnk02niTMez52ul84zMXrW079dMXctw//iWjRm3E86vWcOqnL+ziCKXW+tQ//hV7vGoHxo/fhAvOfT9nn3M9r9nnlUydMpHeTB599ElOOfUKAA54/S68+eDdWNPTy6pVa/j8yRd3efQalhESeFshso31+4iYBlyambsNpf/4TSblftOOadt4pJFo1ZTx3R6C1HG33vwtnnpyUXTqeptvNTX3OPBDLTnXDRd97LbM3KslJ1tPXc+8JUlqNz8JKklS2YygmeKt0M5Hxc4DbgR2iYhFETGrXdeSJKlO2pZ5Z+bRg/eSJKkzLJtLklQ2FQrevttckqSSMfOWJNWCZXNJksokgd7qRG/L5pIklYyZtySpHqqTeBu8JUn1UKV73pbNJUkqGTNvSVI9VOj1qAZvSVItWDaXJEldY+YtSaq+xNnmkiSVSeN73tWJ3gZvSVI99HZ7AK3jPW9JkkrGzFuSVAuWzSVJKpOKTVizbC5JUsmYeUuSaiB9w5okSWXjG9YkSVLXmHlLkurBsrkkSSWSEL6kRZIkdYuZtySpHiybS5JUMtWJ3ZbNJUkqGzNvSVItVOnd5mbekqR6yGzNMoiIODMiHouIO5vaJkbEzyPivuLPrYr2iIhvRMSCiJgfEXsO5UcxeEuS1FpnA4e8pO1EYF5mTgfmFdsAhwLTi2U2cPpQLmDwliRVXwK9LVoGu1TmtcCylzQfDswt1ucCM5raz8mGm4AJETFpsGt4z1uSVHlBtvKe9zYRcWvT9pzMnDPIMdtl5uJi/RFgu2J9MrCwqd+iom0xAzB4S5I0PEsyc6/1PTgzM2LDPpNi8JYk1UN3Z5s/GhGTMnNxURZ/rGh/CJja1G9K0TYg73lLkuqhQ7PN1+ESYGaxPhO4uKn9XcWs832BJ5rK6+tk5i1JUgtFxHnAG2ncG18EfAb4EnBBRMwCHgSOLLpfDhwGLACeBY4dyjUM3pKk6uubbd6JS2UevY5dB/XTN4Hjh3sNg7ckqRZ8w5okSeoaM29JUj1UKPM2eEuSamCDZoqPOJbNJUkqGTNvSVL1JZXKvA3ekqR66NCjYp1g2VySpJIx85Yk1UKVnvM2eEuS6qFCwduyuSRJJWPmLUmqvgR6q5N5G7wlSTXgS1okSVIXmXlLkuqhQpm3wVuSVA8VCt6WzSVJKhkzb0lS9TnbXJKksknI6rzc3LK5JEklY+YtSaqHCk1YM3hLkqqvYve8LZtLklQyZt6SpHqwbC5JUslUKHhbNpckqWTMvCVJNVCtr4oZvCVJ1ZdAry9pkSRJXWLmLUmqB8vmkiSVjMFbkqQySd+wJkmSusfMW5JUfQlZoU+CGrwlSfVg2VySJHWLmbckqR6cbS5JUolk+oY1SZLUPWbekqR6sGwuSVK5pGVzSZLULWbekqQa8HvekiSVS+JLWiRJUveYeUuS6sF3m0uSVB4JpGVzSZLULWbekqTqy7RsLklS2Vg2lyRJXWPmLUmqhwqVzSNH0BtnIuJx4MFuj6OmtgGWdHsQUof57757Xp6Z23bqYhFxBY3/vVthSWYe0qJzrZcRFbzVPRFxa2bu1e1xSJ3kv3uVlfe8JUkqGYO3JEklY/BWnzndHoDUBf67VykZvAVAZvp/YgOIiJ6I+E1E3BkRP46ITTfgXGdHxNuK9e9HxK4D9H1jRLx2Pa7xQES0anJOZfnvXmVl8JaG5rnM3CMzdwOeB45r3hkR6/XYZWa+JzPvHqDLG4FhB29J1WbwlobvOmCnIiu+LiIuAe6OiFER8ZWI+FVEzI+IvweIhm9FxH9FxC+Al/WdKCKuiYi9ivVDIuLXEfHbiJgXEdNo/JLw4SLrf31EbBsRPy2u8auI2L84duuIuCoi7oqI7wPR4b8TSR3kS1qkYSgy7EOBK4qmPYHdMvP3ETEbeCIz946IjYEbIuIq4NXALsCuwHbA3cCZLznvtsD3gAOKc03MzGUR8R3g6cz8atHvXOCUzLw+InYArgT+BPgMcH1mfj4i3gLMautfhKSuMnhLQ7NJRPymWL8OOINGOfuWzPx90f4XwKv67mcD44HpwAHAeZnZAzwcEVf3c/59gWv7zpWZy9YxjoOBXSPWJtZbRsTmxTWOKI69LCKWr9+PKakMDN7S0DyXmXs0NxQB9JnmJuCEzLzyJf0Oa+E4NgL2zcyV/YxFUk14z1tqnSuB90XEGICI2DkiNgOuBd5e3BOfBBzYz7E3AQdExI7FsROL9qeALZr6XQWc0LcREXsUq9cC7yjaDgW2atUPJWnkMXhLrfN9Gvezfx0RdwLfpVHdugi4r9h3DnDjSw/MzMeB2cCFEfFb4Pxi138Af903YQ34ILBXMSHubl6Y9f45GsH/Lhrl8/9p088oaQTw3eaSJJWMmbckSSVj8JYkqWQM3pIklYzBW5KkkjF4S5JUMgZvSZJKxuAtSVLJ/H8I8m1XGu6PawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.1426689177751541\n",
      "test_loss: 0.2321787178516388\n",
      "test_acc: 0.900473952293396\n",
      "precision: 0.9568345323741008\n",
      "recall: 0.6303317535545023\n",
      "specificity 0.990521327014218\n",
      "sensitivity :  0.6303317535545023\n",
      "far 0.009478672985781991\n",
      "frr 0.3696682464454976\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 26, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 48, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 32, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.42029234987372843)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(101)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(8, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(88, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_2)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_3)\n",
    "\n",
    "# Adam\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.00119645731527559), metrics = ['accuracy'])\n",
    "# SGD\n",
    "# model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.0461300729767683, momentum=0.4411297369087802), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "\n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-durham",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
