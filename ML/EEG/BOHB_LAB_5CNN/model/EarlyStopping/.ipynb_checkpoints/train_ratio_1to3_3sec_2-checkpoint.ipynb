{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 1\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.5464 - accuracy: 0.7538 - val_loss: 0.4289 - val_accuracy: 0.7956\n",
      "Epoch 2/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3722 - accuracy: 0.8322\n",
      "Epoch 00002: val_loss improved from inf to 0.34207, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.3700 - accuracy: 0.8340 - val_loss: 0.3421 - val_accuracy: 0.8470\n",
      "Epoch 3/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9053\n",
      "Epoch 00003: val_loss improved from 0.34207 to 0.27785, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2526 - accuracy: 0.9056 - val_loss: 0.2779 - val_accuracy: 0.8603\n",
      "Epoch 4/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 0.9240\n",
      "Epoch 00004: val_loss improved from 0.27785 to 0.16321, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2078 - accuracy: 0.9240 - val_loss: 0.1632 - val_accuracy: 0.9468\n",
      "Epoch 5/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9267\n",
      "Epoch 00005: val_loss improved from 0.16321 to 0.14733, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.1919 - accuracy: 0.9265 - val_loss: 0.1473 - val_accuracy: 0.9439\n",
      "Epoch 6/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9343\n",
      "Epoch 00006: val_loss improved from 0.14733 to 0.14532, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.1676 - accuracy: 0.9341 - val_loss: 0.1453 - val_accuracy: 0.9468\n",
      "Epoch 7/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9391\n",
      "Epoch 00007: val_loss improved from 0.14532 to 0.13060, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.1667 - accuracy: 0.9395 - val_loss: 0.1306 - val_accuracy: 0.9496\n",
      "Epoch 8/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9436\n",
      "Epoch 00008: val_loss did not improve from 0.13060\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.1570 - accuracy: 0.9436 - val_loss: 0.1311 - val_accuracy: 0.9496\n",
      "Epoch 9/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9499\n",
      "Epoch 00009: val_loss did not improve from 0.13060\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1320 - accuracy: 0.9493 - val_loss: 0.1457 - val_accuracy: 0.9458\n",
      "Epoch 10/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9510\n",
      "Epoch 00010: val_loss improved from 0.13060 to 0.12770, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.1219 - accuracy: 0.9512 - val_loss: 0.1277 - val_accuracy: 0.9458\n",
      "Epoch 11/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9517\n",
      "Epoch 00011: val_loss did not improve from 0.12770\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1191 - accuracy: 0.9522 - val_loss: 0.1687 - val_accuracy: 0.9392\n",
      "Epoch 12/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9585\n",
      "Epoch 00012: val_loss improved from 0.12770 to 0.11541, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.1122 - accuracy: 0.9585 - val_loss: 0.1154 - val_accuracy: 0.9563\n",
      "Epoch 13/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1412 - accuracy: 0.9485\n",
      "Epoch 00013: val_loss improved from 0.11541 to 0.11512, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1412 - accuracy: 0.9477 - val_loss: 0.1151 - val_accuracy: 0.9553\n",
      "Epoch 14/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9627\n",
      "Epoch 00014: val_loss did not improve from 0.11512\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.1012 - accuracy: 0.9629 - val_loss: 0.1209 - val_accuracy: 0.9506\n",
      "Epoch 15/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9588\n",
      "Epoch 00015: val_loss did not improve from 0.11512\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1104 - accuracy: 0.9582 - val_loss: 0.1200 - val_accuracy: 0.9544\n",
      "Epoch 16/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9636\n",
      "Epoch 00016: val_loss did not improve from 0.11512\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.0966 - accuracy: 0.9636 - val_loss: 0.1324 - val_accuracy: 0.9449\n",
      "Epoch 17/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9665\n",
      "Epoch 00017: val_loss improved from 0.11512 to 0.10432, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.0878 - accuracy: 0.9670 - val_loss: 0.1043 - val_accuracy: 0.9582\n",
      "Epoch 18/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9585\n",
      "Epoch 00018: val_loss improved from 0.10432 to 0.10011, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.1018 - accuracy: 0.9585 - val_loss: 0.1001 - val_accuracy: 0.9620\n",
      "Epoch 19/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9707\n",
      "Epoch 00019: val_loss did not improve from 0.10011\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.0846 - accuracy: 0.9708 - val_loss: 0.1206 - val_accuracy: 0.9525\n",
      "Epoch 20/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9696\n",
      "Epoch 00020: val_loss improved from 0.10011 to 0.09355, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0812 - accuracy: 0.9696 - val_loss: 0.0935 - val_accuracy: 0.9620\n",
      "Epoch 21/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9703\n",
      "Epoch 00021: val_loss did not improve from 0.09355\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.0762 - accuracy: 0.9705 - val_loss: 0.1309 - val_accuracy: 0.9515\n",
      "Epoch 22/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9675\n",
      "Epoch 00022: val_loss improved from 0.09355 to 0.09030, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0777 - accuracy: 0.9674 - val_loss: 0.0903 - val_accuracy: 0.9658\n",
      "Epoch 23/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9736\n",
      "Epoch 00023: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.0992 - val_accuracy: 0.9686\n",
      "Epoch 24/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9731\n",
      "Epoch 00024: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.0689 - accuracy: 0.9731 - val_loss: 0.0937 - val_accuracy: 0.9601\n",
      "Epoch 25/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9745\n",
      "Epoch 00025: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.0804 - accuracy: 0.9750 - val_loss: 0.1280 - val_accuracy: 0.9534\n",
      "Epoch 26/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9749\n",
      "Epoch 00026: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0701 - accuracy: 0.9747 - val_loss: 0.0955 - val_accuracy: 0.9658\n",
      "Epoch 27/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9784\n",
      "Epoch 00027: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.0595 - accuracy: 0.9785 - val_loss: 0.0923 - val_accuracy: 0.9639\n",
      "Epoch 28/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9762\n",
      "Epoch 00028: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0636 - accuracy: 0.9762 - val_loss: 0.1219 - val_accuracy: 0.9553\n",
      "Epoch 29/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0616 - accuracy: 0.9774\n",
      "Epoch 00029: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0619 - accuracy: 0.9772 - val_loss: 0.0989 - val_accuracy: 0.9677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9813\n",
      "Epoch 00030: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.0560 - accuracy: 0.9813 - val_loss: 0.0904 - val_accuracy: 0.9620\n",
      "Epoch 31/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0582 - accuracy: 0.9777\n",
      "Epoch 00031: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0584 - accuracy: 0.9772 - val_loss: 0.1063 - val_accuracy: 0.9601\n",
      "Epoch 32/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9770\n",
      "Epoch 00032: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0625 - accuracy: 0.9772 - val_loss: 0.1396 - val_accuracy: 0.9487\n",
      "Epoch 33/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9812\n",
      "Epoch 00033: val_loss did not improve from 0.09030\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0560 - accuracy: 0.9813 - val_loss: 0.1274 - val_accuracy: 0.9582\n",
      "Epoch 34/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0554 - accuracy: 0.9755\n",
      "Epoch 00034: val_loss improved from 0.09030 to 0.08983, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0551 - accuracy: 0.9756 - val_loss: 0.0898 - val_accuracy: 0.9686\n",
      "Epoch 35/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 0.9797\n",
      "Epoch 00035: val_loss did not improve from 0.08983\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0512 - accuracy: 0.9800 - val_loss: 0.1025 - val_accuracy: 0.9667\n",
      "Epoch 36/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9839\n",
      "Epoch 00036: val_loss did not improve from 0.08983\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0461 - accuracy: 0.9842 - val_loss: 0.0959 - val_accuracy: 0.9724\n",
      "Epoch 37/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 0.9816\n",
      "Epoch 00037: val_loss improved from 0.08983 to 0.08788, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0468 - accuracy: 0.9819 - val_loss: 0.0879 - val_accuracy: 0.9715\n",
      "Epoch 38/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9841\n",
      "Epoch 00038: val_loss improved from 0.08788 to 0.08469, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0454 - accuracy: 0.9842 - val_loss: 0.0847 - val_accuracy: 0.9724\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0466 - accuracy: 0.9832\n",
      "Epoch 00039: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0467 - accuracy: 0.9832 - val_loss: 0.0859 - val_accuracy: 0.9667\n",
      "Epoch 40/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9818\n",
      "Epoch 00040: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0381 - accuracy: 0.9819 - val_loss: 0.1028 - val_accuracy: 0.9677\n",
      "Epoch 41/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9845\n",
      "Epoch 00041: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0411 - accuracy: 0.9845 - val_loss: 0.1021 - val_accuracy: 0.9667\n",
      "Epoch 42/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9849\n",
      "Epoch 00042: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0428 - accuracy: 0.9851 - val_loss: 0.1095 - val_accuracy: 0.9658\n",
      "Epoch 43/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0446 - accuracy: 0.9844\n",
      "Epoch 00043: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0443 - accuracy: 0.9845 - val_loss: 0.1136 - val_accuracy: 0.9629\n",
      "Epoch 44/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0417 - accuracy: 0.9823\n",
      "Epoch 00044: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0421 - accuracy: 0.9819 - val_loss: 0.0860 - val_accuracy: 0.9696\n",
      "Epoch 45/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9858\n",
      "Epoch 00045: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0385 - accuracy: 0.9857 - val_loss: 0.1199 - val_accuracy: 0.9620\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9784\n",
      "Epoch 00046: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0581 - accuracy: 0.9788 - val_loss: 0.1094 - val_accuracy: 0.9639\n",
      "Epoch 47/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9876\n",
      "Epoch 00047: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.0879 - val_accuracy: 0.9677\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9874\n",
      "Epoch 00048: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0368 - accuracy: 0.9870 - val_loss: 0.1383 - val_accuracy: 0.9563\n",
      "Epoch 49/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9873\n",
      "Epoch 00049: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0349 - accuracy: 0.9873 - val_loss: 0.0994 - val_accuracy: 0.9696\n",
      "Epoch 50/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9774\n",
      "Epoch 00050: val_loss did not improve from 0.08469\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0617 - accuracy: 0.9775 - val_loss: 0.0955 - val_accuracy: 0.9677\n",
      "Epoch 51/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9847\n",
      "Epoch 00051: val_loss improved from 0.08469 to 0.08429, saving model to best_model_2.h5\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0433 - accuracy: 0.9848 - val_loss: 0.0843 - val_accuracy: 0.9677\n",
      "Epoch 52/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9872\n",
      "Epoch 00052: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0319 - accuracy: 0.9870 - val_loss: 0.1542 - val_accuracy: 0.9553\n",
      "Epoch 53/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0323 - accuracy: 0.9874\n",
      "Epoch 00053: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0320 - accuracy: 0.9876 - val_loss: 0.1037 - val_accuracy: 0.9686\n",
      "Epoch 54/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9866\n",
      "Epoch 00054: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0382 - accuracy: 0.9867 - val_loss: 0.1240 - val_accuracy: 0.9658\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9849\n",
      "Epoch 00055: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0377 - accuracy: 0.9848 - val_loss: 0.1370 - val_accuracy: 0.9648\n",
      "Epoch 56/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9880\n",
      "Epoch 00056: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0320 - accuracy: 0.9880 - val_loss: 0.1211 - val_accuracy: 0.9648\n",
      "Epoch 57/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9905\n",
      "Epoch 00057: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.1486 - val_accuracy: 0.9553\n",
      "Epoch 58/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9895\n",
      "Epoch 00058: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0286 - accuracy: 0.9895 - val_loss: 0.0945 - val_accuracy: 0.9734\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/99 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9927\n",
      "Epoch 00059: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.1122 - val_accuracy: 0.9686\n",
      "Epoch 60/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9844\n",
      "Epoch 00060: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0418 - accuracy: 0.9845 - val_loss: 0.1063 - val_accuracy: 0.9658\n",
      "Epoch 61/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9882\n",
      "Epoch 00061: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0327 - accuracy: 0.9883 - val_loss: 0.0999 - val_accuracy: 0.9705\n",
      "Epoch 62/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9892\n",
      "Epoch 00062: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0247 - accuracy: 0.9892 - val_loss: 0.1057 - val_accuracy: 0.9705\n",
      "Epoch 63/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9902\n",
      "Epoch 00063: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0283 - accuracy: 0.9902 - val_loss: 0.1175 - val_accuracy: 0.9686\n",
      "Epoch 64/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9832\n",
      "Epoch 00064: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0435 - accuracy: 0.9835 - val_loss: 0.0958 - val_accuracy: 0.9743\n",
      "Epoch 65/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9914\n",
      "Epoch 00065: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0209 - accuracy: 0.9914 - val_loss: 0.1176 - val_accuracy: 0.9610\n",
      "Epoch 66/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9917\n",
      "Epoch 00066: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 0.1077 - val_accuracy: 0.9667\n",
      "Epoch 67/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9880\n",
      "Epoch 00067: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.1099 - val_accuracy: 0.9677\n",
      "Epoch 68/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9946\n",
      "Epoch 00068: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 4s 43ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.1189 - val_accuracy: 0.9667\n",
      "Epoch 69/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9939\n",
      "Epoch 00069: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0213 - accuracy: 0.9937 - val_loss: 0.1214 - val_accuracy: 0.9696\n",
      "Epoch 70/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0291 - accuracy: 0.9890\n",
      "Epoch 00070: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0288 - accuracy: 0.9892 - val_loss: 0.2932 - val_accuracy: 0.9401\n",
      "Epoch 71/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9900\n",
      "Epoch 00071: val_loss did not improve from 0.08429\n",
      "99/99 [==============================] - 3s 28ms/step - loss: 0.0251 - accuracy: 0.9902 - val_loss: 0.1113 - val_accuracy: 0.9696\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 480, 35)           245       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 160, 35)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 160, 39)           4134      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 53, 39)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 53, 47)            5546      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 17, 47)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 17, 47)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 218)               231952    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 87)                19053     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                968       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 261,910\n",
      "Trainable params: 261,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "33/33 - 0s - loss: 0.0843 - accuracy: 0.9677\n",
      "27/27 - 0s - loss: 0.1990 - accuracy: 0.9396\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAG5CAYAAACnXrwnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjjklEQVR4nO3debweZXnw8d+VBQIESEJiDElYKhEF37J8EKFYFUFZtAZ9FdFWosVGZHFDhRYrVbtIeS2LCxoBDVYQCkQiRRYDFFB20JRNiSwmkS0rAQIh51zvH88EHvDkLMnznCcz8/v6mc+ZuZ97Zu7nmA/Xua65ZyYyE0mSVB5DOj0ASZI0MAZvSZJKxuAtSVLJGLwlSSoZg7ckSSVj8JYkqWQM3pL6FBEPR8T+/ei3XURkRAwbjHFJdWXwVqlExIcj4vaIeDoiHo2In0fEm4vP/qkIHIc29R9WtG1XbP+w2N6zqc8OETHgBx5ExHUR8fG1fHZERNwfESsi4vGIuDwiNi/G+3SxvBARq5q2vxsRbyvGN+sVx9ulaL9uoOOUVD0Gb5VGRHwOOA34V2A8sA3wHWBqU7clwFciYmgvh1oC/HM/z/nRiPjhAMf51mKMH8rMzYHXAxcAZOZBmTkyM0cCPwb+fc12Zh5ZHOJJYO+I2KrpsNOA3w1kHJKqy+CtUoiILYGvAkdn5iWZ+UxmvpCZP8vMLzR1vQJYBfxNL4ebCfx5EWTb4Y3ATZl5F0BmLsnMmZm5op/7rwJ+ChwGUPwh8kEawb5HTeXqj0XE/IhYGhFHRsQbI2JuRCyLiG819R8SEV+KiEci4omIOLf4Ha/5/CPFZ4sj4sRXnGtIRJwQEb8vPr8wIsb087tJagGDt8pib2AEMKuPfgn8I3BSRAxfS59naWTG/9K64b3MLcABEfGViNgnIjZeh2OcCxxerB8A3A38sR/7vQmYQiPYnwacCOwP7Awc2vQHy0eLZV/gz4CRwLcAImIn4EzgI8DWwFbApKZzHAscAry1+Hwp8O0Bfj9J68HgrbLYCliUmav76piZs2mUnnu8Hl34HrBNRBzUovE1n/8G4H3A7sB/A4sj4j/6KOW/8hi/AsZExI40gvi5/dz1a5n5XGZeBTwDnJ+ZT2TmQuAGYLei318D/5GZD2bm08DfA4cVE83eD1yWmddn5vM0/hjqbjrHkcCJmbmg+PyfgPc7SU0aPAZvlcViYOwAAsSXaGSdI3r6sAg6XyuWl4mI7xRl5mU0rql/eM12RMztz8kz8+eZ+VfAGBrX5D9K739M9ORHwDE0suO+Kg5rPN60vrKH7ZHF+tbAI02fPQIMozGXYGtg/poPMvMZGr//NbYFZjX9ju4Duop9JQ0Cg7fK4ibgeRrl2j5l5tXAPOCoXrr9ABhFI0tu3veozByVmaOK/c9bs52Zfz6QQWdmd2bOAa4B3jCQfWkE76OAyzPz2QHu25c/0gjCa2wDrKYR7B8FJq/5ICI2pVH5WGM+cFDT72RUZo4osntJg8DgrVLIzOXAl4FvR8QhEbFpRAyPiIMi4t/XstuJwBd7OeZq4CTg+PUY2rCIGNG0DI+IqRFxWESMjoY9aVwfvnkgB87Mh4r9Tuyr7zo4H/hsRGwfESNpzAG4oPidXAS8OyLeHBEb0Zgo2Pzfiu8C/xIR2wJExLiImIqkQWPwVmlk5jeAz9EoiT9JIwM8hsbM7J76/xK4tY/Dnk8j01xXZ9IoR69ZfkBjAtffAQ8ATwH/CZySmWudLb42mXljZvZnotpAnUMjs78eeAh4jsZENDLzHuBo4Dwav5ulwIKmfU8HZgNXRcQKGn+UvKkNY5S0FpE54GdTSJKkDjLzliSpZAzekiSVjMFbkqSSMXhLklQyBm9JkkrG4F1zEXFgRPw2IuZFxAmdHo80GCLinOKFLHd3eizSujB411jxrO1vAwcBOwEfKl5KIVXdD4EDOz0IaV0ZvOttT2Be8XKKVcBPePm7saVKyszrabzXXSolg3e9TaTpBRQ0nqI1sUNjkST1k8FbkqSSMXjX20Ka3h4FTCraJEkbMIN3vd0GTCneLLURcBiNF05IkjZgBu8aK17/eAxwJXAfcGHxRimp0iLifBrviN8xIhZExBGdHpM0EL5VTJKkkjHzliSpZAzekiSVjMFbkqSSMXhLklQyBm8BEBHTOz0GabD5715lZfDWGv5HTHXkv3uVksFbkqSS2aDu8x47ZmhuN3l4p4dRS08u7mLcVkM7PYxa+t3cTTs9hNp6gecZzsadHkYtPcczrMrnY7DOd8C+m+XiJV0tOdYdc5+/MjM7+krZYZ08+SttN3k4t145ue+OUoUcsPWunR6CNOhuyTmDer7FS7q49cptWnKsoRMeGNuSA60Hy+aSpMpLoLtF/+uPiBgVERdFxP0RcV9E7B0RYyLi6oh4oPg5uugbEXFGRMyLiLkRsXtfxzd4S5LUeqcDV2Tm64BdaLw/4gRgTmZOAeYU2wAHAVOKZTpwZl8H36DK5pIktUfSlf3LmtdXRGwJvAX4KEBmrgJWRcRU4G1Ft5nAdcDxwFTg3GxMQru5yNonZOajazuHmbckqfIaZfNsyQKMjYjbm5ZX3nK4PfAk8IOIuCsizoqIzYDxTQH5MWB8sT4RmN+0/4Kiba3MvCVJGphFmblHL58PA3YHjs3MWyLidF4qkQOQmRkR63y7l5m3JKkWBnHC2gJgQWbeUmxfRCOYPx4REwCKn08Uny8Emm+1mlS0rZXBW5JUeUnSla1Z+jxX5mPA/IjYsWjaD7gXmA1MK9qmAZcW67OBw4tZ53sBy3u73g2WzSVJaodjgR9HxEbAg8DHaCTMF0bEEcAjwKFF38uBg4F5wLNF314ZvCVJtVBMNhsUmflroKfr4vv10DeBowdyfIO3JKnyEugaxODdbl7zliSpZMy8JUm1MJhl83YzeEuSKi+hXzPFy8KyuSRJJWPmLUmqhcF5svngMHhLkiovSWebS5KkzjHzliRVX0JXdRJvg7ckqfoarwStDsvmkiSVjJm3JKkGgi6i04NoGYO3JKnyEuiu0DVvy+aSJJWMmbckqRYsm0uSVCKNV4JWJ3hbNpckqWTMvCVJtdCd1cm8Dd6SpMqzbC5JkjrKzFuSVHlJ0FWhfNXgLUmqBa95S5JUIl7zliRJHWXmLUmqgaArq5OvGrwlSZXXeJ93dYJ3db6JJEk1YeYtSaqFKk1YM3hLkiovs1rXvKvzTSRJqgkzb0lSLXRbNpckqTwaD2mpTrG5Ot9EkqSaMPOWJNVAtSasGbwlSZXnQ1okSVJHmXlLkmqhy1eCSpJUHkk421ySJHWOmbckqRa6nW0uSVJ5+JAWSZLUUWbekqTKS8LZ5pIklY0PaZEkSR1j5i1JqrxMfLa5JEnlEpV6n3d1/gyRJKkmzLwlSZWXWDaXJKl0fEiLJEnqGDNvSVLlJUG3D2mRJKlcLJtLkqSOMfOWJFVe4itBJUkqmaDLh7RIkqS1iYiHI+J/I+LXEXF70TYmIq6OiAeKn6OL9oiIMyJiXkTMjYjd+zq+wVuSVHlryuatWAZg38zcNTP3KLZPAOZk5hRgTrENcBAwpVimA2f2dWCDtySpFrqK0vn6LuthKjCzWJ8JHNLUfm423AyMiogJvR3I4C1J0sCMjYjbm5bpPfRJ4KqIuKPp8/GZ+Wix/hgwvlifCMxv2ndB0bZWTliTJFVeZrRytvmiplL42rw5MxdGxKuAqyPi/pePJzMicl0HYPCWJNXCYL6YJDMXFj+fiIhZwJ7A4xExITMfLcriTxTdFwKTm3afVLStlWVzSZJaKCI2i4jN16wD7wTuBmYD04pu04BLi/XZwOHFrPO9gOVN5fUemXlLkiovge7Bu897PDArIqARZ8/LzCsi4jbgwog4AngEOLTofzlwMDAPeBb4WF8nMHhLkmogBq1snpkPArv00L4Y2K+H9gSOHsg5LJtLklQyZt6SpMprPKSlOo9HNXhLkmrBV4JKkqSOMfOWJFVeEpbNJUkqm+4KFZur800kSaoJM29JUuVlQpdlc0mSyqVK17wtm0uSVDJm3pKkymvMNq9OvmrwliTVQtfgvZik7QzeeklsTmz5rzBsCgC5/ARY/RAx6nQYOhG6FpLLPgX5FGz6cWKT9xQ7DoVhryGfeBPk8s6NX1oH4yZtxRdnHsPo8aPITC7//i+YdcblAEw95kDec9SBdHd1c8vld3LW8f/Z4dFqXfl4VFVWbPEl8vnrYdmxwHCIEcRmnyRX/QqemQGbTSc2+wT59Cnw7Fnks2c1dtz47cSmHzVwq5S6Vnfxvc+fy7y7HmKTkSP4zu0nc8fVcxk9fkv+4j1v5MhdP88Lq1YzatwWnR6q9KLqXADQ+omRMPyNsPK/ioYXIFfAiP1g5axG08pZMGL/P911xLvJ5y4bvLFKLbTksWXMu+shAFY+/Rx/uG8hYyeO4a+OfCc/OfmnvLBqNQDLnnyqk8PUemtc827FsiHYMEahzhs6GbqXEFueTGx1KbHFv0BsAkPGQveTjT7dTza2X2YEbPyX8NyVgz5kqdXGbzuOHXbbnvtveYBJr92a//OXr+eMm/6Vb1z7FV67x2s6PTytp26iJcuGoK3BOyIOjIjfRsS8iDihnefS+hoKw3cmnz2PXDwVciWx2Sd66Jcv3xzxdlh1pyVzld6IzUbw5Ys+z5mf/QHPrljJkGFD2HzMSD619z8w44s/4ksXfK7TQ5Re1LbgHRFDgW8DBwE7AR+KiJ3adT6tp+7HGssLvwEgn7sChu0M3YtgyLhGnyHjoHvxy3aLEe+yZK7SGzpsKCdddBzXnHcDN866FYBFC5Zw4yW3APDb2+aR3d1sOdbr3mW15glrrVg2BO3MvPcE5mXmg5m5CvgJMLWN59P66F4EXY/C0O0BiI33hq558Pw1sMl7G302eS88N+elfWIkbLQnPP+LDgxYap3jzvokf7h/IRef+tIfor+69FZ23fcNAEycMoFhGw1j+SKve5dZla55t3O2+URgftP2AuBNr+wUEdOB6QDbTHTyeyflU18jRn0DGA5d8xu3ijGkcavYJh8obhX79Es7jHgnPH8j5MpODVlabzvv8zrecfhbeXDuI3z3zlMAOOfE87jinGs57uxPMmPuN1i9ajWnfPTbHR6p9JKOR8vMnAHMANhjlxHZR3e10+r7yMXv+5PmXDqt5/4rLyFXXtLmQUntdc8v7+cdQz7Q42cnH/7NQR6N2sX3efffQmBy0/akok2SpEG3ocwUb4V2Fu9vA6ZExPYRsRFwGDC7jeeTJKkW2pZ5Z+bqiDgGuBIYCpyTmfe063ySJK2Nj0cdgMy8HLi8neeQJKk/NpSZ4q1QnW8iSVJNdHy2uSRJbZfONpckqVQSZ5tLkqQOMvOWJNWCZXNJkkqkareKWTaXJKlkzLwlSbVQpczb4C1JqryqvZjEsrkkSSVj5i1JqoUq3edt8JYkVV9W65q3ZXNJkkrGzFuSVHlVu8/b4C1JqoUqBW/L5pIklYyZtySp8qp2n7fBW5JUC1mh4G3ZXJKkkjHzliTVgg9pkSSpRNKHtEiSpE4y85Yk1UKVJqwZvCVJNVCtW8Usm0uSVDJm3pKkWrBsLklSiVTtxSSWzSVJKhkzb0lS9WXjXu+qMHhLkmqhSk9Ys2wuSVLJmHlLkiovqdZsczNvSVINNB7S0oqlX2eLGBoRd0XEZcX29hFxS0TMi4gLImKjon3jYnte8fl2/Tm+wVuSpNb7NHBf0/bJwKmZuQOwFDiiaD8CWFq0n1r065PBW5JUC5mtWfoSEZOAdwFnFdsBvB24qOgyEzikWJ9abFN8vl/Rv1de85Yk1UILr3mPjYjbm7ZnZOaMpu3TgC8CmxfbWwHLMnN1sb0AmFisTwTmN8aXqyNiedF/UW8DMHhLkjQwizJzj54+iIh3A09k5h0R8bZ2DcDgLUmqvEbJe1Bmm+8DvCciDgZGAFsApwOjImJYkX1PAhYW/RcCk4EFETEM2BJY3NdJvOYtSaqFwZhtnpl/n5mTMnM74DDgmsz8a+Ba4P1Ft2nApcX67GKb4vNrMvu+sm7wliSp/Y4HPhcR82hc0z67aD8b2Kpo/xxwQn8OZtlcklQLg/1s88y8DriuWH8Q2LOHPs8BHxjosQ3ekqRaqNIT1gzekqTKS6JSwdtr3pIklYyZtySpFir0Om+DtySpBgbvPu9BYdlckqSSMfOWJNVDhermBm9JUi1YNpckSR1j5i1JqoXBfsJaOxm8JUmVl1g2lyRJHWTmLUmqvgQqlHkbvCVJtVCla96WzSVJKhkzb0lSPVQo8zZ4S5JqwFeCSpKkDjLzliTVg2VzSZJKxFeCSpKkTjLzliTVg2VzSZLKxrK5JEnqEDNvSVI9WDaXJKlkKhS8LZtLklQyZt6SpOrzlaCSJJWPrwSVJEkdY+YtSaqHCmXeBm9JUj1U6Jq3ZXNJkkrGzFuSVAth2VySpBJJKnXN27K5JEkls9bMOyK+SS9/p2Tmp9oyIkmSWi4qNWGtt7L57YM2CkmS2q1CZfO1Bu/MnDmYA5EkSf3T54S1iBgHHA/sBIxY056Zb2/juCRJaq0KZd79mbD2Y+A+YHvgK8DDwG1tHJMkSa2XLVo2AP0J3ltl5tnAC5n5P5n5t4BZtyRJHdKf+7xfKH4+GhHvAv4IjGnfkCRJarEavhL0nyNiS+A44JvAFsBn2zoqSZJarFZPWMvMy4rV5cC+7R2OJEnqS39mm/+AHi7RF9e+JUkqhzpl3sBlTesjgPfSuO4tSZI6oD9l84ubtyPifODGto1IkiT1al3eKjYFeFWrBwLwu7mbcsDWu7bj0NIGa8UH9+r0EKRB133lzYN+zlpNWIuIFbz8SsFjNJ64JklSedTpVrHM3HwwBiJJkvqnzyesRcSc/rRJkrTBatWjUTeQ0ntv7/MeAWwKjI2I0cCaesMWwMRBGJskSa2zgQTeVuitbP4J4DPA1sAdvBS8nwK+1d5hSZLUWrWYsJaZpwOnR8SxmfnNQRyTJEnqRX/eKtYdEaPWbETE6Ig4qn1DkiSpDSp0zbs/wfvvMnPZmo3MXAr8XdtGJElSOwxS8I6IERFxa0T8JiLuiYivFO3bR8QtETEvIi6IiI2K9o2L7XnF59v1dY7+BO+hEfHizXERMRTYqB/7SZJUR88Db8/MXYBdgQMjYi/gZODUzNwBWAocUfQ/AlhatJ9a9OtVf4L3FcAFEbFfROwHnA/8fKDfRJKkTols3dKXbHi62BxeLAm8HbioaJ8JHFKsTy22KT7frzlp7kl/Ho96PDAdOLLYngu8uh/7SZK04WjdE9bGRsTtTdszMnNGc4eiSn0HsAPwbeD3wLLMXF10WcBLt11PBOYDZObqiFgObAUsWtsA+vOEte6IuAV4DXAoMBa4uPe9JEmqrEWZuUdvHTKzC9i1mPA9C3hdKwfQ20NaXgt8qFgWARcUA9q3lQOQJGlQdGCmeGYui4hrgb2BURExrMi+JwELi24LgcnAgogYBmwJLO7tuL1d876fRn3+3Zn55uJe7671/B6SJHXEYF3zjohxa26xjohNgHcA9wHXAu8vuk0DLi3WZxfbFJ9fk5m9nqm3svn7gMOAayPiCuAnvPSUNUmS1LMJwMziuvcQ4MLMvCwi7gV+EhH/DNwFnF30Pxv4UUTMA5bQiL296u0Jaz8FfhoRm9GYCfcZ4FURcSYwKzOvWuevJUnSYBuksnlmzgV266H9QWDPHtqfAz4wkHP0eatYZj6Tmedl5l/RqNHfhe/zliSVySDeKjYY+nOf94syc2lmzsjM/do1IEmS1Lv+3OctSVL5bSBZcysYvCVJ9VCh4D2gsrkkSeo8M29JUi1sKJPNWsHMW5KkkjF4S5JUMpbNJUn1UKGyucFbklR9G9ADVlrBsrkkSSVj5i1JqocKZd4Gb0lSPVQoeFs2lySpZMy8JUmVF1RrwprBW5JUDxUK3pbNJUkqGTNvSVL1Vew+b4O3JKkeKhS8LZtLklQyZt6SpHqoUOZt8JYk1UKVrnlbNpckqWTMvCVJ9VChzNvgLUmqvqRSwduyuSRJJWPmLUmqhSpNWDN4S5LqoULB27K5JEklY+YtSaoFy+aSJJVNhYK3ZXNJkkrGzFuSVH0Vu8/b4C1JqrwolqqwbC5JUsmYeUuS6sGyuSRJ5VKlW8Usm0uSVDJm3pKkeqhQ5m3wliTVQ4WCt2VzSZJKxsxbklR9Wa0JawZvSVI9GLwlSSqXKmXeXvOWJKlkzLwlSfVQoczb4C1JqgXL5pIkqWPMvCVJ1ef7vCVJKqEKBW/L5pIklYyZtySp8oJqTVgzeEuS6qFCwduyuSRJJWPmLUmqhcjqpN4Gb0lS9VXsVjHL5pIktVBETI6IayPi3oi4JyI+XbSPiYirI+KB4ufooj0i4oyImBcRcyNi977OYfCWJNVCZGuWflgNHJeZOwF7AUdHxE7ACcCczJwCzCm2AQ4CphTLdODMvk5g8JYk1UO2aOnrNJmPZuadxfoK4D5gIjAVmFl0mwkcUqxPBc7NhpuBURExobdzGLwlSRqYsRFxe9MyfW0dI2I7YDfgFmB8Zj5afPQYML5YnwjMb9ptQdG2Vk5YkyTVQgsf0rIoM/fo83wRI4GLgc9k5lMR8eJnmZkR6z4iM29JUj0MUtkcICKG0wjcP87MS4rmx9eUw4ufTxTtC4HJTbtPKtrWyuAtSVILRSPFPhu4LzP/o+mj2cC0Yn0acGlT++HFrPO9gOVN5fUeWTaXJFVf/2eKt8I+wEeA/42IXxdt/wB8HbgwIo4AHgEOLT67HDgYmAc8C3ysrxMYvCVJ9TBIwTszb6TxLpSe7NdD/wSOHsg5LJtLklQyZt6SpMrzlaCSJJVRhV5MYtlckqSSMfOWJNWCZXNJksrEV4JKkqROMvMWAOMmbcUXZx7D6PGjyEwu//4vmHXG5XzkpA9w8Mf3Z/mTTwFwzonncevP7+rwaKXWOfETB7DPbn/G0qee5a+/2Hjh05Rtx3H8Efuz0fBhdHV3c8o5c7j3948BsPvrJ/GZw/dl2LAhLFuxkqO+emEnh68BiO5Oj6B1DN4CoGt1F9/7/LnMu+shNhk5gu/cfjJ3XD0XgItPu4yLvvGzDo9Qao///p+7uejKu/jyUQe92HbMh9/C2RffxE2/eZi9d92eYz78Fo762oWM3HRjvvC3+/OZr1/M44tXMHqLTTo4cg1YhcrmBm8BsOSxZSx5bBkAK59+jj/ct5CxE8d0dlDSIPj1/QuZMHaLl7VlwmabbAzAyE035smlTwNwwD6v47rbHuDxxSsAWPrUysEdrFQweOtPjN92HDvstj333/IAO++zI1OPPpB3fOSt/O6O3/O9487l6WXPdHqIUluddu61nPb3/5dj/+atRMD0k84HYPKE0QwbOpTv/OOhbDpiIy644k5+fsO9HR6t+qtKs83bNmEtIs6JiCci4u52nUOtN2KzEXz5os9z5md/wLMrVvKzM69i2g7HcuRuX2DJo8v4xDcO7/QQpbZ73zt24fQfXcfUY2Zw+o+u48TpBwAwdMgQXrf9q/jcv1/Cp79+MX/73r2Y/OrRHR2r+ilplFRasWwA2jnb/IfAgW08vlps6LChnHTRcVxz3g3cOOtWAJY9sZzu7u4XJ7Ht+MYdOjxKqf0OfsvOXHvrAwDMufl37PSaVwPwxJKnuXnuIzz3/GqWr1jJXfcvYMq24zo5VNVU24J3Zl4PLGnX8dV6x531Sf5w/0IuPvWyF9vGvHrUi+v7vHdPHr57fgdGJg2uRUufZvfXTwJgj523YX4xH+SG2+exy45bM3RIsPFGw9h5hwk8vHBxB0eqgYhszbIh6Pg174iYDkwHGMGmHR5Nfe28z+t4x+Fv5cG5j/DdO08BGreF7XvYm3nNrtuRmTz+8JOcduT3OjxSqbW+euy72P31kxi1+SbM/tZ0vn/Rr/i371/NZw/fl6FDg1UvdPFvZ10FwMN/XMLNv3mY/zx5Gt2ZzL72f3lwgcG7NDaQwNsKkW2s30fEdsBlmfmG/vTfIsbkm+JPXnUqVdqKD+7V6SFIg+7uK0/j6SXz1/bO65YbOXpy7rrvp1tyrF/O+sIdmblHSw62jjqeeUuS1G6+ElSSpLLZgGaKt0I7bxU7H7gJ2DEiFkTEEe06lyRJddK2zDszP9SuY0uSNFCWzSVJKpsKBW9fCSpJUsmYeUuSasGyuSRJZZJAd3Wit2VzSZJKxsxbklQP1Um8Dd6SpHqo0jVvy+aSJJWMmbckqR4q9HhUg7ckqRYsm0uSpI4x85YkVV/ibHNJksqk8T7v6kRvg7ckqR66Oz2A1vGatyRJJWPmLUmqBcvmkiSVScUmrFk2lySpZMy8JUk1kD5hTZKksvEJa5IkqWPMvCVJ9WDZXJKkEkkIH9IiSZI6xcxbklQPls0lSSqZ6sRuy+aSJJWNmbckqRZ8trkkSWVToeBt2VySpJIx85YkVV8CFbrP2+AtSaq8ICt1zduyuSRJJWPmLUmqhwpl3gZvSVI9VCh4WzaXJKlkzLwlSdVXsdnmZt6SpFqIzJYsfZ4n4pyIeCIi7m5qGxMRV0fEA8XP0UV7RMQZETEvIuZGxO79+S4Gb0mSWuuHwIGvaDsBmJOZU4A5xTbAQcCUYpkOnNmfExi8JUn1kNmapc/T5PXAklc0TwVmFuszgUOa2s/NhpuBURExoa9zeM1bklQD/Qu8/TQ2Im5v2p6RmTP62Gd8Zj5arD8GjC/WJwLzm/otKNoepRcGb0mSBmZRZu6xrjtnZkbEev0lYfCWJFVf0un7vB+PiAmZ+WhRFn+iaF8ITG7qN6lo65XXvCVJ9dDdomXdzAamFevTgEub2g8vZp3vBSxvKq+vlZm3JEktFBHnA2+jcW18AXAS8HXgwog4AngEOLTofjlwMDAPeBb4WH/OYfCWJNXCYL1VLDM/tJaP9uuhbwJHD/QcBm9JUj34bHNJktQpZt6SpOpLoLs6mbfBW5JUAy19SEvHWTaXJKlkzLwlSfVQoczb4C1JqocKBW/L5pIklYyZtySp+pxtLklS2STkuj+YfENj2VySpJIx85Yk1UOFJqwZvCVJ1Vexa96WzSVJKhkzb0lSPVg2lySpZCoUvC2bS5JUMmbekqQaqNZbxQzekqTqS6Dbh7RIkqQOMfOWJNWDZXNJkkrG4C1JUpmkT1iTJEmdY+YtSaq+hKzQK0EN3pKkerBsLkmSOsXMW5JUD842lySpRDJ9wpokSeocM29JUj1YNpckqVzSsrkkSeoUM29JUg34Pm9Jksol8SEtkiSpc8y8JUn14LPNJUkqjwTSsrkkSeoUM29JUvVlWjaXJKlsLJtLkqSOMfOWJNVDhcrmkRvQE2ci4kngkU6Po6bGAos6PQhpkPnvvnO2zcxxg3WyiLiCxv/frbAoMw9s0bHWyQYVvNU5EXF7Zu7R6XFIg8l/9yorr3lLklQyBm9JkkrG4K01ZnR6AFIH+O9epWTwFgCZ6X/EehERXRHx64i4OyL+KyI2XY9j/TAi3l+snxURO/XS920R8RfrcI6HI6JVk3Mqy3/3KiuDt9Q/KzNz18x8A7AKOLL5w4hYp9suM/PjmXlvL13eBgw4eEuqNoO3NHA3ADsUWfENETEbuDcihkbEKRFxW0TMjYhPAETDtyLitxHxC+BVaw4UEddFxB7F+oERcWdE/CYi5kTEdjT+SPhskfX/ZUSMi4iLi3PcFhH7FPtuFRFXRcQ9EXEWEIP8O5E0iHxIizQARYZ9EHBF0bQ78IbMfCgipgPLM/ONEbEx8MuIuArYDdgR2AkYD9wLnPOK444Dvg+8pTjWmMxcEhHfBZ7OzP9X9DsPODUzb4yIbYArgdcDJwE3ZuZXI+JdwBFt/UVI6iiDt9Q/m0TEr4v1G4CzaZSzb83Mh4r2dwJ/vuZ6NrAlMAV4C3B+ZnYBf4yIa3o4/l7A9WuOlZlL1jKO/YGdIl5MrLeIiJHFOd5X7PvfEbF03b6mpDIweEv9szIzd21uKALoM81NwLGZeeUr+h3cwnEMAfbKzOd6GIukmvCat9Q6VwKfjIjhABHx2ojYDLge+GBxTXwCsG8P+94MvCUiti/2HVO0rwA2b+p3FXDsmo2I2LVYvR74cNF2EDC6VV9K0obH4C21zlk0rmffGRF3A9+jUd2aBTxQfHYucNMrd8zMJ4HpwCUR8RvgguKjnwHvXTNhDfgUsEcxIe5eXpr1/hUawf8eGuXzP7TpO0raAPhsc0mSSsbMW5KkkjF4S5JUMgZvSZJKxuAtSVLJGLwlSSoZg7ckSSVj8JYkqWT+P/fEtKgQ31VMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.08429263532161713\n",
      "test_loss: 0.19904468953609467\n",
      "test_acc: 0.9395734667778015\n",
      "precision: 0.8773584905660378\n",
      "recall: 0.8815165876777251\n",
      "specificity 0.9589257503949447\n",
      "sensitivity :  0.8815165876777251\n",
      "far 0.04107424960505529\n",
      "frr 0.11848341232227488\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 35, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 39, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 47, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.5253474943959077)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(218)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(87, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(11, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_2)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_3)\n",
    "\n",
    "# Adam\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.00031882961655001175), metrics = ['accuracy'])\n",
    "# SGD\n",
    "# model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.0461300729767683, momentum=0.4411297369087802), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "        \n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
