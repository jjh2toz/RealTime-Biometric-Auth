{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 9\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.5018 - accuracy: 0.7766 - val_loss: 0.4370 - val_accuracy: 0.8536\n",
      "Epoch 2/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8359\n",
      "Epoch 00002: val_loss improved from inf to 0.31466, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.3559 - accuracy: 0.8359 - val_loss: 0.3147 - val_accuracy: 0.8660\n",
      "Epoch 3/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8555\n",
      "Epoch 00003: val_loss improved from 0.31466 to 0.25391, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.3071 - accuracy: 0.8552 - val_loss: 0.2539 - val_accuracy: 0.8840\n",
      "Epoch 4/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.8967\n",
      "Epoch 00004: val_loss improved from 0.25391 to 0.18954, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.2573 - accuracy: 0.8967 - val_loss: 0.1895 - val_accuracy: 0.9163\n",
      "Epoch 5/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9195\n",
      "Epoch 00005: val_loss improved from 0.18954 to 0.11315, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.2062 - accuracy: 0.9195 - val_loss: 0.1131 - val_accuracy: 0.9610\n",
      "Epoch 6/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1552 - accuracy: 0.9423\n",
      "Epoch 00006: val_loss improved from 0.11315 to 0.08426, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.1554 - accuracy: 0.9420 - val_loss: 0.0843 - val_accuracy: 0.9686\n",
      "Epoch 7/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1317 - accuracy: 0.9487\n",
      "Epoch 00007: val_loss did not improve from 0.08426\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.1334 - accuracy: 0.9477 - val_loss: 0.1037 - val_accuracy: 0.9686\n",
      "Epoch 8/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9556\n",
      "Epoch 00008: val_loss improved from 0.08426 to 0.05781, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.1197 - accuracy: 0.9556 - val_loss: 0.0578 - val_accuracy: 0.9791\n",
      "Epoch 9/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0936 - accuracy: 0.9662\n",
      "Epoch 00009: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.0933 - accuracy: 0.9664 - val_loss: 0.0720 - val_accuracy: 0.9800\n",
      "Epoch 10/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9538\n",
      "Epoch 00010: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.1273 - accuracy: 0.9541 - val_loss: 0.0866 - val_accuracy: 0.9724\n",
      "Epoch 11/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0994 - accuracy: 0.9643\n",
      "Epoch 00011: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0992 - accuracy: 0.9642 - val_loss: 0.0672 - val_accuracy: 0.9800\n",
      "Epoch 12/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9627\n",
      "Epoch 00012: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.1017 - accuracy: 0.9626 - val_loss: 0.0617 - val_accuracy: 0.9734\n",
      "Epoch 13/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9707\n",
      "Epoch 00013: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.0810 - accuracy: 0.9708 - val_loss: 0.0614 - val_accuracy: 0.9781\n",
      "Epoch 14/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0750 - accuracy: 0.9694\n",
      "Epoch 00014: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.0745 - accuracy: 0.9696 - val_loss: 0.0664 - val_accuracy: 0.9781\n",
      "Epoch 15/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9624\n",
      "Epoch 00015: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0995 - accuracy: 0.9626 - val_loss: 0.0824 - val_accuracy: 0.9762\n",
      "Epoch 16/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0799 - accuracy: 0.9719\n",
      "Epoch 00016: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0798 - accuracy: 0.9718 - val_loss: 0.0650 - val_accuracy: 0.9762\n",
      "Epoch 17/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9732\n",
      "Epoch 00017: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 5s 56ms/step - loss: 0.0719 - accuracy: 0.9734 - val_loss: 0.0582 - val_accuracy: 0.9791\n",
      "Epoch 18/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9689\n",
      "Epoch 00018: val_loss did not improve from 0.05781\n",
      "99/99 [==============================] - 6s 58ms/step - loss: 0.0813 - accuracy: 0.9689 - val_loss: 0.0992 - val_accuracy: 0.9715\n",
      "Epoch 19/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9710\n",
      "Epoch 00019: val_loss improved from 0.05781 to 0.05137, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0818 - accuracy: 0.9712 - val_loss: 0.0514 - val_accuracy: 0.9848\n",
      "Epoch 20/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9700\n",
      "Epoch 00020: val_loss did not improve from 0.05137\n",
      "99/99 [==============================] - 6s 60ms/step - loss: 0.0728 - accuracy: 0.9702 - val_loss: 0.0983 - val_accuracy: 0.9620\n",
      "Epoch 21/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9758\n",
      "Epoch 00021: val_loss did not improve from 0.05137\n",
      "99/99 [==============================] - 6s 59ms/step - loss: 0.0747 - accuracy: 0.9756 - val_loss: 0.0551 - val_accuracy: 0.9781\n",
      "Epoch 22/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9780\n",
      "Epoch 00022: val_loss improved from 0.05137 to 0.04832, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 60ms/step - loss: 0.0626 - accuracy: 0.9772 - val_loss: 0.0483 - val_accuracy: 0.9838\n",
      "Epoch 23/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9786\n",
      "Epoch 00023: val_loss did not improve from 0.04832\n",
      "99/99 [==============================] - 6s 62ms/step - loss: 0.0597 - accuracy: 0.9785 - val_loss: 0.0809 - val_accuracy: 0.9705\n",
      "Epoch 24/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9751\n",
      "Epoch 00024: val_loss improved from 0.04832 to 0.04776, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 63ms/step - loss: 0.0658 - accuracy: 0.9747 - val_loss: 0.0478 - val_accuracy: 0.9838\n",
      "Epoch 25/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9818\n",
      "Epoch 00025: val_loss did not improve from 0.04776\n",
      "99/99 [==============================] - 6s 63ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.0505 - val_accuracy: 0.9829\n",
      "Epoch 26/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9807\n",
      "Epoch 00026: val_loss improved from 0.04776 to 0.03891, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 62ms/step - loss: 0.0563 - accuracy: 0.9807 - val_loss: 0.0389 - val_accuracy: 0.9857\n",
      "Epoch 27/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9844\n",
      "Epoch 00027: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 6s 65ms/step - loss: 0.0448 - accuracy: 0.9845 - val_loss: 0.0412 - val_accuracy: 0.9857\n",
      "Epoch 28/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9816\n",
      "Epoch 00028: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 6s 61ms/step - loss: 0.0505 - accuracy: 0.9816 - val_loss: 0.0404 - val_accuracy: 0.9876\n",
      "Epoch 29/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9834\n",
      "Epoch 00029: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 6s 65ms/step - loss: 0.0451 - accuracy: 0.9835 - val_loss: 0.0537 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9783\n",
      "Epoch 00030: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 7s 66ms/step - loss: 0.0552 - accuracy: 0.9785 - val_loss: 0.1180 - val_accuracy: 0.9544\n",
      "Epoch 31/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9780\n",
      "Epoch 00031: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 6s 63ms/step - loss: 0.0605 - accuracy: 0.9781 - val_loss: 0.0675 - val_accuracy: 0.9791\n",
      "Epoch 32/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9869\n",
      "Epoch 00032: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 6s 65ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0447 - val_accuracy: 0.9857\n",
      "Epoch 33/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9818\n",
      "Epoch 00033: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 7s 66ms/step - loss: 0.0517 - accuracy: 0.9819 - val_loss: 0.0608 - val_accuracy: 0.9743\n",
      "Epoch 34/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9783\n",
      "Epoch 00034: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 7s 70ms/step - loss: 0.0549 - accuracy: 0.9785 - val_loss: 0.0425 - val_accuracy: 0.9876\n",
      "Epoch 35/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0499 - accuracy: 0.9821\n",
      "Epoch 00035: val_loss did not improve from 0.03891\n",
      "99/99 [==============================] - 7s 72ms/step - loss: 0.0496 - accuracy: 0.9823 - val_loss: 0.0579 - val_accuracy: 0.9819\n",
      "Epoch 36/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9810\n",
      "Epoch 00036: val_loss improved from 0.03891 to 0.03786, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 7s 76ms/step - loss: 0.0478 - accuracy: 0.9810 - val_loss: 0.0379 - val_accuracy: 0.9876\n",
      "Epoch 37/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0609 - accuracy: 0.9786\n",
      "Epoch 00037: val_loss did not improve from 0.03786\n",
      "99/99 [==============================] - 7s 75ms/step - loss: 0.0606 - accuracy: 0.9788 - val_loss: 0.0425 - val_accuracy: 0.9867\n",
      "Epoch 38/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9800\n",
      "Epoch 00038: val_loss improved from 0.03786 to 0.03571, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 7s 74ms/step - loss: 0.0459 - accuracy: 0.9800 - val_loss: 0.0357 - val_accuracy: 0.9867\n",
      "Epoch 39/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9823\n",
      "Epoch 00039: val_loss did not improve from 0.03571\n",
      "99/99 [==============================] - 7s 70ms/step - loss: 0.0491 - accuracy: 0.9823 - val_loss: 0.0415 - val_accuracy: 0.9867\n",
      "Epoch 40/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9818\n",
      "Epoch 00040: val_loss did not improve from 0.03571\n",
      "99/99 [==============================] - 6s 65ms/step - loss: 0.0457 - accuracy: 0.9816 - val_loss: 0.0444 - val_accuracy: 0.9800\n",
      "Epoch 41/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9837\n",
      "Epoch 00041: val_loss did not improve from 0.03571\n",
      "99/99 [==============================] - 7s 67ms/step - loss: 0.0451 - accuracy: 0.9838 - val_loss: 0.0592 - val_accuracy: 0.9800\n",
      "Epoch 42/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9861\n",
      "Epoch 00042: val_loss did not improve from 0.03571\n",
      "99/99 [==============================] - 8s 85ms/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0474 - val_accuracy: 0.9829\n",
      "Epoch 43/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9842\n",
      "Epoch 00043: val_loss did not improve from 0.03571\n",
      "99/99 [==============================] - 7s 69ms/step - loss: 0.0445 - accuracy: 0.9842 - val_loss: 0.0589 - val_accuracy: 0.9762\n",
      "Epoch 44/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9844\n",
      "Epoch 00044: val_loss improved from 0.03571 to 0.03426, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 62ms/step - loss: 0.0399 - accuracy: 0.9845 - val_loss: 0.0343 - val_accuracy: 0.9886\n",
      "Epoch 45/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9866\n",
      "Epoch 00045: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 61ms/step - loss: 0.0412 - accuracy: 0.9867 - val_loss: 0.0469 - val_accuracy: 0.9857\n",
      "Epoch 46/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9829\n",
      "Epoch 00046: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 59ms/step - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.0696 - val_accuracy: 0.9715\n",
      "Epoch 47/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9895\n",
      "Epoch 00047: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 0.0484 - val_accuracy: 0.9876\n",
      "Epoch 48/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9828\n",
      "Epoch 00048: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 59ms/step - loss: 0.0446 - accuracy: 0.9829 - val_loss: 0.0467 - val_accuracy: 0.9867\n",
      "Epoch 49/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9850\n",
      "Epoch 00049: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 58ms/step - loss: 0.0387 - accuracy: 0.9851 - val_loss: 0.1310 - val_accuracy: 0.9667\n",
      "Epoch 50/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9813\n",
      "Epoch 00050: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0460 - accuracy: 0.9813 - val_loss: 0.0378 - val_accuracy: 0.9838\n",
      "Epoch 51/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9861\n",
      "Epoch 00051: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0316 - accuracy: 0.9861 - val_loss: 0.0444 - val_accuracy: 0.9886\n",
      "Epoch 52/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0399 - accuracy: 0.9866\n",
      "Epoch 00052: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 60ms/step - loss: 0.0397 - accuracy: 0.9867 - val_loss: 0.0384 - val_accuracy: 0.9924\n",
      "Epoch 53/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9863\n",
      "Epoch 00053: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 58ms/step - loss: 0.0352 - accuracy: 0.9864 - val_loss: 0.0535 - val_accuracy: 0.9867\n",
      "Epoch 54/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9869\n",
      "Epoch 00054: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0349 - accuracy: 0.9870 - val_loss: 0.0436 - val_accuracy: 0.9867\n",
      "Epoch 55/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9892\n",
      "Epoch 00055: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0362 - accuracy: 0.9889 - val_loss: 0.0425 - val_accuracy: 0.9848\n",
      "Epoch 56/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9876\n",
      "Epoch 00056: val_loss did not improve from 0.03426\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.0476 - val_accuracy: 0.9781\n",
      "Epoch 57/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9886\n",
      "Epoch 00057: val_loss improved from 0.03426 to 0.03059, saving model to best_model_10.h5\n",
      "99/99 [==============================] - 6s 58ms/step - loss: 0.0330 - accuracy: 0.9886 - val_loss: 0.0306 - val_accuracy: 0.9895\n",
      "Epoch 58/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9899\n",
      "Epoch 00058: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0260 - accuracy: 0.9899 - val_loss: 0.0479 - val_accuracy: 0.9819\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9848\n",
      "Epoch 00059: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0376 - accuracy: 0.9848 - val_loss: 0.0468 - val_accuracy: 0.9848\n",
      "Epoch 60/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9816\n",
      "Epoch 00060: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 0.0386 - val_accuracy: 0.9838\n",
      "Epoch 61/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9898\n",
      "Epoch 00061: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 6s 57ms/step - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.0597 - val_accuracy: 0.9753\n",
      "Epoch 62/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9860\n",
      "Epoch 00062: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 6s 56ms/step - loss: 0.0359 - accuracy: 0.9861 - val_loss: 0.0358 - val_accuracy: 0.9895\n",
      "Epoch 63/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9876\n",
      "Epoch 00063: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0303 - accuracy: 0.9876 - val_loss: 0.0372 - val_accuracy: 0.9876\n",
      "Epoch 64/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9882\n",
      "Epoch 00064: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.0326 - accuracy: 0.9883 - val_loss: 0.0424 - val_accuracy: 0.9876\n",
      "Epoch 65/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9842\n",
      "Epoch 00065: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0367 - accuracy: 0.9842 - val_loss: 0.0486 - val_accuracy: 0.9791\n",
      "Epoch 66/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0409 - accuracy: 0.9876\n",
      "Epoch 00066: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0441 - val_accuracy: 0.9914\n",
      "Epoch 67/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9886\n",
      "Epoch 00067: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 55ms/step - loss: 0.0330 - accuracy: 0.9886 - val_loss: 0.0424 - val_accuracy: 0.9876\n",
      "Epoch 68/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9851\n",
      "Epoch 00068: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.0429 - accuracy: 0.9851 - val_loss: 0.0382 - val_accuracy: 0.9876\n",
      "Epoch 69/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0345 - accuracy: 0.9876\n",
      "Epoch 00069: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0348 - accuracy: 0.9873 - val_loss: 0.0330 - val_accuracy: 0.9895\n",
      "Epoch 70/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9908\n",
      "Epoch 00070: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 54ms/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.0366 - val_accuracy: 0.9914\n",
      "Epoch 71/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9882\n",
      "Epoch 00071: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.0316 - accuracy: 0.9883 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
      "Epoch 72/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0288 - accuracy: 0.9901\n",
      "Epoch 00072: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.0286 - accuracy: 0.9902 - val_loss: 0.0450 - val_accuracy: 0.9867\n",
      "Epoch 73/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9902\n",
      "Epoch 00073: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.0315 - accuracy: 0.9902 - val_loss: 0.0364 - val_accuracy: 0.9952\n",
      "Epoch 74/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9847\n",
      "Epoch 00074: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.0320 - val_accuracy: 0.9857\n",
      "Epoch 75/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9883\n",
      "Epoch 00075: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.0332 - accuracy: 0.9883 - val_loss: 0.0312 - val_accuracy: 0.9895\n",
      "Epoch 76/200\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9902\n",
      "Epoch 00076: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 52ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 0.0306 - val_accuracy: 0.9886\n",
      "Epoch 77/200\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9888\n",
      "Epoch 00077: val_loss did not improve from 0.03059\n",
      "99/99 [==============================] - 5s 53ms/step - loss: 0.0303 - accuracy: 0.9889 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 480, 27)           189       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 160, 27)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 160, 39)           3198      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 53, 39)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 53, 39)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 223)               234596    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 13)                2912      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 55)                770       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 56        \n",
      "=================================================================\n",
      "Total params: 241,721\n",
      "Trainable params: 241,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "33/33 - 1s - loss: 0.0306 - accuracy: 0.9895\n",
      "27/27 - 0s - loss: 0.0467 - accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAG5CAYAAACnXrwnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAiDElEQVR4nO3debQlZXnv8e8PaERsoGlmGQSVqOhF5KJCzDUgRmmHQIxRSKKoxMYE0agrETU3xqi5aoyzEVHQxghCRAIaZAiDqHHCCRFEOyihEWi6G5BBhu7z3D92NR5I9xlgn7NPVX0/a9U6Nby76t2HXjzneeqtt1JVSJKk9thg1B2QJEnTY/CWJKllDN6SJLWMwVuSpJYxeEuS1DIGb0mSWsbgLWlSSX6R5JlTaLdrkkqy0Wz0S+org7daJckfJ7kkyW1Jrkvy5SS/0xz7uyZwvGhc+42afbs2259utp8yrs2jk0x7woMkFyX5s/UcOyLJT5LcmuSGJGcl2azp723Nck+Su8dtH5tk/6Z/p9/vfE9s9l803X5K6h6Dt1ojyeuBDwD/AGwH7AL8M3DwuGargLcl2XCCU60C3jHFa74syaen2c/fbfp4WFVtBjwOOAWgqhZV1fyqmg98FnjP2u2qelVzihuB/ZJsNe60hwM/nU4/JHWXwVutkGQL4O+Bo6rqC1V1e1XdU1VfrKq/Gtf0bOBu4E8nON0SYM8myM6EJwPfqKrvA1TVqqpaUlW3TvHzdwP/BhwK0Pwh8mIGwX6dxpWrX57kmiQ3JXlVkicnuTTJzUk+Mq79Bkn+JsnVSZYnObH5Ha89/pLm2Mokb7nftTZIckyS/2qOn5pk4RS/m6QhMHirLfYDNgFOn6RdAf8XeGuSeetpcweDzPidw+vefXwLeHaStyV5WpKHPIBznAi8tFl/NnAZ8MspfO6pwO4Mgv0HgLcAzwQeD7xo3B8sL2uWA4BHAvOBjwAk2QP4GPAS4OHAVsBO465xNHAI8LvN8ZuAj07z+0l6EAzeaoutgBVVtXqyhlV1JoPS8zrvRzc+DuySZNGQ+jf++l8FXgDsDfw7sDLJ+yYp5d//HP8JLEzyGAZB/MQpfvTtVXVnVZ0L3A6cXFXLq+pa4KvAk5p2fwK8r6quqqrbgDcBhzYDzV4IfKmqLq6quxj8MTQ27hqvAt5SVcua438HvNBBatLsMXirLVYCW08jQPwNg6xzk3UdbILO25vlPpL8c1NmvpnBPfU/Xrud5NKpXLyqvlxVzwcWMrgn/zIm/mNiXT4DvJpBdjxZxWGtG8at/3od2/Ob9YcDV487djWwEYOxBA8Hrll7oKpuZ/D7X+sRwOnjfkdXAGuaz0qaBQZvtcU3gLsYlGsnVVXnAUuBv5ig2aeABQyy5PGf/YuqWlBVC5rPn7R2u6r2nE6nq2qsqs4HLgCeMJ3PMgjefwGcVVV3TPOzk/klgyC81i7AagbB/jpg57UHkmzKoPKx1jXAonG/kwVVtUmT3UuaBQZvtUJV3QL8LfDRJIck2TTJvCSLkrxnPR97C/DXE5xzNfBW4I0PomsbJdlk3DIvycFJDk2yZQaewuD+8Denc+Kq+nnzubdM1vYBOBl4XZLdksxnMAbglOZ38nngeUl+J8nGDAYKjv9/xbHAO5M8AiDJNkkORtKsMXirNarqn4DXMyiJ38ggA3w1g5HZ62r/deDbk5z2ZAaZ5gP1MQbl6LXLpxgM4Hol8DPgV8C/AP9YVesdLb4+VfW1qprKQLXpOoFBZn8x8HPgTgYD0aiqHwNHAScx+N3cBCwb99kPAmcC5ya5lcEfJU+dgT5KWo9UTXtuCkmSNEJm3pIktYzBW5KkljF4S5LUMgZvSZJaxuAtSVLLGLx7LslBSa5MsjTJMaPujzQbkpzQvJDlslH3RXogDN491sy1/VFgEbAHcFjzUgqp6z4NHDTqTkgPlMG7354CLG1eTnE38Dnu+25sqZOq6mIG73WXWsng3W87Mu4FFAxm0dpxRH2RJE2RwVuSpJYxePfbtYx7exSwU7NPkjSHGbz77TvA7s2bpTYGDmXwwglJ0hxm8O6x5vWPrwbOAa4ATm3eKCV1WpKTGbwj/jFJliU5YtR9kqbDt4pJktQyZt6SJLWMwVuSpJYxeEuS1DIGb0mSWsbgLQCSLB51H6TZ5r97tZXBW2v5PzH1kf/u1UoGb0mSWmZOPee99cINa9ed5426G71048o1bLPVhqPuRi/99EcPG3UXeuueupN52WTU3eilO+t27q47M1vXe/YBD6uVq9YM5VzfvfSuc6pqpK+U3WiUF7+/XXeex7fP2XnyhlKHHLTbU0fdBWnWffOuL8/q9VauWsO3z9llKOfacIefbT2UEz0Icyp4S5I0EwoYY2zU3Rga73lLktQyZt6SpB4o1lR3Mm+DtySp8wZl87kzQPvBsmwuSVLLmHlLknrBAWuSJLVIUayp4SxTkWRBks8n+UmSK5Lsl2RhkvOS/Kz5uWXTNkk+lGRpkkuT7D3Z+Q3ekiQN3weBs6vqscATgSuAY4Dzq2p34PxmG2ARsHuzLAY+NtnJDd6SpF4Yo4ayTCbJFsDTgeMBquruqroZOBhY0jRbAhzSrB8MnFgD3wQWJNlhomt4z1uS1HkFrBneaPOtk1wybvu4qjpu3PZuwI3Ap5I8Efgu8Fpgu6q6rmlzPbBds74jcM24zy9r9l3Hehi8JUmanhVVtc8ExzcC9gaOrqpvJfkgvymRA1BVleQB/zVh2VyS1AuzVTZnkDkvq6pvNdufZxDMb1hbDm9+Lm+OXwuMf7HHTs2+9TJ4S5I6r2DWRptX1fXANUke0+w6ELgcOBM4vNl3OHBGs34m8NJm1Pm+wC3jyuvrZNlckqThOxr4bJKNgauAlzNImE9NcgRwNfCipu1ZwHOApcAdTdsJGbwlSb0wm1O0VNUPgHXdFz9wHW0LOGo65zd4S5I6r6hhjjYfOe95S5LUMmbekqTuK1jTncTb4C1J6r7BK0G7w7K5JEktY+YtSeqBsIaMuhNDY/CWJHVeAWMduudt2VySpJYx85Yk9YJlc0mSWmTwStDuBG/L5pIktYyZtySpF8aqO5m3wVuS1HmWzSVJ0kiZeUuSOq8IazqUrxq8JUm94D1vSZJaxHvekiRppMy8JUk9ENZUd/JVg7ckqfMG7/PuTvDuzjeRJKknzLwlSb3QpQFrBm9JUudVdeued3e+iSRJPWHmLUnqhTHL5pIktcdgkpbuFJu7800kSeoJM29JUg90a8CawVuS1HlO0iJJkkbKzFuS1AtrfCWoJEntUcTR5pIkaXTMvCVJvTDmaHNJktrDSVokSdJImXlLkjqviKPNJUlqGydpkSRJI2PmLUnqvCqc21ySpHZJp97n3Z0/QyRJ6gkzb0lS5xWWzSVJah0naZEkSSNj5i1J6rwijDlJiyRJ7WLZXJIkjYyZtySp8wpfCSpJUsuENU7SIkmSRsXMW5LUeZbNJUlqIcvmkiRpvZL8IsmPkvwgySXNvoVJzkvys+bnls3+JPlQkqVJLk2y92TnN3hLkjqvKozVBkNZpuGAqtqrqvZpto8Bzq+q3YHzm22ARcDuzbIY+NhkJ7ZsLknqhTnwYpKDgf2b9SXARcAbm/0nVlUB30yyIMkOVXXd+k408m8iSVLLbJ3kknHL4nW0KeDcJN8dd3y7cQH5emC7Zn1H4Jpxn13W7FsvM29JUucVMDa8AWsrxpXC1+d3quraJNsC5yX5yX36U1VJ6oF2wOAtSeqBzGrZvKqubX4uT3I68BTghrXl8CQ7AMub5tcCO4/7+E7NvvWybC5J0hAleViSzdauA88CLgPOBA5vmh0OnNGsnwm8tBl1vi9wy0T3u8HMW5LUA4NJWmbtOe/tgNOTwCDOnlRVZyf5DnBqkiOAq4EXNe3PAp4DLAXuAF4+2QUM3pKkXpitV4JW1VXAE9exfyVw4Dr2F3DUdK5h2VySpJYx85YkdV6R2SybzziDtySpF8Y6VGzuzjeRJKknzLwlSZ1XBWssm0uS1C5duudt2VySpJYx85Ykdd5gtHl38lWDtySpF9YM78UkI2fw1m9kM7LFP8BGuwNQtxwDG2xP5r8GNnoUtfIPYfVlTeONyBbvhI0eD9mI+vXpcPvHR9d36QF6/bGvZN9Fe3Hzjb9i8T5vAuAlb3kBi16xP7fceCsAJ7z1VL5zzg9H2U09SLM8PeqMM3jrXtn8b6i7LoabjwbmQTaBDW6lbj6KbPH2+zbeZBGwMbXyecAmZJsvU3d+CdZM+CIcac457zMXc+ax5/HXnzzyPvu/8OGz+fwHzhpRr6SJGbw1kPkw78lwyxubHfdA3QNrbl3PBwqyKbDhIMjXPTB22yx1VhqeH339SrbbZetRd0Mzrlv3vLvzTfTgbLgzjK0iW7ybbHUG2fydkIeuv/2dZ0PdQbb9T7LNV6jbj4e6Zfb6K82w33/V73Hst/+B1x/7SuYv2HTU3dEQjJGhLHPBjAbvJAcluTLJ0iTHzOS19GBtCPMeT91xErXyYKhfk4cduf7m8/YExqjlT6NWHEAe9orBHwBSB3zxE//By/Z4PX/+1Lew6vqbWfyuPxl1l6T7mLHgnWRD4KPAImAP4LAke8zU9fQgjV0/WO4ZDMqpO88eDEZbj2zy/MH9cVbD2Cq4+3sw7wmz1FlpZt28/FeMjRVVxZdPuJDH7vPIUXdJD9LaGdaGscwFM5l5PwVYWlVXVdXdwOeAg2fwenowxlbAmutgw90AyEP2gzVL19u8xq4jG+832MhDYeO9YPVVs9BRaeYt3H7BvetPO3gffnH5stF1RkMzVhsMZZkLZnLA2o7ANeO2lwFPvX+jJIuBxQC77Oj4uVGqX72dLPgnYB6suWbwqNhDfo9s/rewwUKy5Sdg9RXUTa+AO/4FtngX2eosSKg7ToPVV476K0jT9qYlR7Hn/3kcW2w9n88u/RCfeftp7Pn0x/GoPR9BVXHD1Sv44NEnjLqb0n2MPFpW1XHAcQD7PHGTGnF3+m31FdTKF9x3313nUTee9z/b1h3Uza+ZnX5JM+j/Hf7R/7Hv7CVfGUFPNJN8n/fUXQuMH8G0U7NPkqRZN1dGig/DTBbvvwPsnmS3JBsDhwJnzuD1JEnqhRnLvKtqdZJXA+cAGwInVNWPZ+p6kiStj9OjTkNVnQU4v6AkaeTmykjxYejON5EkqSdGPtpckqQZV442lySpVQpHm0uSpBEy85Yk9YJlc0mSWqRrj4pZNpckqWXMvCVJvdClzNvgLUnqvK69mMSyuSRJLWPmLUnqhS49523wliR1X3Xrnrdlc0mSWsbMW5LUeV17ztvgLUnqhS4Fb8vmkiS1jJm3JKnzuvact8FbktQL1aHgbdlckqSWMfOWJPWCk7RIktQi5SQtkiRplMy8JUm90KUBawZvSVIPdOtRMcvmkiS1jJm3JKkXLJtLktQiXXsxiWVzSZJaxsxbktR9NXjWuysM3pKkXujSDGuWzSVJahmDtySp84rBaPNhLFORZMMk30/ypWZ7tyTfSrI0ySlJNm72P6TZXtoc33Uq5zd4S5J6YDBJyzCWKXotcMW47XcD76+qRwM3AUc0+48Abmr2v79pNymDtyRJQ5RkJ+C5wCeb7QDPAD7fNFkCHNKsH9xs0xw/sGk/IQesSZJ6YYijzbdOcsm47eOq6rhx2x8A/hrYrNneCri5qlY328uAHZv1HYFrBv2r1UluadqvmKgDBm9JUi8McYa1FVW1z7oOJHkesLyqvptk/2Fd8P4M3pIkDc/TgN9P8hxgE2Bz4IPAgiQbNdn3TsC1TftrgZ2BZUk2ArYAVk52Ee95S5I6r2p2RptX1Zuqaqeq2hU4FLigqv4EuBB4YdPscOCMZv3MZpvm+AVVkxf4zbwlSb0w4rnN3wh8Lsk7gO8Dxzf7jwc+k2QpsIpBwJ+UwVuSpBlQVRcBFzXrVwFPWUebO4E/mu65Dd6SpF5wbnNJklrG93lLktQixdSnNm0DR5tLktQyZt6SpF7o0C1vg7ckqQeqW/e8LZtLktQyZt6SpH7oUN3c4C1J6gXL5pIkaWTMvCVJveAMa5IktUhh2VySJI2QmbckqfsK6FDmbfCWJPVCl+55WzaXJKllzLwlSf3Qoczb4C1J6gFfCSpJkkbIzFuS1A+WzSVJahFfCSpJkkbJzFuS1A+WzSVJahvL5pIkaUTMvCVJ/WDZXJKklulQ8LZsLklSy5h5S5K6z1eCSpLUPr4SVJIkjYyZtySpHzqUeRu8JUn90KF73pbNJUlqGTNvSVIvxLK5JEktUnTqnrdlc0mSWma9mXeSDzPB3ylV9ZoZ6ZEkSUOXTg1Ym6hsfsms9UKSpJnWobL5eoN3VS2ZzY5IkqSpmXTAWpJtgDcCewCbrN1fVc+YwX5JkjRcHcq8pzJg7bPAFcBuwNuAXwDfmcE+SZI0fDWkZQ6YSvDeqqqOB+6pqq9U1SsAs25JkkZkKs9539P8vC7Jc4FfAgtnrkuSJA1ZD18J+o4kWwBvAD4MbA68bkZ7JUnSkPVqhrWq+lKzegtwwMx2R5IkTWYqo80/xTpu0Tf3viVJaoc+Zd7Al8atbwL8AYP73pIkaQSmUjY/bfx2kpOBr81YjyRJ0oQeyFvFdge2HXZHAH566aY8++F7zcSppTlrxZF7j7oL0qxbfdpXZv2avRqwluRW7nun4HoGM65JktQefXpUrKo2m42OSJKkqZl0hrUk509lnyRJc9awpkadQuk9ySZJvp3kh0l+nORtzf7dknwrydIkpyTZuNn/kGZ7aXN818musd7g3Vx8IbB1ki2TLGyWXYEdJ+++JElzyOzNbX4X8IyqeiKwF3BQkn2BdwPvr6pHAzcBRzTtjwBuava/v2k3oYky7yOB7wKPbX6uXc4APjKl7kuSNEekhrNMpgZuazbnNUsxeC/I55v9S4BDmvWDm22a4wcmmfAG/UTv8/4g8MEkR1fVhyfvriRJvbB1kkvGbR9XVceNb5BkQwYJ76OBjwL/BdxcVaubJsv4TRV7R+AagKpaneQWYCtgxfo6MJVHxcaSLKiqm5sObQkcVlX/PIXPSpI0NwzvUbEVVbXPhJeqWgPslWQBcDqDKvbQTOWVoK9cG7ibDt0EvHKYnZAkacaN4H3eTfy8ENgPWJBkbdK8E3Bts34tsDNAc3wLYOVE551K8N5wfO29KQVsPJ3OS5LUF0m2aTJukjwU+D3gCgZB/IVNs8MZjCEDOLPZpjl+QVVN+GfCVMrmZwOnJPl4s30k8OUpfgdJkkZuqoPNhmQHYEmT7G4AnFpVX0pyOfC5JO8Avg8c37Q/HvhMkqXAKuDQyS4wleD9RmAx8Kpm+1Jg+2l9DUmSRm2WZlirqkuBJ61j/1XAU9ax/07gj6ZzjUnL5lU1BnwL+EVz0WcwSP8lSdIIrDfzTvJbwGHNsgI4BaCqDpidrkmSNEQ9eTHJT4CvAs+rqqUASV43K72SJGnIuvRWsYnK5i8ArgMuTPKJJAcC3XkliyRJLbXe4F1V/1ZVhzJ4sPxC4C+BbZN8LMmzZql/kiQNxwie854pUxmwdntVnVRVz2fwUPn38X3ekqQ2GdK85nOl9D6VSVruVVU3VdVxVXXgTHVIkiRNbCrPeUuS1H5zJGseBoO3JKkfOhS8p1U2lyRJo2fmLUnqhbky2GwYzLwlSWoZg7ckSS1j2VyS1A8dKpsbvCVJ3TeHJlgZBsvmkiS1jJm3JKkfOpR5G7wlSf3QoeBt2VySpJYx85YkdV7o1oA1g7ckqR86FLwtm0uS1DJm3pKk7uvYc94Gb0lSP3QoeFs2lySpZcy8JUn90KHM2+AtSeqFLt3ztmwuSVLLmHlLkvqhQ5m3wVuS1H1Fp4K3ZXNJklrGzFuS1AtdGrBm8JYk9UOHgrdlc0mSWsbMW5LUC5bNJUlqmw4Fb8vmkiS1jJm3JKn7Ovact8FbktR5aZausGwuSVLLmHlLkvrBsrkkSe3SpUfFLJtLktQyZt6SpH7oUOZt8JYk9UOHgrdlc0mSWsbMW5LUfdWtAWsGb0lSPxi8JUlqly5l3t7zliSpZcy8JUn90KHM2+AtSeoFy+aSJGlkDN6SpO6rIS6TSLJzkguTXJ7kx0le2+xfmOS8JD9rfm7Z7E+SDyVZmuTSJHtPdg2DtySpH2YpeAOrgTdU1R7AvsBRSfYAjgHOr6rdgfObbYBFwO7Nshj42GQXMHhLkjREVXVdVX2vWb8VuALYETgYWNI0WwIc0qwfDJxYA98EFiTZYaJrOGBNktR5YagD1rZOcsm47eOq6rh1XjfZFXgS8C1gu6q6rjl0PbBds74jcM24jy1r9l3Hehi8JUn9MLzgvaKq9pmsUZL5wGnAX1bVr5L8pitVlTzwPycsm0uSNGRJ5jEI3J+tqi80u29YWw5vfi5v9l8L7Dzu4zs1+9bL4C1J6oVUDWWZ9DqDFPt44Iqqet+4Q2cChzfrhwNnjNv/0mbU+b7ALePK6+tk2VyS1H1THyk+DE8DXgL8KMkPmn1vBt4FnJrkCOBq4EXNsbOA5wBLgTuAl092AYO3JElDVFVfYzBGbl0OXEf7Ao6azjUM3pKkXujS9KgGb0lSP3QoeDtgTZKkljHzliT1gmVzSZLapkPB27K5JEktY+YtSeq+smwuSVL7dCh4WzaXJKllzLwlSZ035FeCjpzBW5LUD1N4qUhbWDaXJKllzLwlSb1g2VySpDaZ3VeCzjjL5pIktYyZt+71huP/nKc+939z8/JbWLznG0bdHWlWbLflfN5++CK22nxTqorTvvYjTr7w+zxz79151XP3Y7ftt+Il7z6Jy//7hns/84pnP5mDf/t/MVZjvOeUC/nGFVeP8BtoqjI26h4Mj5m37nXupy/izYveOepuSLNqzZrifad9hT/8+yW89D0n8+Lf3YtHbr+Q//rlSt5w3Bf53tJl92n/yO0X8ux9HssL376Eoz78Bd502IFskIyo95qWGtIyBxi8da8fffUKbl1126i7Ic2qFb+6nZ9csxyAO+66h59fv5JtFszn59ev4uobbvof7fd/4qM455KfcM/qNfxy5a+45sabecKu2892t9VzBm9JauywcHMes/O2XPaL69fbZpsFm3H9Tb/5I3f5Tbex7YL5s9E9PUip4SxzwYwF7yQnJFme5LKZuoYkDctDHzKP9x75fN77rxdx+513j7o7GrZiMEnLMJY5YCYz708DB83g+SVpKDbaYAPeu/j5fPnbV3DBD5ZO2PbGm29l+y1/k2lvu+V8lt/s7SbNrhkL3lV1MbBqps4vScPy1pc8i59fv4p/Of97k7a96NKrePY+j2XeRhvy8K02Z5dtF0xYZtfc0aWy+cgfFUuyGFgMsAmbjrg3/fbmz76WPfd/PFtsvRkn/fexnPh3p3L2CReMulvSjNrrUQ/nefvuwU+X3cjn3vynAHzkjK8zb6MNeeOLD2DL+Q/lQ0cdwpXLbuSoD3+Bq65bybnfvZLT/vZw1oyN8a7PXcDYHCmlahId+s+UmsF/dEl2Bb5UVU+YSvvNs7CemgNnrD/SXLTiyP1G3QVp1l152vu5Y/k1s/aM3fwtd669DnjtUM719dP/6rtVtc9QTvYAjTzzliRppvlKUEmS2mYOjRQfhpl8VOxk4BvAY5IsS3LETF1LkqQ+mbHMu6oOm6lzS5I0XZbNJUlqmw4Fb6dHlSSpZcy8JUm9YNlckqQ2KWCsO9HbsrkkSS1j5i1J6ofuJN4Gb0lSP3Tpnrdlc0mSWsbMW5LUDx2aHtXgLUnqBcvmkiRpZMy8JUndVzjaXJKkNhm8z7s70dvgLUnqh7FRd2B4vOctSVLLmHlLknrBsrkkSW3SsQFrls0lSWoZM29JUg+UM6xJktQ2zrAmSZJGxsxbktQPls0lSWqRgjhJiyRJGhUzb0lSP3SobG7mLUnqhxrSMokkJyRZnuSycfsWJjkvyc+an1s2+5PkQ0mWJrk0yd5T+SoGb0mShuvTwEH323cMcH5V7Q6c32wDLAJ2b5bFwMemcgGDtySpF1I1lGUyVXUxsOp+uw8GljTrS4BDxu0/sQa+CSxIssNk1/CetySpH4Z3z3vrJJeM2z6uqo6b5DPbVdV1zfr1wHbN+o7ANePaLWv2XccEDN6SJE3Piqra54F+uKoqeXDzvRm8JUndV8Bon/O+IckOVXVdUxZf3uy/Fth5XLudmn0T8p63JKnzwnDudz+Id4KfCRzerB8OnDFu/0ubUef7AreMK6+vl5m3JElDlORkYH8G98aXAW8F3gWcmuQI4GrgRU3zs4DnAEuBO4CXT+UaBm9JUj/M0iQtVXXYeg4duI62BRw13WsYvCVJ/eAMa5IkaVTMvCVJ3Tf60eZDZfCWJPXCgxgpPudYNpckqWXMvCVJ/dChzNvgLUnqgepU8LZsLklSy5h5S5K6r+hU5m3wliT1Q4ceFbNsLklSy5h5S5J6oUvPeRu8JUn90KHgbdlckqSWMfOWJHVfAWPdybwN3pKkHnCSFkmSNEJm3pKkfuhQ5m3wliT1Q4eCt2VzSZJaxsxbktR9jjaXJKltCqo7k5tbNpckqWXMvCVJ/dChAWsGb0lS93Xsnrdlc0mSWsbMW5LUD5bNJUlqmQ4Fb8vmkiS1jJm3JKkHuvVWMYO3JKn7ChhzkhZJkjQiZt6SpH6wbC5JUssYvCVJapNyhjVJkjQ6Zt6SpO4rqA69EtTgLUnqB8vmkiRpVMy8JUn94GhzSZJapMoZ1iRJ0uiYeUuS+sGyuSRJ7VKWzSVJ0qiYeUuSesD3eUuS1C6Fk7RIkqTRMfOWJPWDc5tLktQeBZRlc0mSNCpm3pKk7quybC5JUttYNpckSSNj5i1J6ocOlc1Tc2jGmSQ3AlePuh89tTWwYtSdkGaZ/+5H5xFVtc1sXSzJ2Qz+ew/Diqo6aEjnekDmVPDW6CS5pKr2GXU/pNnkv3u1lfe8JUlqGYO3JEktY/DWWseNugPSCPjvXq1k8BYAVeX/xCaQZE2SHyS5LMm/Jtn0QZzr00le2Kx/MskeE7TdP8lvP4Br/CLJsAbndJb/7tVWBm9pan5dVXtV1ROAu4FXjT+Y5AE9dllVf1ZVl0/QZH9g2sFbUrcZvKXp+yrw6CYr/mqSM4HLk2yY5B+TfCfJpUmOBMjAR5JcmeQ/gG3XnijJRUn2adYPSvK9JD9Mcn6SXRn8kfC6Juv/P0m2SXJac43vJHla89mtkpyb5MdJPglkln8nkmaRk7RI09Bk2IuAs5tdewNPqKqfJ1kM3FJVT07yEODrSc4FngQ8BtgD2A64HDjhfufdBvgE8PTmXAuralWSY4Hbquq9TbuTgPdX1deS7AKcAzwOeCvwtar6+yTPBY6Y0V+EpJEyeEtT89AkP2jWvwocz6Cc/e2q+nmz/1nAnmvvZwNbALsDTwdOrqo1wC+TXLCO8+8LXLz2XFW1aj39eCawR3JvYr15kvnNNV7QfPbfk9z0wL6mpDYweEtT8+uq2mv8jiaA3j5+F3B0VZ1zv3bPGWI/NgD2rao719EXST3hPW9peM4B/jzJPIAkv5XkYcDFwIube+I7AAes47PfBJ6eZLfmswub/bcCm41rdy5w9NqNJHs1qxcDf9zsWwRsOawvJWnuMXhLw/NJBvezv5fkMuDjDKpbpwM/a46dCHzj/h+sqhuBxcAXkvwQOKU59EXgD9YOWANeA+zTDIi7nN+Men8bg+D/Ywbl8/+eoe8oaQ5wbnNJklrGzFuSpJYxeEuS1DIGb0mSWsbgLUlSyxi8JUlqGYO3JEktY/CWJKll/j/EbEgFdDKnZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.03059057891368866\n",
      "test_loss: 0.046724833548069\n",
      "test_acc: 0.9810426831245422\n",
      "precision: 0.9333333333333333\n",
      "recall: 0.995260663507109\n",
      "specificity 0.976303317535545\n",
      "sensitivity :  0.995260663507109\n",
      "far 0.023696682464454975\n",
      "frr 0.004739336492890996\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 27, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 39, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "# conv_3 = tf.keras.layers.Conv1D(filters = 47, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "# max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.4717023407916654)(max_2)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(223)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(13, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(55, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_2)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_3)\n",
    "\n",
    "# Adam\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.00036794403873239846), metrics = ['accuracy'])\n",
    "# SGD\n",
    "# model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.0461300729767683, momentum=0.4411297369087802), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "        \n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
