{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "academic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incorporate-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow = 2.\n",
    "# python = 3.6\n",
    "\n",
    "\n",
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "\n",
    "\n",
    "val_loss_all = []\n",
    "\n",
    "test_loss_all = []\n",
    "test_acc_all = []\n",
    "test_pre_all = []\n",
    "frr_all = []\n",
    "far_all = []\n",
    "\n",
    "conf_matrix_sco = []\n",
    "test_pre_sco = []\n",
    "test_rec_sco = []\n",
    "test_spedi_sco = []\n",
    "test_sensi_sco = []\n",
    "\n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reduced-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "authentic-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub number\n",
    "sub_num = 7\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stable-belle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.4982 - accuracy: 0.7538 - val_loss: 0.4668 - val_accuracy: 0.7367\n",
      "Epoch 2/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.4061 - accuracy: 0.7945\n",
      "Epoch 00002: val_loss improved from inf to 0.43274, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.4079 - accuracy: 0.7934 - val_loss: 0.4327 - val_accuracy: 0.7928\n",
      "Epoch 3/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3989 - accuracy: 0.8196\n",
      "Epoch 00003: val_loss improved from 0.43274 to 0.42592, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3986 - accuracy: 0.8210 - val_loss: 0.4259 - val_accuracy: 0.8184\n",
      "Epoch 4/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8273\n",
      "Epoch 00004: val_loss improved from 0.42592 to 0.41431, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3942 - accuracy: 0.8273 - val_loss: 0.4143 - val_accuracy: 0.8222\n",
      "Epoch 5/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3812 - accuracy: 0.8341\n",
      "Epoch 00005: val_loss improved from 0.41431 to 0.41043, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3807 - accuracy: 0.8352 - val_loss: 0.4104 - val_accuracy: 0.8308\n",
      "Epoch 6/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8376\n",
      "Epoch 00006: val_loss improved from 0.41043 to 0.40230, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3788 - accuracy: 0.8384 - val_loss: 0.4023 - val_accuracy: 0.8270\n",
      "Epoch 7/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3704 - accuracy: 0.8473\n",
      "Epoch 00007: val_loss did not improve from 0.40230\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3685 - accuracy: 0.8489 - val_loss: 0.4226 - val_accuracy: 0.8194\n",
      "Epoch 8/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3686 - accuracy: 0.8425\n",
      "Epoch 00008: val_loss improved from 0.40230 to 0.38295, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3689 - accuracy: 0.8435 - val_loss: 0.3829 - val_accuracy: 0.8432\n",
      "Epoch 9/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8663\n",
      "Epoch 00009: val_loss improved from 0.38295 to 0.37036, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3441 - accuracy: 0.8660 - val_loss: 0.3704 - val_accuracy: 0.8574\n",
      "Epoch 10/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3280 - accuracy: 0.8766\n",
      "Epoch 00010: val_loss did not improve from 0.37036\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3287 - accuracy: 0.8755 - val_loss: 0.4114 - val_accuracy: 0.8042\n",
      "Epoch 11/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8631\n",
      "Epoch 00011: val_loss improved from 0.37036 to 0.34483, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3394 - accuracy: 0.8634 - val_loss: 0.3448 - val_accuracy: 0.8840\n",
      "Epoch 12/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3202 - accuracy: 0.8872\n",
      "Epoch 00012: val_loss did not improve from 0.34483\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3211 - accuracy: 0.8869 - val_loss: 0.3450 - val_accuracy: 0.8679\n",
      "Epoch 13/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3035 - accuracy: 0.8818\n",
      "Epoch 00013: val_loss improved from 0.34483 to 0.33232, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3020 - accuracy: 0.8821 - val_loss: 0.3323 - val_accuracy: 0.8945\n",
      "Epoch 14/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.3011 - accuracy: 0.8921\n",
      "Epoch 00014: val_loss improved from 0.33232 to 0.33221, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.3029 - accuracy: 0.8907 - val_loss: 0.3322 - val_accuracy: 0.8745\n",
      "Epoch 15/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.8950\n",
      "Epoch 00015: val_loss improved from 0.33221 to 0.31700, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2955 - accuracy: 0.8945 - val_loss: 0.3170 - val_accuracy: 0.8992\n",
      "Epoch 16/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.8937\n",
      "Epoch 00016: val_loss did not improve from 0.31700\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2967 - accuracy: 0.8939 - val_loss: 0.3377 - val_accuracy: 0.8878\n",
      "Epoch 17/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.8924\n",
      "Epoch 00017: val_loss improved from 0.31700 to 0.30366, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2897 - accuracy: 0.8929 - val_loss: 0.3037 - val_accuracy: 0.8983\n",
      "Epoch 18/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2808 - accuracy: 0.8898\n",
      "Epoch 00018: val_loss did not improve from 0.30366\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2796 - accuracy: 0.8907 - val_loss: 0.3084 - val_accuracy: 0.9106\n",
      "Epoch 19/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2703 - accuracy: 0.9024\n",
      "Epoch 00019: val_loss did not improve from 0.30366\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2705 - accuracy: 0.9027 - val_loss: 0.3332 - val_accuracy: 0.8764\n",
      "Epoch 20/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2831 - accuracy: 0.8898\n",
      "Epoch 00020: val_loss improved from 0.30366 to 0.30001, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.2815 - accuracy: 0.8904 - val_loss: 0.3000 - val_accuracy: 0.8983\n",
      "Epoch 21/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.9011\n",
      "Epoch 00021: val_loss improved from 0.30001 to 0.29131, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2653 - accuracy: 0.9005 - val_loss: 0.2913 - val_accuracy: 0.9059\n",
      "Epoch 22/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2620 - accuracy: 0.9037\n",
      "Epoch 00022: val_loss improved from 0.29131 to 0.28042, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2615 - accuracy: 0.9043 - val_loss: 0.2804 - val_accuracy: 0.9144\n",
      "Epoch 23/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9127\n",
      "Epoch 00023: val_loss did not improve from 0.28042\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2446 - accuracy: 0.9129 - val_loss: 0.3279 - val_accuracy: 0.8878\n",
      "Epoch 24/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2438 - accuracy: 0.9101\n",
      "Epoch 00024: val_loss did not improve from 0.28042\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2446 - accuracy: 0.9091 - val_loss: 0.3321 - val_accuracy: 0.8460\n",
      "Epoch 25/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2504 - accuracy: 0.8985\n",
      "Epoch 00025: val_loss did not improve from 0.28042\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2499 - accuracy: 0.8996 - val_loss: 0.3233 - val_accuracy: 0.8821\n",
      "Epoch 26/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9108\n",
      "Epoch 00026: val_loss did not improve from 0.28042\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2372 - accuracy: 0.9106 - val_loss: 0.3690 - val_accuracy: 0.8726\n",
      "Epoch 27/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9034\n",
      "Epoch 00027: val_loss improved from 0.28042 to 0.27648, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2520 - accuracy: 0.9027 - val_loss: 0.2765 - val_accuracy: 0.8935\n",
      "Epoch 28/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9130\n",
      "Epoch 00028: val_loss did not improve from 0.27648\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2291 - accuracy: 0.9132 - val_loss: 0.3092 - val_accuracy: 0.8935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9091\n",
      "Epoch 00029: val_loss improved from 0.27648 to 0.26136, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2303 - accuracy: 0.9081 - val_loss: 0.2614 - val_accuracy: 0.9068\n",
      "Epoch 30/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2088 - accuracy: 0.9227\n",
      "Epoch 00030: val_loss did not improve from 0.26136\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2086 - accuracy: 0.9227 - val_loss: 0.2790 - val_accuracy: 0.9087\n",
      "Epoch 31/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9233\n",
      "Epoch 00031: val_loss did not improve from 0.26136\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2136 - accuracy: 0.9233 - val_loss: 0.3155 - val_accuracy: 0.8508\n",
      "Epoch 32/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2090 - accuracy: 0.9214\n",
      "Epoch 00032: val_loss did not improve from 0.26136\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2099 - accuracy: 0.9208 - val_loss: 0.2699 - val_accuracy: 0.9154\n",
      "Epoch 33/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.2166 - accuracy: 0.9201\n",
      "Epoch 00033: val_loss improved from 0.26136 to 0.25143, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.2156 - accuracy: 0.9205 - val_loss: 0.2514 - val_accuracy: 0.9049\n",
      "Epoch 34/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1972 - accuracy: 0.9275\n",
      "Epoch 00034: val_loss did not improve from 0.25143\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1979 - accuracy: 0.9274 - val_loss: 0.2518 - val_accuracy: 0.9116\n",
      "Epoch 35/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9272\n",
      "Epoch 00035: val_loss improved from 0.25143 to 0.20810, saving model to best_model_8.h5\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1821 - accuracy: 0.9274 - val_loss: 0.2081 - val_accuracy: 0.9163\n",
      "Epoch 36/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1724 - accuracy: 0.9278\n",
      "Epoch 00036: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1706 - accuracy: 0.9287 - val_loss: 0.2451 - val_accuracy: 0.9021\n",
      "Epoch 37/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9262\n",
      "Epoch 00037: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1816 - accuracy: 0.9271 - val_loss: 0.2541 - val_accuracy: 0.9011\n",
      "Epoch 38/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9381\n",
      "Epoch 00038: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1542 - accuracy: 0.9385 - val_loss: 0.3300 - val_accuracy: 0.8688\n",
      "Epoch 39/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9343\n",
      "Epoch 00039: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1566 - accuracy: 0.9347 - val_loss: 0.2538 - val_accuracy: 0.9135\n",
      "Epoch 40/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1531 - accuracy: 0.9423\n",
      "Epoch 00040: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1531 - accuracy: 0.9420 - val_loss: 0.2440 - val_accuracy: 0.8992\n",
      "Epoch 41/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9420\n",
      "Epoch 00041: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1475 - accuracy: 0.9417 - val_loss: 0.2653 - val_accuracy: 0.9068\n",
      "Epoch 42/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1514 - accuracy: 0.9381\n",
      "Epoch 00042: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1508 - accuracy: 0.9385 - val_loss: 0.2099 - val_accuracy: 0.9106\n",
      "Epoch 43/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9478\n",
      "Epoch 00043: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1329 - accuracy: 0.9487 - val_loss: 0.2585 - val_accuracy: 0.9163\n",
      "Epoch 44/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9478\n",
      "Epoch 00044: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1379 - accuracy: 0.9480 - val_loss: 0.2704 - val_accuracy: 0.9125\n",
      "Epoch 45/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9507\n",
      "Epoch 00045: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1267 - accuracy: 0.9503 - val_loss: 0.2703 - val_accuracy: 0.9192\n",
      "Epoch 46/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9423\n",
      "Epoch 00046: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1458 - accuracy: 0.9426 - val_loss: 0.2266 - val_accuracy: 0.9183\n",
      "Epoch 47/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9568\n",
      "Epoch 00047: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1118 - accuracy: 0.9572 - val_loss: 0.2530 - val_accuracy: 0.9163\n",
      "Epoch 48/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9549\n",
      "Epoch 00048: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1149 - accuracy: 0.9550 - val_loss: 0.4276 - val_accuracy: 0.9049\n",
      "Epoch 49/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9439\n",
      "Epoch 00049: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1447 - accuracy: 0.9442 - val_loss: 0.2781 - val_accuracy: 0.9116\n",
      "Epoch 50/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9617\n",
      "Epoch 00050: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1009 - accuracy: 0.9607 - val_loss: 0.2716 - val_accuracy: 0.9173\n",
      "Epoch 51/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9607\n",
      "Epoch 00051: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1105 - accuracy: 0.9607 - val_loss: 0.2746 - val_accuracy: 0.9002\n",
      "Epoch 52/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9568\n",
      "Epoch 00052: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1191 - accuracy: 0.9566 - val_loss: 0.2637 - val_accuracy: 0.9078\n",
      "Epoch 53/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.1033 - accuracy: 0.9588\n",
      "Epoch 00053: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1036 - accuracy: 0.9588 - val_loss: 0.2356 - val_accuracy: 0.9030\n",
      "Epoch 54/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0971 - accuracy: 0.9623\n",
      "Epoch 00054: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0969 - accuracy: 0.9623 - val_loss: 0.2829 - val_accuracy: 0.9030\n",
      "Epoch 55/200\n",
      "97/99 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9655\n",
      "Epoch 00055: val_loss did not improve from 0.20810\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0899 - accuracy: 0.9661 - val_loss: 0.2871 - val_accuracy: 0.9002\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 480, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 480, 180)          1260      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 160, 180)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 160, 21)           11361     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 53, 21)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 53, 193)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 17, 193)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 193)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 216)               354240    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 36)                7812      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 231)               8547      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 1856      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 397,437\n",
      "Trainable params: 397,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "33/33 - 0s - loss: 0.2081 - accuracy: 0.9163\n",
      "27/27 - 0s - loss: 0.3024 - accuracy: 0.8661\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.20810016989707947\n",
      "test_loss: 0.3024187386035919\n",
      "test_acc: 0.8661137223243713\n",
      "precision: 0.69140625\n",
      "recall: 0.8388625592417062\n",
      "specificity 0.8751974723538705\n",
      "sensitivity :  0.8388625592417062\n",
      "far 0.12480252764612954\n",
      "frr 0.16113744075829384\n"
     ]
    }
   ],
   "source": [
    "# model CNN-LSTM    \n",
    "inputs = tf.keras.Input(shape = (480, 2))\n",
    "conv_1 = tf.keras.layers.Conv1D(filters = 180, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(inputs)\n",
    "max_1 = tf.keras.layers.MaxPool1D(3)(conv_1)\n",
    "    \n",
    "conv_2 = tf.keras.layers.Conv1D(filters = 21, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_1)\n",
    "max_2 = tf.keras.layers.MaxPool1D(3)(conv_2)\n",
    "    \n",
    "conv_3 = tf.keras.layers.Conv1D(filters = 193, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu')(max_2)\n",
    "max_3 = tf.keras.layers.MaxPool1D(3)(conv_3)\n",
    "    \n",
    "\n",
    "D_out_1 = tf.keras.layers.Dropout(0.5904351267174524)(max_3)\n",
    "    \n",
    "    \n",
    "lstm_1 = tf.keras.layers.LSTM(216)(D_out_1)\n",
    "    \n",
    "dense_1 = tf.keras.layers.Dense(36, activation = 'relu')(lstm_1)\n",
    "dense_2 = tf.keras.layers.Dense(231, activation = 'relu')(dense_1)\n",
    "dense_3 = tf.keras.layers.Dense(8, activation = 'relu')(dense_2)\n",
    "dense_4 = tf.keras.layers.Dense(1, activation = 'sigmoid')(dense_3)\n",
    "    \n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_4)\n",
    "\n",
    "# Adam\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.00048477250069401214), metrics = ['accuracy'])\n",
    "# SGD\n",
    "# model.compile(loss= 'binary_crossentropy', optimizer= tf.keras.optimizers.SGD(learning_rate=0.0461300729767683, momentum=0.4411297369087802), metrics=['accuracy'])\n",
    "    \n",
    "# EarlyStopping 조기종료 및 모델 학습\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
    "check_point = MyModelCheckpoint('best_model_' + str(sub_num + 1) + '.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# EarlyStopping 사용\n",
    "hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [early_stopping, check_point])\n",
    "# EarlyStopping 미사용\n",
    "# hist = model.fit(train_data_set, train_label_set, epochs = 200, batch_size = 32, validation_data = (val_data_set, val_label_set), callbacks = [check_point])\n",
    "        \n",
    "# model save .h5형식\n",
    "model = tf.keras.models.load_model('best_model_' + str(sub_num + 1) + '.h5')\n",
    "model.save('Binary_BOHB_' + str(sub_num + 1) + '.h5')\n",
    "model.summary() \n",
    "        \n",
    "val_loss, val_acc = model.evaluate(val_data_set, val_label_set, verbose = 2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_data_n, test_label, verbose = 2)\n",
    "test_pred = model.predict(test_data_n)\n",
    "        \n",
    "    \n",
    "# 각 행은 1sec, 0.5 <= 자신, 0.5 > 타인\n",
    "for i in range(len(test_pred)):\n",
    "    if(test_pred[i] >= 0.5):\n",
    "        test_pred[i] = 1\n",
    "    \n",
    "    else: \n",
    "        test_pred[i] = 0\n",
    "    \n",
    "    \n",
    "val_loss_all.append(val_loss)\n",
    "    \n",
    "test_loss_all.append(test_loss)\n",
    "test_acc_all.append(test_acc)\n",
    "test_pre_all.append(test_pred)\n",
    "    \n",
    "\n",
    "conf_matrix = confusion_matrix(test_label, test_pred) \n",
    "conf_matrix_sco.append(conf_matrix)\n",
    "    \n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "precision = conf_matrix[1][1] / conf_col[1]\n",
    "recall = conf_matrix[1][1] / conf_row[1]\n",
    "specificity = conf_matrix[0][0] / conf_row[0]\n",
    "sensitivity = conf_matrix[1][1] / conf_row[1]\n",
    "frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "frr_all.append(frr)\n",
    "far_all.append(far)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    " \n",
    "    \n",
    "test_pre_sco.append(precision)\n",
    "test_rec_sco.append(recall)\n",
    "test_spedi_sco.append(specificity)\n",
    "test_sensi_sco.append(sensitivity)\n",
    "    \n",
    "print('val_loss:', val_loss)\n",
    "print('test_loss:', test_loss)\n",
    "print('test_acc:', test_acc)\n",
    "    \n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('specificity', specificity)\n",
    "print('sensitivity : ', sensitivity)\n",
    "print('far', far)\n",
    "print('frr', frr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-january",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
