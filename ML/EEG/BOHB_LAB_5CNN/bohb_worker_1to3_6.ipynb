{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charitable-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 504960, 2)\n",
      "(11, 101280, 2)\n",
      "(11572, 480, 2)\n",
      "(2321, 480, 2)\n",
      "{'dropout_rate': 0.8350894246115568, 'lr': 0.04435978356692676, 'lstm_units': 154, 'num_conv_layers': 3, 'num_fc_units_1': 75, 'num_fc_units_2': 53, 'num_filters_1': 4, 'optimizer': 'Adam', 'num_filters_2': 4, 'num_filters_3': 37}\n",
      "{'loss': 0.26330798864364624, 'info': {'test accuracy': 0.75, 'train accuracy': 0.7544360160827637, 'validation accuracy': 0.7366920113563538, 'number of parameters': 134540}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import keras\n",
    "    from keras.datasets import mnist\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "    from keras.layers import Conv1D, MaxPool1D\n",
    "    from keras import backend as K\n",
    "except:\n",
    "    raise ImportError(\"For this example you need to install keras.\")\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "except:\n",
    "    raise ImportError(\"For this example you need to install pytorch-vision.\")\n",
    "\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from tensorflow.keras.callbacks import  EarlyStopping\n",
    "import random\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "    \n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "# 기존 train 데이터 불러오기\n",
    "# [11, 505000, 2]\n",
    "train_sub_data = scipy.io.loadmat('../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "\n",
    "# 기존 test 데이터 불러오기\n",
    "# [11, 101402, 2]\n",
    "test_sub_data = scipy.io.loadmat('../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# 데이터 크기를 480로 맞춰줌\n",
    "# 5554560/480 = 115,72\n",
    "data_size = 480\n",
    "train_sub_size = 1052 # 1명당 3초 데이터 1052개\n",
    "test_sub_size = 211\n",
    "\n",
    "# # 데이터를 480(= 160*3) 크기로 사용할 수 있도록 그 배수로 전체 데이터 잘라줌\n",
    "data_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 트레인 sub 수\n",
    "train_sub_cnt = train_sub_data.shape[0]\n",
    "# test 데이터 sub 수\n",
    "test_sub_cnt = test_sub_data.shape[0]\n",
    "\n",
    "# sub_cnt = train_sub_cnt + public_sub_cnt\n",
    "\n",
    "train_sub_cut = train_sub_data[:, 0:data_cut_size, :]\n",
    "test_sub_cut = test_sub_data[:, 0:test_cut_size, :]\n",
    "\n",
    "print(train_sub_cut.shape)\n",
    "print(test_sub_cut.shape)\n",
    "\n",
    "# 데이터를 스케일링 하기 위해 2D로 reshape\n",
    "train_sub_2D = train_sub_cut.reshape(-1, 1)\n",
    "test_2D = test_sub_cut.reshape(-1, 1)\n",
    "\n",
    "# 데이터를 StandardScaler로 스케일링\n",
    "# 데이터들 모두 같은 방식으로 스케일링함\n",
    "SDscaler = StandardScaler()\n",
    "SDscaler.fit(train_sub_2D)\n",
    "train_scaled = SDscaler.transform(train_sub_2D)\n",
    "test_scaled = SDscaler.transform(test_2D)\n",
    "\n",
    "train_data = train_scaled.reshape(train_sub_cnt * train_sub_size, data_size, 2)\n",
    "test_data = test_scaled.reshape(test_sub_cnt * test_sub_size, data_size, 2)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "#test data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(test_sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_sub_size:(i+1)*test_sub_size, :, :])\n",
    "# print(test_data_each)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(train_sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_sub_size:(i+1)*train_sub_size, :, :])\n",
    "    \n",
    "# sub index\n",
    "i = 5\n",
    "# 1 to ratio\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[i]\n",
    "test_data_n = test_data_each[i]\n",
    "\n",
    "cnt = 0\n",
    "for j in range(train_sub_cnt):\n",
    "    if j != i and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != i and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "\n",
    "cnt = 0\n",
    "for j in range(test_sub_cnt):\n",
    "    if j != i and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != i and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "\n",
    "train_label = np.zeros(train_sub_size*(ratio+1))\n",
    "test_label = np.zeros(test_sub_size*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_sub_size):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_sub_size):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "# train 데이터를 train과 validation으로 분배\n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "val_data_set = train_data_shuffled[:train_sub_size]\n",
    "train_data_set = train_data_shuffled[train_sub_size:]\n",
    "\n",
    "val_label_set = train_label_shuffled[:train_sub_size]\n",
    "train_label_set = train_label_shuffled[train_sub_size:]\n",
    "\n",
    "import ConfigSpace as CS\n",
    "import ConfigSpace.hyperparameters as CSH\n",
    "\n",
    "from hpbandster.core.worker import Worker\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "# 작업자 클래스\n",
    "class KerasWorker(Worker):\n",
    "    def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            self.batch_size = 64\n",
    "\n",
    "            img_rows = 480\n",
    "            img_cols = 2\n",
    "            self.num_classes = 1\n",
    "            \n",
    "            x_train, y_train = train_data_set, train_label_set\n",
    "            x_validation, y_validation = val_data_set, val_label_set\n",
    "            x_test, y_test   = test_data_n, test_label\n",
    "\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols)\n",
    "                    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols)\n",
    "                    self.input_shape = (img_rows, img_cols)\n",
    "            else:\n",
    "                    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols)\n",
    "                    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols)\n",
    "                    self.input_shape = (img_rows, img_cols)\n",
    "\n",
    "            self.x_train, self.y_train = x_train, y_train\n",
    "            self.x_validation, self.y_validation = x_validation, y_validation\n",
    "            self.x_test, self.y_test   = x_test, y_test\n",
    "            \n",
    "            self.input_shape = (img_rows, img_cols)\n",
    "\n",
    "\n",
    "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
    "            \"\"\"\n",
    "            Simple example for a compute function using a feed forward network.\n",
    "            It is trained on the MNIST dataset.\n",
    "            The input parameter \"config\" (dictionary) contains the sampled configurations passed by the bohb optimizer\n",
    "            \"\"\"\n",
    "\n",
    "            model = Sequential()\n",
    "            \n",
    "            # Cnn 층\n",
    "            model.add(Conv1D(config['num_filters_1'], kernel_size=3, strides = 1, padding = 'same',\n",
    "                            activation='relu', input_shape=self.input_shape))\n",
    "            \n",
    "            model.add(MaxPool1D(pool_size=3))\n",
    "            \n",
    "            if config['num_conv_layers'] > 1:\n",
    "                model.add(Conv1D(config['num_filters_2'], kernel_size=3, strides = 1, padding = 'same',\n",
    "                            activation='relu', input_shape=self.input_shape))\n",
    "                model.add(MaxPool1D(pool_size=3))\n",
    "                \n",
    "            if config['num_conv_layers'] > 2:\n",
    "                model.add(Conv1D(config['num_filters_3'], kernel_size=3, strides = 1, padding = 'same',\n",
    "                            activation='relu', input_shape=self.input_shape))\n",
    "                model.add(MaxPool1D(pool_size=3))\n",
    "                \n",
    "            \n",
    "            if config['num_conv_layers'] > 3:\n",
    "                model.add(Conv1D(config['num_filters_4'], kernel_size=3, strides = 1, padding = 'same',\n",
    "                            activation='relu', input_shape=self.input_shape))\n",
    "                model.add(MaxPool1D(pool_size=3))\n",
    "            \n",
    "                \n",
    "            if config['num_conv_layers'] > 4:\n",
    "                model.add(Conv1D(config['num_filters_5'], kernel_size=3, strides = 1, padding = 'same',\n",
    "                            activation='relu', input_shape=self.input_shape))\n",
    "                model.add(MaxPool1D(pool_size=3))\n",
    "\n",
    "            model.add(Dropout(config['dropout_rate']))\n",
    "            \n",
    "            # LSTM 층\n",
    "            model.add(LSTM(config['lstm_units']))\n",
    "            \n",
    "            # Dense 층\n",
    "            model.add(Dense(config['num_fc_units_1'], activation='relu'))\n",
    "            \n",
    "            if config['num_fc_layers'] > 1:\n",
    "                model.add(Dense(config['num_fc_units_2'], activation='relu'))\n",
    "            \n",
    "            if config['num_fc_layers'] > 2:\n",
    "                model.add(Dense(config['num_fc_units_3'], activation='relu'))\n",
    "            \n",
    "            model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "            \n",
    "            if config['optimizer'] == 'Adam':\n",
    "                    optimizer = keras.optimizers.Adam(lr=config['lr'])\n",
    "            else:\n",
    "                    optimizer = keras.optimizers.SGD(lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                                      optimizer=optimizer,\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "            if config['optimizer'] == 'Adam':\n",
    "                    optimizer = keras.optimizers.Adam(lr=config['lr'])\n",
    "            else:\n",
    "                    optimizer = keras.optimizers.SGD(lr=config['lr'], momentum=config['sgd_momentum'])\n",
    "\n",
    "            model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                                      optimizer=optimizer,\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "            model.fit(self.x_train, self.y_train,\n",
    "                              batch_size=self.batch_size,\n",
    "                              epochs=int(budget),\n",
    "                              verbose=0,\n",
    "                              validation_data=(self.x_validation, self.y_validation)\n",
    "                     )\n",
    "\n",
    "            train_score = model.evaluate(self.x_train, self.y_train, verbose=0)\n",
    "            val_score = model.evaluate(self.x_validation, self.y_validation, verbose=0)\n",
    "            test_score = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "\n",
    "            #import IPython; IPython.embed()\n",
    "            return ({\n",
    "                    'loss': 1-val_score[1],\n",
    "                    'info': {       'test accuracy': test_score[1],\n",
    "                                            'train accuracy': train_score[1],\n",
    "                                            'validation accuracy': val_score[1],\n",
    "                                            'number of parameters': model.count_params(),\n",
    "                                    }\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_configspace():\n",
    "            \"\"\"\n",
    "            It builds the configuration space with the needed hyperparameters.\n",
    "            It is easily possible to implement different types of hyperparameters.\n",
    "            Beside float-hyperparameters on a log scale, it is also able to handle categorical input parameter.\n",
    "            :return: ConfigurationsSpace-Object\n",
    "            \"\"\"\n",
    "            cs = CS.ConfigurationSpace()\n",
    "\n",
    "            lr = CSH.UniformFloatHyperparameter('lr', lower=1e-6, upper=1e-1, default_value='1e-2', log=True)\n",
    "\n",
    "            # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
    "            # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
    "            # SGD has a different parameter 'momentum'.\n",
    "            optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "            sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
    "\n",
    "            cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
    "\n",
    "\n",
    "\n",
    "            num_conv_layers =  CSH.UniformIntegerHyperparameter('num_conv_layers', lower=1, upper=5, default_value=3)\n",
    "\n",
    "            num_filters_1 = CSH.UniformIntegerHyperparameter('num_filters_1', lower=4, upper=256, default_value=16, log=True)\n",
    "            num_filters_2 = CSH.UniformIntegerHyperparameter('num_filters_2', lower=4, upper=256, default_value=32, log=True)\n",
    "            num_filters_3 = CSH.UniformIntegerHyperparameter('num_filters_3', lower=4, upper=256, default_value=64, log=True)\n",
    "            num_filters_4 = CSH.UniformIntegerHyperparameter('num_filters_4', lower=4, upper=256, default_value=128, log=True)\n",
    "            num_filters_5 = CSH.UniformIntegerHyperparameter('num_filters_5', lower=4, upper=256, default_value=256, log=True)\n",
    "\n",
    "            cs.add_hyperparameters([num_conv_layers, num_filters_1, num_filters_2, num_filters_3, num_filters_4, num_filters_5])\n",
    "\n",
    "            dropout_rate = CSH.UniformFloatHyperparameter('dropout_rate', lower=0.0, upper=0.9, default_value=0.5, log=False)\n",
    "            \n",
    "            lstm_units = CSH.UniformIntegerHyperparameter('lstm_units', lower=8, upper=256, default_value=64, log=False)\n",
    "            \n",
    "            num_fc_layers =  CSH.UniformIntegerHyperparameter('num_fc_layers', lower=1, upper=3, default_value=2) \n",
    "            \n",
    "            num_fc_units_1 = CSH.UniformIntegerHyperparameter('num_fc_units_1', lower=8, upper=256, default_value=128, log=True)\n",
    "            num_fc_units_2 = CSH.UniformIntegerHyperparameter('num_fc_units_2', lower=8, upper=256, default_value=64, log=True)\n",
    "            num_fc_units_3 = CSH.UniformIntegerHyperparameter('num_fc_units_3', lower=8, upper=256, default_value=32, log=True)\n",
    "\n",
    "            cs.add_hyperparameters([dropout_rate, lstm_units, num_fc_layers, num_fc_units_1, num_fc_units_2, num_fc_units_3])\n",
    "\n",
    "\n",
    "            # The hyperparameter sgd_momentum will be used,if the configuration\n",
    "            # contains 'SGD' as optimizer.\n",
    "            cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            # You can also use inequality conditions:\n",
    "            cond = CS.GreaterThanCondition(num_filters_2, num_conv_layers, 1)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(num_filters_3, num_conv_layers, 2)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(num_filters_4, num_conv_layers, 3)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(num_filters_5, num_conv_layers, 4)\n",
    "            cs.add_condition(cond)\n",
    "            \n",
    "            cond = CS.GreaterThanCondition(num_fc_units_2, num_fc_layers, 1)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            cond = CS.GreaterThanCondition(num_fc_units_3, num_fc_layers, 2)\n",
    "            cs.add_condition(cond)\n",
    "\n",
    "            return cs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 작업자 시작\n",
    "    worker = KerasWorker(run_id='0')\n",
    "    \n",
    "    # 옵티마이저 실행\n",
    "    cs = worker.get_configspace()\n",
    "\n",
    "    # 결과 분석\n",
    "    config = cs.sample_configuration().get_dictionary()\n",
    "    print(config)\n",
    "    res = worker.compute(config=config, budget=2, working_directory='../BOHB_LAB_5CNN')\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
