{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excessive-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "inner-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "    \n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "attempted-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 메소드 train_step, test_step, compile() 오버라이딩\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \n",
    "        1) 옵티마이저 : Student 가중치를 위한 Keras 옵티마이저\n",
    "        2) 메트릭 : 평가를 위한 Keras 메트릭\n",
    "        3) student_loss_fn : Student Model의 예측값과 실제값 차이에 대한 손실 함수\n",
    "        4) distillation_loss_fn : Soft Student Model 의 예측과 Soft Teacher Model의 예측 차이에 대한 손실 함수\n",
    "        5) alpha : student_loss_fn과 1-alpha to distillation_loss_fn에 대한 가중치\n",
    "        6) temperature : 확률 분포를 softening 하기 위한 Temperature\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        \n",
    "        # Forward pass of teacher(교사의 Forward Pass)\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student (학생의 Forward Pass)\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            \n",
    "            # Compute losses (Losses 계산)\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "            \n",
    "        # Compute gradients (Gradients 계산)\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights (가중치 업데이트)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update the metrics configured in 'compile()'. (컴파일 안에서 메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        \n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "        \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data(데이터 언패킹)\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions(예측 수행)\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss(loss 계산)\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.(메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "competitive-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))\n",
    "\n",
    "# sub number\n",
    "sub_num = 7\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "# val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "# train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "# val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "# train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (train_data_shuffled, train_label_shuffled), (test_data_n, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "velvet-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher # 교사 모델 생성\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 188, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 117, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 183, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.5254390056631926),\n",
    "        layers.LSTM(166),\n",
    "        layers.Dense(11, activation = 'relu'),\n",
    "        layers.Dense(223, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student # 학생 모델 생성\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 90, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 55, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 90, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.LSTM(166),\n",
    "        layers.Dense(5, activation = 'relu'),\n",
    "        layers.Dense(100, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison # 후행 비교를 위한 학생 복제\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "renewable-consortium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_29 (Conv1D)           (None, 480, 188)          1316      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 160, 188)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 160, 117)          66105     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 53, 117)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 53, 183)           64416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 17, 183)           0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 17, 183)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 166)               232400    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 11)                1837      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 223)               2676      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 224       \n",
      "=================================================================\n",
      "Total params: 368,974\n",
      "Trainable params: 368,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.4927 - binary_accuracy: 0.7640\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.4083 - binary_accuracy: 0.8018\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.3815 - binary_accuracy: 0.8265\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.3228 - binary_accuracy: 0.8693\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.2791 - binary_accuracy: 0.8890\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 6s 27ms/step - loss: 0.2628 - binary_accuracy: 0.8964\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.2507 - binary_accuracy: 0.9038\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.2403 - binary_accuracy: 0.9085\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.2395 - binary_accuracy: 0.9066\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 6s 27ms/step - loss: 0.2251 - binary_accuracy: 0.9080\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.2214 - binary_accuracy: 0.9159\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.2134 - binary_accuracy: 0.9166\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.2118 - binary_accuracy: 0.9166\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.1970 - binary_accuracy: 0.9213\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.1925 - binary_accuracy: 0.9230\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.2033 - binary_accuracy: 0.9161\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1904 - binary_accuracy: 0.9240\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1738 - binary_accuracy: 0.9311\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1708 - binary_accuracy: 0.9320\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1716 - binary_accuracy: 0.9318\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1523 - binary_accuracy: 0.9356\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1601 - binary_accuracy: 0.9380\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.1512 - binary_accuracy: 0.9387\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1499 - binary_accuracy: 0.9370\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1431 - binary_accuracy: 0.9415\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1290 - binary_accuracy: 0.9503\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.1560 - binary_accuracy: 0.9327\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1210 - binary_accuracy: 0.9499\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1233 - binary_accuracy: 0.9489\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.1183 - binary_accuracy: 0.9508\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1142 - binary_accuracy: 0.9503\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1136 - binary_accuracy: 0.9508\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.1100 - binary_accuracy: 0.9584\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.1031 - binary_accuracy: 0.9577\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0944 - binary_accuracy: 0.9655\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0936 - binary_accuracy: 0.9655\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0865 - binary_accuracy: 0.9677\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0833 - binary_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0832 - binary_accuracy: 0.9663\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0910 - binary_accuracy: 0.9632\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0683 - binary_accuracy: 0.9729\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0832 - binary_accuracy: 0.9693\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0776 - binary_accuracy: 0.9679\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 7s 32ms/step - loss: 0.0606 - binary_accuracy: 0.9767\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0597 - binary_accuracy: 0.9772\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0559 - binary_accuracy: 0.9793\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0630 - binary_accuracy: 0.9760\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0477 - binary_accuracy: 0.9819\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0449 - binary_accuracy: 0.9838\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0502 - binary_accuracy: 0.9822\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0422 - binary_accuracy: 0.9827\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0412 - binary_accuracy: 0.9834\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0398 - binary_accuracy: 0.9848\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 7s 31ms/step - loss: 0.0570 - binary_accuracy: 0.9788\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 0.0362 - binary_accuracy: 0.9876\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 0.0424 - binary_accuracy: 0.9834\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0305 - binary_accuracy: 0.9891\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0378 - binary_accuracy: 0.9867\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0230 - binary_accuracy: 0.9910\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0435 - binary_accuracy: 0.9860\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 0.0257 - binary_accuracy: 0.9900\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0307 - binary_accuracy: 0.9888\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0407 - binary_accuracy: 0.9848\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 0.0431 - binary_accuracy: 0.9836\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0277 - binary_accuracy: 0.9881\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0229 - binary_accuracy: 0.9922\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0267 - binary_accuracy: 0.9898\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0205 - binary_accuracy: 0.9919\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0308 - binary_accuracy: 0.9884\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0242 - binary_accuracy: 0.9914\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0241 - binary_accuracy: 0.9895\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0233 - binary_accuracy: 0.9919\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0135 - binary_accuracy: 0.9952\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0389 - binary_accuracy: 0.9860\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0378 - binary_accuracy: 0.9857\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0146 - binary_accuracy: 0.9957\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0433 - binary_accuracy: 0.9862\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0169 - binary_accuracy: 0.9945\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 0.0159 - binary_accuracy: 0.9952\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0208 - binary_accuracy: 0.9929\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0169 - binary_accuracy: 0.9938\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0141 - binary_accuracy: 0.9967\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0086 - binary_accuracy: 0.9967\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0459 - binary_accuracy: 0.9857\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0141 - binary_accuracy: 0.9950\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0192 - binary_accuracy: 0.9941\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0159 - binary_accuracy: 0.9945\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0249 - binary_accuracy: 0.9917\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.0197 - binary_accuracy: 0.9936\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0175 - binary_accuracy: 0.9952\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 0.0233 - binary_accuracy: 0.9910\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.0189 - binary_accuracy: 0.9938\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.0207 - binary_accuracy: 0.9922\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 0.0162 - binary_accuracy: 0.9938\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 6s 27ms/step - loss: 0.0168 - binary_accuracy: 0.9955\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 0.0215 - binary_accuracy: 0.9922\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.0189 - binary_accuracy: 0.9929\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 0.0126 - binary_accuracy: 0.9950\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 0.0158 - binary_accuracy: 0.9957\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 0.0214 - binary_accuracy: 0.9914\n",
      "27/27 - 0s - loss: 0.7421 - binary_accuracy: 0.8614\n",
      "[[540  93]\n",
      " [ 24 187]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "teacher_accuracy\n",
      "0.8613743782043457\n",
      "teacher_specificity\n",
      "0.8530805687203792\n",
      "teacher_sensitivity\n",
      "0.8862559241706162\n",
      "teacher_precision\n",
      "0.6678571428571428\n",
      "teacher_recall\n",
      "0.8862559241706162\n",
      "teacher_frr\n",
      "0.11374407582938388\n",
      "teacher_far\n",
      "0.14691943127962084\n"
     ]
    }
   ],
   "source": [
    "# Train teacher as usual # 평소와 같은 방법으로 교사모델 훈련시작\n",
    "teacher.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00021455459389909653),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "teacher.summary()\n",
    "\n",
    "# Train and evaluate teacher on data. # 데이터셋을 통해 교사의 트레인 및 평가\n",
    "hist = teacher.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "teacher.save('Teacher_Model_8.h5')\n",
    "\n",
    "teacher_loss, teacher_acc = teacher.evaluate(x_test, y_test, verbose = 2)\n",
    "teacher_pred = teacher.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(teacher_pred)):\n",
    "    if(0.5 <= teacher_pred[i]):\n",
    "        teacher_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        teacher_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, teacher_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "teacher_sen = conf_matrix[1][1] / row[1]\n",
    "teacher_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "teacher_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "teacher_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "teacher_prec = conf_matrix[1][1] / conf_col[1]\n",
    "teacher_recall = conf_matrix[1][1] / conf_row[1]\n",
    "    \n",
    "print('teacher_accuracy')\n",
    "print(teacher_acc)\n",
    "print('teacher_specificity')\n",
    "print(teacher_spe)\n",
    "print('teacher_sensitivity')\n",
    "print(teacher_sen)\n",
    "print('teacher_precision')\n",
    "print(teacher_prec)\n",
    "print('teacher_recall')\n",
    "print(teacher_recall)\n",
    "print('teacher_frr')\n",
    "print(teacher_frr)\n",
    "print('teacher_far')\n",
    "print(teacher_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accurate-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.7533 - student_loss: 0.4767 - distillation_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.8208 - student_loss: 0.3963 - distillation_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.8303 - student_loss: 0.3851 - distillation_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.8598 - student_loss: 0.3329 - distillation_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.8923 - student_loss: 0.2769 - distillation_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.8959 - student_loss: 0.2603 - distillation_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9092 - student_loss: 0.2477 - distillation_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9137 - student_loss: 0.2267 - distillation_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9183 - student_loss: 0.2120 - distillation_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9163 - student_loss: 0.2153 - distillation_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9240 - student_loss: 0.1968 - distillation_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9261 - student_loss: 0.1875 - distillation_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9228 - student_loss: 0.1923 - distillation_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9240 - student_loss: 0.1812 - distillation_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9325 - student_loss: 0.1742 - distillation_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9330 - student_loss: 0.1614 - distillation_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9342 - student_loss: 0.1574 - distillation_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9373 - student_loss: 0.1506 - distillation_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9404 - student_loss: 0.1504 - distillation_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9465 - student_loss: 0.1349 - distillation_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9529 - student_loss: 0.1168 - distillation_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9510 - student_loss: 0.1208 - distillation_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9518 - student_loss: 0.1194 - distillation_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9575 - student_loss: 0.1121 - distillation_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9475 - student_loss: 0.1282 - distillation_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9629 - student_loss: 0.1026 - distillation_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9603 - student_loss: 0.1005 - distillation_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9658 - student_loss: 0.0873 - distillation_loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9615 - student_loss: 0.0995 - distillation_loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9689 - student_loss: 0.0783 - distillation_loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9734 - student_loss: 0.0702 - distillation_loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9715 - student_loss: 0.0789 - distillation_loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9753 - student_loss: 0.0677 - distillation_loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9734 - student_loss: 0.0714 - distillation_loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9696 - student_loss: 0.0799 - distillation_loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9755 - student_loss: 0.0640 - distillation_loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9774 - student_loss: 0.0639 - distillation_loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9727 - student_loss: 0.0723 - distillation_loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9741 - student_loss: 0.0594 - distillation_loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9786 - student_loss: 0.0574 - distillation_loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9667 - student_loss: 0.0872 - distillation_loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9846 - student_loss: 0.0424 - distillation_loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9827 - student_loss: 0.0487 - distillation_loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9803 - student_loss: 0.0537 - distillation_loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9791 - student_loss: 0.0552 - distillation_loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9812 - student_loss: 0.0587 - distillation_loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9865 - student_loss: 0.0351 - distillation_loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9827 - student_loss: 0.0475 - distillation_loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9831 - student_loss: 0.0457 - distillation_loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9874 - student_loss: 0.0361 - distillation_loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9869 - student_loss: 0.0379 - distillation_loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9853 - student_loss: 0.0365 - distillation_loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9829 - student_loss: 0.0457 - distillation_loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9867 - student_loss: 0.0396 - distillation_loss: 0.0000e+00\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9886 - student_loss: 0.0328 - distillation_loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9879 - student_loss: 0.0339 - distillation_loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9884 - student_loss: 0.0360 - distillation_loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9843 - student_loss: 0.0395 - distillation_loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9914 - student_loss: 0.0278 - distillation_loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9846 - student_loss: 0.0438 - distillation_loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9857 - student_loss: 0.0403 - distillation_loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9910 - student_loss: 0.0234 - distillation_loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9834 - student_loss: 0.0441 - distillation_loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9955 - student_loss: 0.0176 - distillation_loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9886 - student_loss: 0.0296 - distillation_loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9872 - student_loss: 0.0324 - distillation_loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9891 - student_loss: 0.0297 - distillation_loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9888 - student_loss: 0.0313 - distillation_loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9910 - student_loss: 0.0242 - distillation_loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9922 - student_loss: 0.0236 - distillation_loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9938 - student_loss: 0.0161 - distillation_loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9879 - student_loss: 0.0341 - distillation_loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9924 - student_loss: 0.0258 - distillation_loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9876 - student_loss: 0.0325 - distillation_loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9952 - student_loss: 0.0145 - distillation_loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9919 - student_loss: 0.0197 - distillation_loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9891 - student_loss: 0.0323 - distillation_loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9912 - student_loss: 0.0243 - distillation_loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9919 - student_loss: 0.0251 - distillation_loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9850 - student_loss: 0.0464 - distillation_loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9957 - student_loss: 0.0161 - distillation_loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9922 - student_loss: 0.0221 - distillation_loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9914 - student_loss: 0.0252 - distillation_loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9929 - student_loss: 0.0180 - distillation_loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9893 - student_loss: 0.0291 - distillation_loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9926 - student_loss: 0.0173 - distillation_loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9952 - student_loss: 0.0193 - distillation_loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9950 - student_loss: 0.0159 - distillation_loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9936 - student_loss: 0.0201 - distillation_loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9929 - student_loss: 0.0196 - distillation_loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9924 - student_loss: 0.0275 - distillation_loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9931 - student_loss: 0.0247 - distillation_loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 4s 19ms/step - binary_accuracy: 0.9926 - student_loss: 0.0198 - distillation_loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9936 - student_loss: 0.0211 - distillation_loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 4s 18ms/step - binary_accuracy: 0.9943 - student_loss: 0.0191 - distillation_loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 4s 17ms/step - binary_accuracy: 0.9922 - student_loss: 0.0210 - distillation_loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 4s 17ms/step - binary_accuracy: 0.9943 - student_loss: 0.0184 - distillation_loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 3s 17ms/step - binary_accuracy: 0.9919 - student_loss: 0.0239 - distillation_loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 4s 17ms/step - binary_accuracy: 0.9914 - student_loss: 0.0205 - distillation_loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 4s 17ms/step - binary_accuracy: 0.9938 - student_loss: 0.0211 - distillation_loss: 0.0000e+00\n",
      "27/27 - 0s - binary_accuracy: 0.8732 - student_loss: 0.6531\n",
      "[[581  52]\n",
      " [ 55 156]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "student_accuracy\n",
      "0.8732227683067322\n",
      "student_specificity\n",
      "0.9178515007898894\n",
      "student_sensitivity\n",
      "0.7393364928909952\n",
      "student_precision\n",
      "0.75\n",
      "student_recall\n",
      "0.7393364928909952\n",
      "student_frr\n",
      "0.26066350710900477\n",
      "student_far\n",
      "0.08214849921011058\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "  \n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    student_loss_fn=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "student.save('Student_Model_8.h5')\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "student_acc = distiller.evaluate(x_test, y_test, verbose = 2)\n",
    "student_pred = student.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(student_pred)):\n",
    "    if(0.5 <= student_pred[i]):\n",
    "        student_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        student_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, student_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "student_sen = conf_matrix[1][1] / row[1]\n",
    "student_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "student_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "student_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "student_prec = conf_matrix[1][1] / conf_col[1]\n",
    "student_recall = conf_matrix[1][1] / conf_row[1]\n",
    "\n",
    "print('student_accuracy')\n",
    "print(student_acc)\n",
    "print('student_specificity')\n",
    "print(student_spe)\n",
    "print('student_sensitivity')\n",
    "print(student_sen)\n",
    "print('student_precision')\n",
    "print(student_prec)\n",
    "print('student_recall')\n",
    "print(student_recall)\n",
    "print('student_frr')\n",
    "print(student_frr)\n",
    "print('student_far')\n",
    "print(student_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "headed-mission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# gzip을 통해 모델을 실제로 압축하고 압축된 크기를 측정\n",
    "# 모델의 크기(바이트)를 반환한다.\n",
    "def get_gzipped_model_size(file):\n",
    "  \n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "imperial-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved teacher model to: /tmp/tmpxz9e2f_j.h5\n",
      "Saved student model to: /tmp/tmpl57k88o9.h5\n",
      "Size of gzipped Teacher model: 1370236.00 bytes\n",
      "Size of gzipped Student model: 756953.00 bytes\n"
     ]
    }
   ],
   "source": [
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, teacher_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(teacher, teacher_file, include_optimizer=False)\n",
    "print('Saved teacher model to:', teacher_file)\n",
    "\n",
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, student_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(student, student_file, include_optimizer=False)\n",
    "print('Saved student model to:', student_file)\n",
    "\n",
    "print(\"Size of gzipped Teacher model: %.2f bytes\" % (get_gzipped_model_size(teacher_file)))\n",
    "print(\"Size of gzipped Student model: %.2f bytes\" % (get_gzipped_model_size(student_file)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
