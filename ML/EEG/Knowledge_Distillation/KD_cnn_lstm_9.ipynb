{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excessive-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "inner-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "    \n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attempted-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 메소드 train_step, test_step, compile() 오버라이딩\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \n",
    "        1) 옵티마이저 : Student 가중치를 위한 Keras 옵티마이저\n",
    "        2) 메트릭 : 평가를 위한 Keras 메트릭\n",
    "        3) student_loss_fn : Student Model의 예측값과 실제값 차이에 대한 손실 함수\n",
    "        4) distillation_loss_fn : Soft Student Model 의 예측과 Soft Teacher Model의 예측 차이에 대한 손실 함수\n",
    "        5) alpha : student_loss_fn과 1-alpha to distillation_loss_fn에 대한 가중치\n",
    "        6) temperature : 확률 분포를 softening 하기 위한 Temperature\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        \n",
    "        # Forward pass of teacher(교사의 Forward Pass)\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student (학생의 Forward Pass)\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            \n",
    "            # Compute losses (Losses 계산)\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "            \n",
    "        # Compute gradients (Gradients 계산)\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights (가중치 업데이트)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update the metrics configured in 'compile()'. (컴파일 안에서 메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        \n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "        \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data(데이터 언패킹)\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions(예측 수행)\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss(loss 계산)\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.(메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "competitive-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))\n",
    "\n",
    "# sub number\n",
    "sub_num = 8\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "# val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "# train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "# val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "# train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (train_data_shuffled, train_label_shuffled), (test_data_n, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "velvet-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher # 교사 모델 생성\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 240, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 44, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.6273313485219749),\n",
    "        layers.LSTM(156),\n",
    "        layers.Dense(253, activation = 'relu'),\n",
    "        layers.Dense(34, activation = 'relu'),\n",
    "        layers.Dense(55, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student # 학생 모델 생성\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 120, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 22, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.LSTM(156),\n",
    "        layers.Dense(125, activation = 'relu'),\n",
    "        layers.Dense(15, activation = 'relu'),\n",
    "        layers.Dense(25, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison # 후행 비교를 위한 학생 복제\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "renewable-consortium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 480, 240)          1680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 160, 240)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 160, 44)           31724     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 53, 44)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 53, 44)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 156)               125424    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 253)               39721     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 34)                8636      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 55)                1925      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 56        \n",
      "=================================================================\n",
      "Total params: 209,166\n",
      "Trainable params: 209,166\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.5379 - binary_accuracy: 0.7559\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 9s 41ms/step - loss: 0.4578 - binary_accuracy: 0.8001\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 9s 42ms/step - loss: 0.4093 - binary_accuracy: 0.8234\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.3901 - binary_accuracy: 0.8365\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.3697 - binary_accuracy: 0.8377\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.3544 - binary_accuracy: 0.8534\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 11s 51ms/step - loss: 0.3426 - binary_accuracy: 0.8548\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 11s 53ms/step - loss: 0.3307 - binary_accuracy: 0.8626\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 11s 52ms/step - loss: 0.3256 - binary_accuracy: 0.8617\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 11s 52ms/step - loss: 0.3089 - binary_accuracy: 0.8736\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 10s 46ms/step - loss: 0.3032 - binary_accuracy: 0.8705\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 10s 48ms/step - loss: 0.2927 - binary_accuracy: 0.8729\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.3010 - binary_accuracy: 0.8674\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 10s 48ms/step - loss: 0.2925 - binary_accuracy: 0.8755\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 10s 48ms/step - loss: 0.2821 - binary_accuracy: 0.8802\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2619 - binary_accuracy: 0.8876\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2609 - binary_accuracy: 0.8843\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 10s 49ms/step - loss: 0.2614 - binary_accuracy: 0.8800\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2655 - binary_accuracy: 0.8881\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 10s 48ms/step - loss: 0.2646 - binary_accuracy: 0.8847\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 10s 48ms/step - loss: 0.2561 - binary_accuracy: 0.8921\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2526 - binary_accuracy: 0.8916\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2523 - binary_accuracy: 0.8921\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2500 - binary_accuracy: 0.8952\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2411 - binary_accuracy: 0.9002\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2262 - binary_accuracy: 0.9057\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2277 - binary_accuracy: 0.9040\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 10s 47ms/step - loss: 0.2384 - binary_accuracy: 0.9042\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2407 - binary_accuracy: 0.8969\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 9s 44ms/step - loss: 0.2336 - binary_accuracy: 0.9011\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2314 - binary_accuracy: 0.9002\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2154 - binary_accuracy: 0.9083\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 9s 44ms/step - loss: 0.2235 - binary_accuracy: 0.9059\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2229 - binary_accuracy: 0.9073\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.2242 - binary_accuracy: 0.9021\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.2143 - binary_accuracy: 0.9102\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.2113 - binary_accuracy: 0.9140\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.2183 - binary_accuracy: 0.9102\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 10s 46ms/step - loss: 0.2147 - binary_accuracy: 0.9137\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 9s 44ms/step - loss: 0.2076 - binary_accuracy: 0.9128\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.2072 - binary_accuracy: 0.9161\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 10s 46ms/step - loss: 0.2104 - binary_accuracy: 0.9137\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.2089 - binary_accuracy: 0.9133\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 9s 44ms/step - loss: 0.2076 - binary_accuracy: 0.9128\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.2105 - binary_accuracy: 0.9102\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2002 - binary_accuracy: 0.9194\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 9s 44ms/step - loss: 0.2148 - binary_accuracy: 0.9073\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 9s 45ms/step - loss: 0.1941 - binary_accuracy: 0.9221\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 10s 45ms/step - loss: 0.2072 - binary_accuracy: 0.9144\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1965 - binary_accuracy: 0.9199\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1965 - binary_accuracy: 0.9199\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.2003 - binary_accuracy: 0.9171\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1964 - binary_accuracy: 0.9171\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.2006 - binary_accuracy: 0.9130\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1897 - binary_accuracy: 0.9263\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1889 - binary_accuracy: 0.9237\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 9s 43ms/step - loss: 0.1916 - binary_accuracy: 0.9192\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 9s 41ms/step - loss: 0.2025 - binary_accuracy: 0.9154\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 9s 42ms/step - loss: 0.1890 - binary_accuracy: 0.9242\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 9s 41ms/step - loss: 0.1833 - binary_accuracy: 0.9199\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 8s 40ms/step - loss: 0.1953 - binary_accuracy: 0.9209\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 8s 38ms/step - loss: 0.1897 - binary_accuracy: 0.9261\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 8s 39ms/step - loss: 0.1847 - binary_accuracy: 0.9218\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 8s 37ms/step - loss: 0.1797 - binary_accuracy: 0.9270\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 8s 39ms/step - loss: 0.1823 - binary_accuracy: 0.9273\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1792 - binary_accuracy: 0.9273\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1886 - binary_accuracy: 0.9190\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1729 - binary_accuracy: 0.9263\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1731 - binary_accuracy: 0.9261\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1760 - binary_accuracy: 0.9244\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1856 - binary_accuracy: 0.9235\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1737 - binary_accuracy: 0.9256\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1755 - binary_accuracy: 0.9263\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1728 - binary_accuracy: 0.9297\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1621 - binary_accuracy: 0.9339\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1858 - binary_accuracy: 0.9247\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 7s 35ms/step - loss: 0.1760 - binary_accuracy: 0.9282\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1593 - binary_accuracy: 0.9346\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1696 - binary_accuracy: 0.9342\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1783 - binary_accuracy: 0.9230\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1754 - binary_accuracy: 0.9230\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1702 - binary_accuracy: 0.9294\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1574 - binary_accuracy: 0.9365\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1638 - binary_accuracy: 0.9313\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1721 - binary_accuracy: 0.9266\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1584 - binary_accuracy: 0.9356\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1563 - binary_accuracy: 0.9337\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1699 - binary_accuracy: 0.9316\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1637 - binary_accuracy: 0.9285\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1591 - binary_accuracy: 0.9316\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1616 - binary_accuracy: 0.9306\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1675 - binary_accuracy: 0.9320\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1710 - binary_accuracy: 0.9280\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1554 - binary_accuracy: 0.9339\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1591 - binary_accuracy: 0.9339\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 7s 34ms/step - loss: 0.1535 - binary_accuracy: 0.9358\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1505 - binary_accuracy: 0.9385\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1577 - binary_accuracy: 0.9358\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1499 - binary_accuracy: 0.9354\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 7s 33ms/step - loss: 0.1793 - binary_accuracy: 0.9254\n",
      "27/27 - 0s - loss: 0.1467 - binary_accuracy: 0.9408\n",
      "[[607  26]\n",
      " [ 24 187]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "teacher_accuracy\n",
      "0.9407582879066467\n",
      "teacher_specificity\n",
      "0.9589257503949447\n",
      "teacher_sensitivity\n",
      "0.8862559241706162\n",
      "teacher_precision\n",
      "0.8779342723004695\n",
      "teacher_recall\n",
      "0.8862559241706162\n",
      "teacher_frr\n",
      "0.11374407582938388\n",
      "teacher_far\n",
      "0.04107424960505529\n"
     ]
    }
   ],
   "source": [
    "# Train teacher as usual # 평소와 같은 방법으로 교사모델 훈련시작\n",
    "teacher.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00024850487236764685),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "teacher.summary()\n",
    "\n",
    "# Train and evaluate teacher on data. # 데이터셋을 통해 교사의 트레인 및 평가\n",
    "hist = teacher.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "teacher.save('Teacher_Model_9.h5')\n",
    "\n",
    "teacher_loss, teacher_acc = teacher.evaluate(x_test, y_test, verbose = 2)\n",
    "teacher_pred = teacher.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(teacher_pred)):\n",
    "    if(0.5 <= teacher_pred[i]):\n",
    "        teacher_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        teacher_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, teacher_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "teacher_sen = conf_matrix[1][1] / row[1]\n",
    "teacher_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "teacher_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "teacher_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "teacher_prec = conf_matrix[1][1] / conf_col[1]\n",
    "teacher_recall = conf_matrix[1][1] / conf_row[1]\n",
    "    \n",
    "print('teacher_accuracy')\n",
    "print(teacher_acc)\n",
    "print('teacher_specificity')\n",
    "print(teacher_spe)\n",
    "print('teacher_sensitivity')\n",
    "print(teacher_sen)\n",
    "print('teacher_precision')\n",
    "print(teacher_prec)\n",
    "print('teacher_recall')\n",
    "print(teacher_recall)\n",
    "print('teacher_frr')\n",
    "print(teacher_frr)\n",
    "print('teacher_far')\n",
    "print(teacher_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accurate-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.7410 - student_loss: 0.5577 - distillation_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.7769 - student_loss: 0.4829 - distillation_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.7742 - student_loss: 0.4740 - distillation_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8023 - student_loss: 0.4435 - distillation_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8151 - student_loss: 0.4132 - distillation_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8298 - student_loss: 0.3944 - distillation_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8386 - student_loss: 0.3775 - distillation_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8436 - student_loss: 0.3754 - distillation_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8460 - student_loss: 0.3642 - distillation_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8465 - student_loss: 0.3621 - distillation_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8472 - student_loss: 0.3521 - distillation_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8493 - student_loss: 0.3459 - distillation_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8460 - student_loss: 0.3509 - distillation_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8489 - student_loss: 0.3435 - distillation_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 7s 33ms/step - binary_accuracy: 0.8531 - student_loss: 0.3280 - distillation_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8693 - student_loss: 0.3098 - distillation_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 7s 32ms/step - binary_accuracy: 0.8688 - student_loss: 0.3089 - distillation_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 7s 31ms/step - binary_accuracy: 0.8679 - student_loss: 0.3059 - distillation_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 7s 31ms/step - binary_accuracy: 0.8698 - student_loss: 0.2940 - distillation_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 6s 31ms/step - binary_accuracy: 0.8700 - student_loss: 0.2948 - distillation_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8721 - student_loss: 0.2910 - distillation_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8802 - student_loss: 0.2711 - distillation_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8800 - student_loss: 0.2740 - distillation_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8833 - student_loss: 0.2625 - distillation_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8762 - student_loss: 0.2700 - distillation_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8710 - student_loss: 0.3026 - distillation_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8151 - student_loss: 0.4248 - distillation_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.7788 - student_loss: 0.4592 - distillation_loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8453 - student_loss: 0.3632 - distillation_loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8462 - student_loss: 0.3386 - distillation_loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8686 - student_loss: 0.3237 - distillation_loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8757 - student_loss: 0.2908 - distillation_loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8686 - student_loss: 0.3021 - distillation_loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8788 - student_loss: 0.2815 - distillation_loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8826 - student_loss: 0.2736 - distillation_loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8721 - student_loss: 0.2714 - distillation_loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8843 - student_loss: 0.2784 - distillation_loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8831 - student_loss: 0.2624 - distillation_loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8909 - student_loss: 0.2585 - distillation_loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8881 - student_loss: 0.2512 - distillation_loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8881 - student_loss: 0.2634 - distillation_loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.8983 - student_loss: 0.2428 - distillation_loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8959 - student_loss: 0.2463 - distillation_loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8952 - student_loss: 0.2454 - distillation_loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9021 - student_loss: 0.2360 - distillation_loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8962 - student_loss: 0.2369 - distillation_loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8985 - student_loss: 0.2349 - distillation_loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.8942 - student_loss: 0.2400 - distillation_loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9061 - student_loss: 0.2296 - distillation_loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9033 - student_loss: 0.2246 - distillation_loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9030 - student_loss: 0.2244 - distillation_loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9002 - student_loss: 0.2353 - distillation_loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9092 - student_loss: 0.2178 - distillation_loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8935 - student_loss: 0.2474 - distillation_loss: 0.0000e+00\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.8959 - student_loss: 0.2602 - distillation_loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9011 - student_loss: 0.2269 - distillation_loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9052 - student_loss: 0.2274 - distillation_loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9054 - student_loss: 0.2305 - distillation_loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9109 - student_loss: 0.2118 - distillation_loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9097 - student_loss: 0.2110 - distillation_loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 6s 29ms/step - binary_accuracy: 0.9052 - student_loss: 0.2209 - distillation_loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9049 - student_loss: 0.2175 - distillation_loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9104 - student_loss: 0.2227 - distillation_loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9135 - student_loss: 0.2107 - distillation_loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9173 - student_loss: 0.2075 - distillation_loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9121 - student_loss: 0.2058 - distillation_loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9147 - student_loss: 0.2074 - distillation_loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9102 - student_loss: 0.2074 - distillation_loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9135 - student_loss: 0.2024 - distillation_loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9152 - student_loss: 0.2078 - distillation_loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9142 - student_loss: 0.2002 - distillation_loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9137 - student_loss: 0.1993 - distillation_loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9137 - student_loss: 0.2053 - distillation_loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9111 - student_loss: 0.2127 - distillation_loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9240 - student_loss: 0.1907 - distillation_loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9221 - student_loss: 0.1861 - distillation_loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9209 - student_loss: 0.1897 - distillation_loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9237 - student_loss: 0.1810 - distillation_loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9206 - student_loss: 0.1874 - distillation_loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9261 - student_loss: 0.1833 - distillation_loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9292 - student_loss: 0.1821 - distillation_loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9185 - student_loss: 0.1913 - distillation_loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9161 - student_loss: 0.1938 - distillation_loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9228 - student_loss: 0.1769 - distillation_loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9254 - student_loss: 0.1824 - distillation_loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9237 - student_loss: 0.1774 - distillation_loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9275 - student_loss: 0.1781 - distillation_loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9244 - student_loss: 0.1732 - distillation_loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9263 - student_loss: 0.1752 - distillation_loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9280 - student_loss: 0.1764 - distillation_loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9292 - student_loss: 0.1642 - distillation_loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9304 - student_loss: 0.1616 - distillation_loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9285 - student_loss: 0.1628 - distillation_loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9228 - student_loss: 0.1755 - distillation_loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9282 - student_loss: 0.1705 - distillation_loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9332 - student_loss: 0.1593 - distillation_loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9327 - student_loss: 0.1692 - distillation_loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9337 - student_loss: 0.1634 - distillation_loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9308 - student_loss: 0.1608 - distillation_loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 6s 28ms/step - binary_accuracy: 0.9349 - student_loss: 0.1588 - distillation_loss: 0.0000e+00\n",
      "27/27 - 0s - binary_accuracy: 0.9396 - student_loss: 0.1785\n",
      "[[616  17]\n",
      " [ 34 177]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "student_accuracy\n",
      "0.9395734667778015\n",
      "student_specificity\n",
      "0.9731437598736177\n",
      "student_sensitivity\n",
      "0.8388625592417062\n",
      "student_precision\n",
      "0.9123711340206185\n",
      "student_recall\n",
      "0.8388625592417062\n",
      "student_frr\n",
      "0.16113744075829384\n",
      "student_far\n",
      "0.026856240126382307\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "  \n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    student_loss_fn=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "student.save('Student_Model_9.h5')\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "student_acc = distiller.evaluate(x_test, y_test, verbose = 2)\n",
    "student_pred = student.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(student_pred)):\n",
    "    if(0.5 <= student_pred[i]):\n",
    "        student_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        student_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, student_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "student_sen = conf_matrix[1][1] / row[1]\n",
    "student_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "student_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "student_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "student_prec = conf_matrix[1][1] / conf_col[1]\n",
    "student_recall = conf_matrix[1][1] / conf_row[1]\n",
    "\n",
    "print('student_accuracy')\n",
    "print(student_acc)\n",
    "print('student_specificity')\n",
    "print(student_spe)\n",
    "print('student_sensitivity')\n",
    "print(student_sen)\n",
    "print('student_precision')\n",
    "print(student_prec)\n",
    "print('student_recall')\n",
    "print(student_recall)\n",
    "print('student_frr')\n",
    "print(student_frr)\n",
    "print('student_far')\n",
    "print(student_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "headed-mission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# gzip을 통해 모델을 실제로 압축하고 압축된 크기를 측정\n",
    "# 모델의 크기(바이트)를 반환한다.\n",
    "def get_gzipped_model_size(file):\n",
    "  \n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "useful-happiness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved teacher model to: /tmp/tmpne_wez9u.h5\n",
      "Saved student model to: /tmp/tmpy51qc7hq.h5\n",
      "Size of gzipped Teacher model: 779839.00 bytes\n",
      "Size of gzipped Student model: 533434.00 bytes\n"
     ]
    }
   ],
   "source": [
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, teacher_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(teacher, teacher_file, include_optimizer=False)\n",
    "print('Saved teacher model to:', teacher_file)\n",
    "\n",
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, student_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(student, student_file, include_optimizer=False)\n",
    "print('Saved student model to:', student_file)\n",
    "\n",
    "print(\"Size of gzipped Teacher model: %.2f bytes\" % (get_gzipped_model_size(teacher_file)))\n",
    "print(\"Size of gzipped Student model: %.2f bytes\" % (get_gzipped_model_size(student_file)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
