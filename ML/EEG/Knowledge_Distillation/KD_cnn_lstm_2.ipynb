{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "excessive-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from numba import cuda\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "inner-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(777)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    \n",
    "  try:\n",
    "      \n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "      \n",
    "  except RuntimeError as e:\n",
    "      \n",
    "    print(e)\n",
    "    \n",
    "class MyModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyModelCheckpoint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    # redefine the save so it only activates after 100 epochs\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch >= 1: super(MyModelCheckpoint, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "attempted-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 메소드 train_step, test_step, compile() 오버라이딩\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        \n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \n",
    "        1) 옵티마이저 : Student 가중치를 위한 Keras 옵티마이저\n",
    "        2) 메트릭 : 평가를 위한 Keras 메트릭\n",
    "        3) student_loss_fn : Student Model의 예측값과 실제값 차이에 대한 손실 함수\n",
    "        4) distillation_loss_fn : Soft Student Model 의 예측과 Soft Teacher Model의 예측 차이에 대한 손실 함수\n",
    "        5) alpha : student_loss_fn과 1-alpha to distillation_loss_fn에 대한 가중치\n",
    "        6) temperature : 확률 분포를 softening 하기 위한 Temperature\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "        \n",
    "        # Forward pass of teacher(교사의 Forward Pass)\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student (학생의 Forward Pass)\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            \n",
    "            # Compute losses (Losses 계산)\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "            \n",
    "        # Compute gradients (Gradients 계산)\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        \n",
    "        # Update weights (가중치 업데이트)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update the metrics configured in 'compile()'. (컴파일 안에서 메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        \n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "        \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data(데이터 언패킹)\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions(예측 수행)\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss(loss 계산)\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.(메트릭 업데이트)\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance(퍼포먼스 dictionary 리턴)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "competitive-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1052, 480, 2)\n",
      "(11, 211, 480, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = scipy.io.loadmat('../../datas/160hz/train_5day_160hz.mat', squeeze_me=True)['data']\n",
    "test_data = scipy.io.loadmat('../../datas/160hz/test_6day_160hz(2).mat', squeeze_me=True)['data']\n",
    "\n",
    "# sub 수\n",
    "sub_cnt = train_data.shape[0]\n",
    "\n",
    "# 3sec 데이터 크기\n",
    "data_size = 480\n",
    "\n",
    "# 1명당 3초 데이터 개수\n",
    "train_data_cnt = 1052\n",
    "test_data_cnt = 211\n",
    "\n",
    "# 3sec 480(= 160*3) 크기로 데이터 길이 설정\n",
    "train_cut_size = 504960 # 480*1052 = 504960\n",
    "test_cut_size = 101280 # 480*211 = 101280\n",
    "\n",
    "# 3sec 데이터 길이 자르기\n",
    "# train: 504,960 / test: 101,280\n",
    "train_data = train_data[:,0:train_cut_size,:]\n",
    "test_data = test_data[:,0:test_cut_size,:]\n",
    "\n",
    "# flatten(): 3D -> 1D / reshape(-1,1): -1 마지막 인덱스\n",
    "train_flatten = train_data.flatten().reshape(-1,1)\n",
    "test_flatten = test_data.flatten().reshape(-1,1)\n",
    "\n",
    "# StandardScaler(): train에 맞춰 표준화\n",
    "data_scaler = StandardScaler()\n",
    "    \n",
    "data_scaler.fit(train_flatten)\n",
    "train_scaler = data_scaler.transform(train_flatten)\n",
    "test_scaler = data_scaler.transform(test_flatten)\n",
    "    \n",
    "# train, test 데이터 reshape\n",
    "train_data = train_scaler.reshape(train_data_cnt * sub_cnt, data_size, 2) \n",
    "test_data = test_scaler.reshape(test_data_cnt * sub_cnt, data_size, 2)\n",
    "\n",
    "#train data를 sub:other=1:1로 만들기 위해서 각 sub 추출\n",
    "train_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    train_data_each.insert(i, train_data[i*train_data_cnt:(i+1)*train_data_cnt, :, :])\n",
    "print(np.shape(train_data_each))\n",
    "\n",
    "#test data를 sub:other=1:3로 만들기 위해서 각 sub 추출\n",
    "test_data_each = []\n",
    "for i in range(sub_cnt):\n",
    "    test_data_each.insert(i, test_data[i*test_data_cnt:(i+1)*test_data_cnt, :, :])\n",
    "print(np.shape(test_data_each))\n",
    "\n",
    "# sub number\n",
    "sub_num = 1\n",
    "\n",
    "#1 to 3 비율로 설정\n",
    "ratio = 3\n",
    "\n",
    "train_data_n = train_data_each[sub_num]\n",
    "test_data_n = test_data_each[sub_num]\n",
    "\n",
    "# train data를 sub:other = 1:3으로 만들기\n",
    "# 3초 덩어리 개수 1052 : 3156\n",
    "# => 315 * 4 + 316 * 6 = 1260 + 1896 = 3156\n",
    "\n",
    "# test data를 sub:other = 1:3로 만들기\n",
    "# 3초 덩어리 개수 211 : 633\n",
    "# 63 * 7 + 64 * 3 = 633\n",
    "\n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 4:\n",
    "        cnt = cnt + 1\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 315)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 4:\n",
    "        train_data_n = np.append(train_data_n, np.array(random.sample(list(train_data_each[j]), 316)), axis = 0)\n",
    "#     print(\"train_data_n.shape\")\n",
    "#     print(train_data_n.shape)\n",
    "#     print(\"train_data_n\")\n",
    "#     print(train_data_n)\n",
    "        \n",
    "cnt = 0\n",
    "for j in range(sub_cnt):\n",
    "    if j != sub_num and cnt < 7:\n",
    "        cnt = cnt + 1\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 63)), axis = 0)\n",
    "    elif j != sub_num and cnt >= 7:\n",
    "        test_data_n = np.append(test_data_n, np.array(random.sample(list(test_data_each[j]), 64)), axis = 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "train_label = np.zeros(train_data_cnt*(ratio+1))\n",
    "test_label = np.zeros(test_data_cnt*(ratio+1))\n",
    "\n",
    "for j in range(len(train_label)):\n",
    "    if (j < train_data_cnt):\n",
    "        train_label[j] = 1\n",
    "\n",
    "for j in range(len(test_label)):\n",
    "    if (j < test_data_cnt):\n",
    "        test_label[j] = 1\n",
    "        \n",
    "train_data_shuffled, train_label_shuffled = sk.utils.shuffle(train_data_n, train_label, random_state = 0)\n",
    "\n",
    "# val_data_set = train_data_shuffled[:train_data_cnt]\n",
    "# train_data_set = train_data_shuffled[train_data_cnt:]\n",
    "\n",
    "# val_label_set = train_label_shuffled[:train_data_cnt]\n",
    "# train_label_set = train_label_shuffled[train_data_cnt:]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (train_data_shuffled, train_label_shuffled), (test_data_n, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "velvet-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the teacher # 교사 모델 생성\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 154, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 190, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 18, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.4476962408598348),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(174, activation = 'relu'),\n",
    "        layers.Dense(36, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student # 학생 모델 생성\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(480, 2)),\n",
    "        layers.Conv1D(filters = 16, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Conv1D(filters = 8, kernel_size = 3, strides = 1, padding = 'same', activation = 'relu'),\n",
    "        layers.MaxPooling1D(3),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.LSTM(20),\n",
    "        layers.Dense(5, activation = 'relu'),\n",
    "        layers.Dense(1, activation = 'sigmoid'),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison # 후행 비교를 위한 학생 복제\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "renewable-consortium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_20 (Conv1D)           (None, 480, 154)          1078      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 160, 154)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 160, 190)          87970     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 53, 190)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 53, 18)            10278     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 17, 18)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 17, 18)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                21248     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 174)               11310     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 36)                6300      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 37        \n",
      "=================================================================\n",
      "Total params: 138,221\n",
      "Trainable params: 138,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.6639 - binary_accuracy: 0.6953\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.6095 - binary_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5828 - binary_accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5715 - binary_accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5665 - binary_accuracy: 0.7500\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5626 - binary_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5587 - binary_accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 0.5555 - binary_accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 0.5513 - binary_accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5463 - binary_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5403 - binary_accuracy: 0.7502\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5322 - binary_accuracy: 0.7507\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.5203 - binary_accuracy: 0.7533\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 0.5078 - binary_accuracy: 0.7569\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 0.4872 - binary_accuracy: 0.7666\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.4652 - binary_accuracy: 0.7807\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.4348 - binary_accuracy: 0.8032\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.3913 - binary_accuracy: 0.8332\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.3448 - binary_accuracy: 0.8653\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.2888 - binary_accuracy: 0.8966\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.2581 - binary_accuracy: 0.9118\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.2345 - binary_accuracy: 0.9216\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.2115 - binary_accuracy: 0.9282\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1980 - binary_accuracy: 0.9377\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1907 - binary_accuracy: 0.9373\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 3s 14ms/step - loss: 0.1868 - binary_accuracy: 0.9380\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1748 - binary_accuracy: 0.9399\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1658 - binary_accuracy: 0.9439\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1622 - binary_accuracy: 0.9449\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1622 - binary_accuracy: 0.9442\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1646 - binary_accuracy: 0.9430\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1656 - binary_accuracy: 0.9423\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1558 - binary_accuracy: 0.9482\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1535 - binary_accuracy: 0.9470\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1409 - binary_accuracy: 0.9539\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1400 - binary_accuracy: 0.9532\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1552 - binary_accuracy: 0.9420\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1470 - binary_accuracy: 0.9503\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1377 - binary_accuracy: 0.9539\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1399 - binary_accuracy: 0.9518\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1326 - binary_accuracy: 0.9565\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1330 - binary_accuracy: 0.9565\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1265 - binary_accuracy: 0.9598\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1330 - binary_accuracy: 0.9556\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1270 - binary_accuracy: 0.9563\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1351 - binary_accuracy: 0.9560\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1186 - binary_accuracy: 0.9627\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1223 - binary_accuracy: 0.9598\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1203 - binary_accuracy: 0.9608\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1186 - binary_accuracy: 0.9584\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1192 - binary_accuracy: 0.9603\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 3s 15ms/step - loss: 0.1174 - binary_accuracy: 0.9608\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1165 - binary_accuracy: 0.9594\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1114 - binary_accuracy: 0.9615\n",
      "Epoch 55/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1074 - binary_accuracy: 0.9615\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1169 - binary_accuracy: 0.9598\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1122 - binary_accuracy: 0.9606\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1139 - binary_accuracy: 0.9617\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1094 - binary_accuracy: 0.9615\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1099 - binary_accuracy: 0.9629\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1054 - binary_accuracy: 0.9658\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1081 - binary_accuracy: 0.9625\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1067 - binary_accuracy: 0.9629\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 4s 17ms/step - loss: 0.1074 - binary_accuracy: 0.9634\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1097 - binary_accuracy: 0.9629\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.0982 - binary_accuracy: 0.9691\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 4s 17ms/step - loss: 0.1079 - binary_accuracy: 0.9629\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 4s 17ms/step - loss: 0.0986 - binary_accuracy: 0.9651\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.1090 - binary_accuracy: 0.9629\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 3s 16ms/step - loss: 0.0988 - binary_accuracy: 0.9663\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 4s 17ms/step - loss: 0.0958 - binary_accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 4s 18ms/step - loss: 0.0873 - binary_accuracy: 0.9722\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0949 - binary_accuracy: 0.9653\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0968 - binary_accuracy: 0.9672\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0863 - binary_accuracy: 0.9708\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0910 - binary_accuracy: 0.9703\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0885 - binary_accuracy: 0.9689\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0943 - binary_accuracy: 0.9677\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 4s 19ms/step - loss: 0.0906 - binary_accuracy: 0.9677\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0938 - binary_accuracy: 0.9670\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.1009 - binary_accuracy: 0.9646\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.0984 - binary_accuracy: 0.9651\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0872 - binary_accuracy: 0.9686\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 4s 20ms/step - loss: 0.0852 - binary_accuracy: 0.9724\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0893 - binary_accuracy: 0.9705\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0781 - binary_accuracy: 0.9758\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 5s 22ms/step - loss: 0.0869 - binary_accuracy: 0.9701\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0943 - binary_accuracy: 0.9665\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 5s 22ms/step - loss: 0.0897 - binary_accuracy: 0.9674\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 4s 21ms/step - loss: 0.0837 - binary_accuracy: 0.9722\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 5s 21ms/step - loss: 0.0818 - binary_accuracy: 0.9710\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 5s 22ms/step - loss: 0.0800 - binary_accuracy: 0.9717\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.0858 - binary_accuracy: 0.9705\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.0754 - binary_accuracy: 0.9758\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.0775 - binary_accuracy: 0.9729\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.0734 - binary_accuracy: 0.9743\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.0789 - binary_accuracy: 0.9734\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.0786 - binary_accuracy: 0.9724\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 5s 25ms/step - loss: 0.0929 - binary_accuracy: 0.9672\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 0.0869 - binary_accuracy: 0.9689\n",
      "27/27 - 0s - loss: 0.1475 - binary_accuracy: 0.9538\n",
      "[[627   6]\n",
      " [ 33 178]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "teacher_accuracy\n",
      "0.9537914395332336\n",
      "teacher_specificity\n",
      "0.990521327014218\n",
      "teacher_sensitivity\n",
      "0.8436018957345972\n",
      "teacher_precision\n",
      "0.967391304347826\n",
      "teacher_recall\n",
      "0.8436018957345972\n",
      "teacher_frr\n",
      "0.15639810426540285\n",
      "teacher_far\n",
      "0.009478672985781991\n"
     ]
    }
   ],
   "source": [
    "# Train teacher as usual # 평소와 같은 방법으로 교사모델 훈련시작\n",
    "teacher.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.0003045598974602239, momentum=0.8915087720100587),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "teacher.summary()\n",
    "\n",
    "# Train and evaluate teacher on data. # 데이터셋을 통해 교사의 트레인 및 평가\n",
    "hist = teacher.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "teacher.save('Teacher_Model_2.h5')\n",
    "\n",
    "teacher_loss, teacher_acc = teacher.evaluate(x_test, y_test, verbose = 2)\n",
    "teacher_pred = teacher.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(teacher_pred)):\n",
    "    if(0.5 <= teacher_pred[i]):\n",
    "        teacher_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        teacher_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, teacher_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "teacher_sen = conf_matrix[1][1] / row[1]\n",
    "teacher_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "teacher_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "teacher_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "teacher_prec = conf_matrix[1][1] / conf_col[1]\n",
    "teacher_recall = conf_matrix[1][1] / conf_row[1]\n",
    "    \n",
    "print('teacher_accuracy')\n",
    "print(teacher_acc)\n",
    "print('teacher_specificity')\n",
    "print(teacher_spe)\n",
    "print('teacher_sensitivity')\n",
    "print(teacher_sen)\n",
    "print('teacher_precision')\n",
    "print(teacher_prec)\n",
    "print('teacher_recall')\n",
    "print(teacher_recall)\n",
    "print('teacher_frr')\n",
    "print(teacher_frr)\n",
    "print('teacher_far')\n",
    "print(teacher_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "accurate-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "211/211 [==============================] - 5s 24ms/step - binary_accuracy: 0.7498 - student_loss: 0.5370 - distillation_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.7540 - student_loss: 0.4415 - distillation_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.8194 - student_loss: 0.3674 - distillation_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.8840 - student_loss: 0.2915 - distillation_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9080 - student_loss: 0.2450 - distillation_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9211 - student_loss: 0.2086 - distillation_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "211/211 [==============================] - 5s 24ms/step - binary_accuracy: 0.9228 - student_loss: 0.2133 - distillation_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9294 - student_loss: 0.1954 - distillation_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9275 - student_loss: 0.1946 - distillation_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9370 - student_loss: 0.1769 - distillation_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9413 - student_loss: 0.1673 - distillation_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9458 - student_loss: 0.1576 - distillation_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9449 - student_loss: 0.1535 - distillation_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9508 - student_loss: 0.1434 - distillation_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9553 - student_loss: 0.1363 - distillation_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9520 - student_loss: 0.1312 - distillation_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "211/211 [==============================] - 5s 24ms/step - binary_accuracy: 0.9551 - student_loss: 0.1258 - distillation_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9489 - student_loss: 0.1395 - distillation_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9546 - student_loss: 0.1283 - distillation_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9548 - student_loss: 0.1265 - distillation_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9537 - student_loss: 0.1246 - distillation_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9534 - student_loss: 0.1279 - distillation_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9584 - student_loss: 0.1211 - distillation_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9527 - student_loss: 0.1251 - distillation_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9565 - student_loss: 0.1166 - distillation_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9610 - student_loss: 0.1188 - distillation_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9582 - student_loss: 0.1097 - distillation_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9591 - student_loss: 0.1152 - distillation_loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9634 - student_loss: 0.1019 - distillation_loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9641 - student_loss: 0.0990 - distillation_loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9577 - student_loss: 0.1103 - distillation_loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9622 - student_loss: 0.1022 - distillation_loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9596 - student_loss: 0.1078 - distillation_loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9587 - student_loss: 0.1107 - distillation_loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9584 - student_loss: 0.1116 - distillation_loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9610 - student_loss: 0.1010 - distillation_loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9627 - student_loss: 0.1076 - distillation_loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9589 - student_loss: 0.1073 - distillation_loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9629 - student_loss: 0.1047 - distillation_loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9674 - student_loss: 0.0942 - distillation_loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9601 - student_loss: 0.1065 - distillation_loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9636 - student_loss: 0.1028 - distillation_loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9636 - student_loss: 0.0994 - distillation_loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9601 - student_loss: 0.1044 - distillation_loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9684 - student_loss: 0.0948 - distillation_loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9648 - student_loss: 0.0960 - distillation_loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9648 - student_loss: 0.0969 - distillation_loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9658 - student_loss: 0.0984 - distillation_loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9648 - student_loss: 0.0992 - distillation_loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9660 - student_loss: 0.0949 - distillation_loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9644 - student_loss: 0.0933 - distillation_loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9658 - student_loss: 0.0938 - distillation_loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9634 - student_loss: 0.0971 - distillation_loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9658 - student_loss: 0.0931 - distillation_loss: 0.0000e+00\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211/211 [==============================] - 5s 21ms/step - binary_accuracy: 0.9677 - student_loss: 0.0842 - distillation_loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9665 - student_loss: 0.0898 - distillation_loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9663 - student_loss: 0.0931 - distillation_loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9665 - student_loss: 0.0881 - distillation_loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9670 - student_loss: 0.0920 - distillation_loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9658 - student_loss: 0.0917 - distillation_loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9684 - student_loss: 0.0866 - distillation_loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "211/211 [==============================] - 5s 23ms/step - binary_accuracy: 0.9684 - student_loss: 0.0884 - distillation_loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9684 - student_loss: 0.0867 - distillation_loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "211/211 [==============================] - 5s 21ms/step - binary_accuracy: 0.9660 - student_loss: 0.0871 - distillation_loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9724 - student_loss: 0.0757 - distillation_loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9646 - student_loss: 0.0932 - distillation_loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9689 - student_loss: 0.0805 - distillation_loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9703 - student_loss: 0.0789 - distillation_loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9734 - student_loss: 0.0774 - distillation_loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9701 - student_loss: 0.0838 - distillation_loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "211/211 [==============================] - 5s 21ms/step - binary_accuracy: 0.9674 - student_loss: 0.0941 - distillation_loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9646 - student_loss: 0.0906 - distillation_loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9679 - student_loss: 0.0840 - distillation_loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9693 - student_loss: 0.0805 - distillation_loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9727 - student_loss: 0.0761 - distillation_loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9691 - student_loss: 0.0787 - distillation_loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9701 - student_loss: 0.0760 - distillation_loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9722 - student_loss: 0.0788 - distillation_loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9674 - student_loss: 0.0887 - distillation_loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9736 - student_loss: 0.0770 - distillation_loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9722 - student_loss: 0.0811 - distillation_loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "211/211 [==============================] - 5s 21ms/step - binary_accuracy: 0.9710 - student_loss: 0.0767 - distillation_loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9720 - student_loss: 0.0805 - distillation_loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "211/211 [==============================] - 5s 22ms/step - binary_accuracy: 0.9729 - student_loss: 0.0807 - distillation_loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9698 - student_loss: 0.0835 - distillation_loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9703 - student_loss: 0.0792 - distillation_loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "211/211 [==============================] - 5s 21ms/step - binary_accuracy: 0.9674 - student_loss: 0.0770 - distillation_loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9727 - student_loss: 0.0764 - distillation_loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9746 - student_loss: 0.0764 - distillation_loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9705 - student_loss: 0.0780 - distillation_loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9712 - student_loss: 0.0785 - distillation_loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9729 - student_loss: 0.0743 - distillation_loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9696 - student_loss: 0.0803 - distillation_loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9717 - student_loss: 0.0772 - distillation_loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9717 - student_loss: 0.0758 - distillation_loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9712 - student_loss: 0.0745 - distillation_loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9712 - student_loss: 0.0799 - distillation_loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "211/211 [==============================] - 4s 20ms/step - binary_accuracy: 0.9734 - student_loss: 0.0732 - distillation_loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9710 - student_loss: 0.0757 - distillation_loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "211/211 [==============================] - 4s 21ms/step - binary_accuracy: 0.9736 - student_loss: 0.0692 - distillation_loss: 0.0000e+00\n",
      "27/27 - 0s - binary_accuracy: 0.9597 - student_loss: 0.1121\n",
      "[[626   7]\n",
      " [ 27 184]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [633 211]\n",
      "student_accuracy\n",
      "0.9597156643867493\n",
      "student_specificity\n",
      "0.9889415481832543\n",
      "student_sensitivity\n",
      "0.8720379146919431\n",
      "student_precision\n",
      "0.9633507853403142\n",
      "student_recall\n",
      "0.8720379146919431\n",
      "student_frr\n",
      "0.12796208530805686\n",
      "student_far\n",
      "0.011058451816745656\n"
     ]
    }
   ],
   "source": [
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "  \n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    student_loss_fn=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, batch_size=20, epochs=100, verbose = 1)\n",
    "\n",
    "# save model\n",
    "student.save('Student_Model_2.h5')\n",
    "\n",
    "# Evaluate student on test dataset\n",
    "student_acc = distiller.evaluate(x_test, y_test, verbose = 2)\n",
    "student_pred = student.predict(x_test)\n",
    "    \n",
    "# 확률이 0.5이상이면 자신(1), 작으면 타인(0)\n",
    "for i in range(len(student_pred)):\n",
    "    if(0.5 <= student_pred[i]):\n",
    "        student_pred[i] = 1\n",
    "\n",
    "    else:\n",
    "        student_pred[i] = 0\n",
    "        \n",
    "# confusion matrix 생성\n",
    "conf_matrix = confusion_matrix(y_test, student_pred)\n",
    "    \n",
    "#############################################################\n",
    "#       conf_matrix[0][0] = TN      conf_matrix[0][1] = FP  #\n",
    "#       conf_matrix[1][0] = FN      conf_matrix[1][1] = TP  #\n",
    "#############################################################\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "cax = ax.matshow(conf_matrix)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "    plt.text(j, i, conf_matrix[i, j], color=\"white\")\n",
    "\n",
    "plt.title('CNN+LSTM model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# sum 이용 각 행 더하기\n",
    "row = conf_matrix.sum(axis=1)\n",
    "print('\\n', row)\n",
    "\n",
    "conf_row = conf_matrix.sum(axis = 1)\n",
    "conf_col = conf_matrix.sum(axis = 0)\n",
    "\n",
    "# row[0] = spec의 분모, row[1] = sens의 분모\n",
    "\n",
    "student_sen = conf_matrix[1][1] / row[1]\n",
    "student_spe = conf_matrix[0][0] / row[0]\n",
    "    \n",
    "student_frr = conf_matrix[1][0] / (conf_matrix[1][1]+conf_matrix[1][0])\n",
    "student_far = conf_matrix[0][1] / (conf_matrix[0][1]+conf_matrix[0][0])\n",
    "    \n",
    "student_prec = conf_matrix[1][1] / conf_col[1]\n",
    "student_recall = conf_matrix[1][1] / conf_row[1]\n",
    "\n",
    "print('student_accuracy')\n",
    "print(student_acc)\n",
    "print('student_specificity')\n",
    "print(student_spe)\n",
    "print('student_sensitivity')\n",
    "print(student_sen)\n",
    "print('student_precision')\n",
    "print(student_prec)\n",
    "print('student_recall')\n",
    "print(student_recall)\n",
    "print('student_frr')\n",
    "print(student_frr)\n",
    "print('student_far')\n",
    "print(student_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "headed-mission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# gzip을 통해 모델을 실제로 압축하고 압축된 크기를 측정\n",
    "# 모델의 크기(바이트)를 반환한다.\n",
    "def get_gzipped_model_size(file):\n",
    "  \n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "nasty-pencil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved teacher model to: /tmp/tmp6bsvr8xc.h5\n",
      "Saved student model to: /tmp/tmptfc0p_z7.h5\n",
      "Size of gzipped Teacher model: 514236.00 bytes\n",
      "Size of gzipped Student model: 13750.00 bytes\n"
     ]
    }
   ],
   "source": [
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, teacher_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(teacher, teacher_file, include_optimizer=False)\n",
    "print('Saved teacher model to:', teacher_file)\n",
    "\n",
    "# 임시 파일을 생성하고 그 파일에 학습한 모델을 저장한다.\n",
    "# keras_file는 file path\n",
    "_, student_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(student, student_file, include_optimizer=False)\n",
    "print('Saved student model to:', student_file)\n",
    "\n",
    "print(\"Size of gzipped Teacher model: %.2f bytes\" % (get_gzipped_model_size(teacher_file)))\n",
    "print(\"Size of gzipped Student model: %.2f bytes\" % (get_gzipped_model_size(student_file)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
